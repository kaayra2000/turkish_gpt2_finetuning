{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eFIbL_V3PIwT"
      },
      "source": [
        "# Giriş işlemleri"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vZonvfzWJ4MV"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install trl bitsandbytes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lLjbKqLWPMCs"
      },
      "source": [
        "## Kütüphane içe aktarma"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "whu-0JW1OV36"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import torch\n",
        "from trl import SFTTrainer, SFTConfig\n",
        "from datasets import Dataset\n",
        "from typing import List, Literal, Tuple, Any, Dict\n",
        "from transformers import (AutoModelForCausalLM,\n",
        "                          AutoTokenizer,\n",
        "                          BitsAndBytesConfig, PreTrainedTokenizer)\n",
        "from peft import LoraConfig, get_peft_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1VOz7owfPO2Q"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AKfyD5oZN8k5"
      },
      "outputs": [],
      "source": [
        "def is_colab() -> bool:\n",
        "    \"\"\"\n",
        "    Google Colab ortamında çalışıp çalışmadığını kontrol eden fonksiyon.\n",
        "\n",
        "    Args:\n",
        "        None\n",
        "\n",
        "    Returns:\n",
        "        bool: Eğer kod Google Colab'da çalışıyorsa True, aksi halde False döndürür\n",
        "    \"\"\"\n",
        "    try:\n",
        "        import google.colab\n",
        "        return True\n",
        "    except ImportError:\n",
        "        return False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XeFEFuO2N_Qz",
        "outputId": "41b50acb-d1b3-4447-98bb-944b4daf8424"
      },
      "outputs": [],
      "source": [
        "# Kök dizin belirleme\n",
        "if is_colab():\n",
        "    \"\"\"\n",
        "    Eğer kod Google Colab ortamında çalışıyorsa, Google Drive'ı bağlar ve\n",
        "    kök dizini Google Drive içindeki \"turkish_gpt2_finetuning\" klasörü olarak ayarlar.\n",
        "    \"\"\"\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')  # Google Drive'ı Colab ortamına bağlar\n",
        "    kok_dizin = \"/content/drive/MyDrive/turkish_gpt2_finetuning\"  # Drive içindeki çalışma klasörünü belirler\n",
        "else:\n",
        "    \"\"\"\n",
        "    Eğer kod yerel bir ortamda çalışıyorsa, kök dizini mevcut çalışma dizini olarak ayarlar.\n",
        "    \"\"\"\n",
        "    kok_dizin = os.getcwd()  # Mevcut çalışma dizinini alır\n",
        "\n",
        "# Belirlenen kök dizini kullanıcıya bilgi olarak gösterir\n",
        "print(f\"Kök dizin: {kok_dizin}\\n Not: eğer colab kullanıyorsanız, dizini değiştirmeniz gerekir.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "STt93S85Yyxx"
      },
      "source": [
        "## Veri kümesi biçimlendirme, yol ve model yolu tanımlamaları"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EeKc_832OR8P"
      },
      "outputs": [],
      "source": [
        "# Veri kümesi yolu ve karıştırma seed değeri tanımlamaları\n",
        "veri_kumesi_yolu = os.path.join(kok_dizin, \"soru_cevap.csv\")\n",
        "veri_kumesi_karistirma = 571  # Karıştırmada kullanılacak sabit seed değeri\n",
        "sonuc_dizini = os.path.join(kok_dizin, \"sonuclar\")  # Sonuçların kaydedileceği dizin\n",
        "\n",
        "# Model kaydetme dizinleri\n",
        "# gpt4o verisiyle eğitilmiş modeller\n",
        "gpt2_medium_kaydetme_dizini_gpt4o = os.path.join(kok_dizin, \"gpt2_medium_gpt4o\")\n",
        "gpt2_large_kaydetme_dizini_gpt4o = os.path.join(kok_dizin, \"gpt2_large_gpt4o\")\n",
        "\n",
        "# deepseek verisiyle eğitilmiş modeller\n",
        "gpt2_medium_kaydetme_dizini_deepseek = os.path.join(kok_dizin, \"gpt2_medium_deepseek\")\n",
        "gpt2_large_kaydetme_dizini_deepseek = os.path.join(kok_dizin, \"gpt2_large_deepseek\")\n",
        "\n",
        "# Model adı tanımlamaları\n",
        "gpt2_medium_model_adi = \"ytu-ce-cosmos/turkish-gpt2-medium\"\n",
        "gpt2_large_model_adi = \"ytu-ce-cosmos/turkish-gpt2-large\"\n",
        "\n",
        "# Veri kümesi sütun tanımlamaları\n",
        "soru_sutunu = \"Soru\"\n",
        "gpt4o_cevap_sutunu = \"gpt4o cevabı\"\n",
        "deepseek_cevap_sutunu = \"deepseek cevabı\"\n",
        "\n",
        "# Özel token tanımlamaları (eğitim formatı için)\n",
        "soru_baslangic = \"<SORU>\"\n",
        "soru_bitis = \"</SORU>\"\n",
        "cevap_baslangic = \"<CEVAP>\"\n",
        "cevap_bitis = \"</CEVAP>\"\n",
        "ornek_bitis = \"<|endoftext|>\"  # GPT-2'nin EOS tokeni\n",
        "\n",
        "# Tokenizer'a eklenecek özel tokenler listesi\n",
        "ozel_tokenler = [soru_baslangic, soru_bitis, cevap_baslangic, cevap_bitis]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gwpuWbgSPQtI"
      },
      "source": [
        "## Veri kümesi okuma"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rZnjjD0oPBjo",
        "outputId": "6ac8a558-8d38-44d2-c071-6adceb56825b"
      },
      "outputs": [],
      "source": [
        "# -----------------------------------------------\n",
        "# CSV dosyasını okuma ve gerekli sütunları ayıklama\n",
        "# -----------------------------------------------\n",
        "\n",
        "try:\n",
        "    # -------------------------------------------------\n",
        "    # 1) CSV dosyasını diskteki yolundan (veri_kumesi_yolu)\n",
        "    #    pandas DataFrame olarak içeri aktar.\n",
        "    # -------------------------------------------------\n",
        "    df = pd.read_csv(veri_kumesi_yolu)\n",
        "\n",
        "    # -------------------------------------------------\n",
        "    # 2) Kullanıcıdan/konfigürasyondan gelen beklenen\n",
        "    #    sütun isimlerini bir liste hâlinde tanımla.\n",
        "    #\n",
        "    #    Burada:\n",
        "    #      - \"Soru\" : soruları içeren sütun\n",
        "    #      - gpt4o_cevap_sutunu : GPT-4o modelinin cevapları\n",
        "    #      - deepseek_cevap_sutunu : DeepSeek modelinin cevapları\n",
        "    # -------------------------------------------------\n",
        "    gereken_sutunlar = [\"Soru\", gpt4o_cevap_sutunu, deepseek_cevap_sutunu]\n",
        "\n",
        "    # -------------------------------------------------\n",
        "    # 3) Her bir gerekli sütunun DataFrame’de mevcut\n",
        "    #    olup olmadığını kontrol et. Eksikse uyarı ver.\n",
        "    # -------------------------------------------------\n",
        "    for sutun in gereken_sutunlar:\n",
        "        if sutun not in df.columns:\n",
        "            print(f\"'{sutun}' sütunu veri kümesinde bulunamadı!\")\n",
        "\n",
        "    # -------------------------------------------------\n",
        "    # 4) Sadece gerekli sütunları içeren yeni bir\n",
        "    #    DataFrame (soru_cevap_df) oluştur.\n",
        "    # -------------------------------------------------\n",
        "    soru_cevap_df = df[gereken_sutunlar]\n",
        "\n",
        "    # -------------------------------------------------\n",
        "    # 5) Doğrulama amacıyla ilk 5 satırı ekrana yazdır.\n",
        "    # -------------------------------------------------\n",
        "    print(\"Veri kümesinin ilk birkaç satırı:\")\n",
        "    print(soru_cevap_df.head(5))\n",
        "\n",
        "    # -------------------------------------------------\n",
        "    # 6) Toplam kaç soru-cevap çifti olduğunu bildir.\n",
        "    # -------------------------------------------------\n",
        "    print(f\"\\nToplam {len(soru_cevap_df)} soru-cevap çifti bulundu.\")\n",
        "\n",
        "# -------------------------------------------------\n",
        "# 7) Dosya bulunamazsa kullanıcıyı bilgilendir.\n",
        "# -------------------------------------------------\n",
        "except FileNotFoundError:\n",
        "    print(f\"'{veri_kumesi_yolu}' dosyası bulunamadı. Lütfen dosya yolunu kontrol ediniz.\")\n",
        "\n",
        "# -------------------------------------------------\n",
        "# 8) Diğer tüm hataları genel başlıkta yakala\n",
        "#    ve mesajını göster.\n",
        "# -------------------------------------------------\n",
        "except Exception as e:\n",
        "    print(f\"Veri okuma hatası: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HILJV27GPiIc",
        "outputId": "b95b5604-9493-41e2-e259-5c991b762bc6"
      },
      "outputs": [],
      "source": [
        "# ----------------------------------------------\n",
        "# Veri kümesini karıştırma (shuffle) işlemi\n",
        "# ----------------------------------------------\n",
        "\n",
        "# 1) sample(frac=1) metodu:\n",
        "#    • frac=1 -> DataFrame’in tamamını örnekle, yani tüm satırları rastgele sıraya diz.\n",
        "#    • random_state=veri_kumesi_karistirma -> \n",
        "#      Aynı seed değeriyle her çalıştırmada aynı karışık sıralamayı elde\n",
        "#      etmeyi sağlar (deneysellik için tekrar üretilebilirlik) [[2]].\n",
        "shuffled_df = soru_cevap_df.sample(\n",
        "    frac=1,                     # Tüm satırları al\n",
        "    random_state=veri_kumesi_karistirma  # Sabit tohum (seed)\n",
        ")\n",
        "\n",
        "# ----------------------------------------------\n",
        "# İndeksleri sıfırlama\n",
        "# ----------------------------------------------\n",
        "# 2) Karıştırma sonrası eski indeksler anlamını yitirir.\n",
        "#    reset_index(drop=True) ile:\n",
        "#      • drop=True   -> Eski indeks sütunu DataFrame’e eklenmez,\n",
        "#                       doğrudan atılır.\n",
        "#      • Yeni indeksler 0..N-1 şeklinde oluşturulur.\n",
        "shuffled_df = shuffled_df.reset_index(drop=True)\n",
        "\n",
        "# ----------------------------------------------\n",
        "# Karıştırılmış veri kümesini gözlemleme\n",
        "# ----------------------------------------------\n",
        "# 3) İlk 5 satırı yazdırarak karıştırmanın\n",
        "#    başarıyla gerçekleştiğini doğrula.\n",
        "print(\"Karıştırılmış veri kümesinin ilk birkaç satırı:\")\n",
        "print(shuffled_df.head(5))\n",
        "\n",
        "# 4) Toplam satır sayısını ekrana yazdır.\n",
        "print(f\"\\nKarıştırılmış veri kümesi boyutu: {len(shuffled_df)} satır\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JXG9nHCqZSFT"
      },
      "source": [
        "# Veri kümesi işleme"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eWMjPMiuW44_"
      },
      "source": [
        "## Veri kümesi biçimlendirme fonksiyonu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JWeG9G8pURiv"
      },
      "outputs": [],
      "source": [
        "def veri_kumesini_egitim_formatina_donustur(\n",
        "    df              : pd.DataFrame,   # Soru–cevap verilerini tutan pandas DataFrame\n",
        "    cevap_sutunu    : str,            # Hangi sütunun “cevap” olarak kullanılacağını belirtir\n",
        "    *,\n",
        "    soru_baslangic  : str = \"<SORU>\",      # Soru başlangıcını işaretleyen özel etiket\n",
        "    soru_bitis      : str = \"</SORU>\",     # Soru bitişini   işaretleyen özel etiket\n",
        "    cevap_baslangic : str = \"<CEVAP>\",     # Cevap başlangıcını işaretleyen özel etiket\n",
        "    cevap_bitis     : str = \"</CEVAP>\",    # Cevap bitişini   işaretleyen özel etiket\n",
        "    ornek_bitis     : str = \"<|endoftext|>\"# Her örneğin sonunda yer alacak EOS biçimindeki etiket\n",
        ") -> List[str]:\n",
        "    \"\"\"\n",
        "    Soru–cevap veri kümesini GPT-2 eğitimine uygun tekil metin dizilerine dönüştürür.\n",
        "\n",
        "    Fonksiyon, her satır (soru-cevap çifti) için aşağıdaki genel formata sahip\n",
        "    birleştirilmiş bir string üretir ve çıkan tüm örnekleri liste hâlinde döndürür:\n",
        "\n",
        "        <SORU>  {soru_metni}  </SORU>  <CEVAP>  {cevap_metni}  </CEVAP><|endoftext|>\n",
        "\n",
        "    NOT: Yalnızca metin birleştirmesi yapar, herhangi bir tokenizasyon işlemi uygulamaz.\n",
        "\n",
        "    Parametreler\n",
        "    ------------\n",
        "    df : pandas.DataFrame\n",
        "        “Soru” ve istenen cevap sütununu içeren veri çerçevesi.\n",
        "    cevap_sutunu : str\n",
        "        Cevapların bulunduğu sütun adı (örn. \"gpt4o cevabı\").\n",
        "    soru_baslangic : str, varsayılan \"<SORU>\"\n",
        "        Soru başlangıcı için kullanılacak özel belirteç.\n",
        "    soru_bitis : str, varsayılan \"</SORU>\"\n",
        "        Soru bitişi   için kullanılacak özel belirteç.\n",
        "    cevap_baslangic : str, varsayılan \"<CEVAP>\"\n",
        "        Cevap başlangıcı için kullanılacak özel belirteç.\n",
        "    cevap_bitis : str, varsayılan \"</CEVAP>\"\n",
        "        Cevap bitişi   için kullanılacak özel belirteç.\n",
        "    ornek_bitis : str, varsayılan \"<|endoftext|>\"\n",
        "        Her örneğin sonunda eklenecek bitiş/EOS belirteci.\n",
        "\n",
        "    Dönüş Değeri\n",
        "    ------------\n",
        "    List[str]\n",
        "        Eğitimde doğrudan kullanılmaya hazır, “birleştirilmiş” metin dizilerinin listesi.\n",
        "    \"\"\"\n",
        "\n",
        "    # ---------------------------------------------------------------------------- #\n",
        "    # 1) Çıktıları tutmak için boş bir liste oluşturuyoruz.                        #\n",
        "    #    Her iterasyonda, yeni biçimlendirilmiş metin bu listeye eklenecek.        #\n",
        "    # ---------------------------------------------------------------------------- #\n",
        "    egitim_metinleri: List[str] = []\n",
        "\n",
        "    # ---------------------------------------------------------------------------- #\n",
        "    # 2) DataFrame'deki her satır (index, satir) ikilisi üzerinde döngü kuruyoruz. #\n",
        "    #    • _      : index değeri (kullanılmadığı için alt tire)                    #\n",
        "    #    • satir  : pandas.Series, o anki satıra ait veriler                       #\n",
        "    # ---------------------------------------------------------------------------- #\n",
        "    for _, satir in df.iterrows():\n",
        "\n",
        "        # Satırdan “Soru” sütununu alıp baştaki/sondaki boşlukları temizliyoruz.\n",
        "        soru: str = satir[\"Soru\"].strip()\n",
        "\n",
        "        # Aynı işlemi cevap sütunu için de yapıyoruz (gönderilen isim dinamik).\n",
        "        cevap: str = satir[cevap_sutunu].strip()\n",
        "\n",
        "        # -------------------------------------------------------------------- #\n",
        "        # 3) Soru ve cevabı istenen etiketlerle sarmalayıp tek bir string      #\n",
        "        #    oluşturuyoruz. Yapı:                                              #\n",
        "        #      <SORU> Soru </SORU> <CEVAP> Cevap </CEVAP><|endoftext|>         #\n",
        "        # -------------------------------------------------------------------- #\n",
        "        bicimlendirilmis_metin: str = (\n",
        "            f\"{soru_baslangic} {soru} {soru_bitis} \"\n",
        "            f\"{cevap_baslangic} {cevap} {cevap_bitis}\"\n",
        "            f\"{ornek_bitis}\"\n",
        "        )\n",
        "\n",
        "        # 4) Elde edilen string’i çıktı listesine ekliyoruz.\n",
        "        egitim_metinleri.append(bicimlendirilmis_metin)\n",
        "\n",
        "    # ---------------------------------------------------------------------------- #\n",
        "    # 5) Özet bilgi: toplam kaç örnek üretildiğini ve şablonun nasıl göründüğünü   #\n",
        "    #    kullanıcıya yazdırıyoruz.                                                #\n",
        "    # ---------------------------------------------------------------------------- #\n",
        "    print(f\"Toplam {len(egitim_metinleri)} adet eğitim örneği oluşturuldu.\")\n",
        "    print(\n",
        "        f\"Örnek biçimi: \"\n",
        "        f\"{soru_baslangic} [Soru] {soru_bitis} \"\n",
        "        f\"{cevap_baslangic} [Cevap] {cevap_bitis}{ornek_bitis}\"\n",
        "    )\n",
        "\n",
        "    # 6) Hazırlanan tüm metinleri geri döndürüyoruz.\n",
        "    return egitim_metinleri"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MfiIVCNKh28b"
      },
      "source": [
        "## Veri kümelerini işle ve oluştur"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5GskjGIMZTnE",
        "outputId": "dc130944-715a-4682-f7a2-f5b2827d587f"
      },
      "outputs": [],
      "source": [
        "veri_kumesi_gpt4o = veri_kumesini_egitim_formatina_donustur(shuffled_df, gpt4o_cevap_sutunu)\n",
        "veri_kumesi_deepseek = veri_kumesini_egitim_formatina_donustur(shuffled_df, deepseek_cevap_sutunu)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "US-pveshRaVV"
      },
      "source": [
        "# Eğitim işlemleri"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HSNOJdatXExf"
      },
      "source": [
        "## Model Yükleme ve Eğitme Fonksiyonları"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vN_BKyEaiBQV"
      },
      "source": [
        "### Eğitim argümanları oluşturma fonksiyonu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R4fThwcfZ8mR"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "max_seq_length = 1024   # üstte sabit tanım\n",
        "\n",
        "def training_arguments_getir(\n",
        "    kaydetme_dizin      : str,\n",
        "    veri_kumesi_uzunlugu: int,\n",
        "    *,\n",
        "    learning_rate : float = 1e-4,\n",
        "    batch_size    : int   = 2,\n",
        "    grad_accum    : int   = 32,\n",
        "    epochs        : int   = 1,\n",
        "    max_seq_length: int   = 1024,\n",
        "    save_per_epoch: int   = 4,\n",
        "    log_per_epoch : int   = 12,\n",
        ") -> SFTConfig:\n",
        "    \"\"\"\n",
        "    TRL kütüphanesinin `SFTConfig` sınıfını kolayca oluşturmak için\n",
        "    öntanımlı hiper-parametrelerle donatılmış yardımcı fonksiyon.\n",
        "\n",
        "    Args:\n",
        "        kaydetme_dizin (str): Checkpoint ve log dosyalarının kaydedileceği dizin.\n",
        "        veri_kumesi_uzunlugu (int): Toplam örnek (satır) sayısı.\n",
        "        learning_rate (float, optional): Öğrenme oranı. Varsayılan 1e-4.\n",
        "        batch_size (int, optional): GPU başına batch büyüklüğü. Varsayılan 2.\n",
        "        grad_accum (int, optional): Gradient accumulation adım sayısı.\n",
        "        epochs (int, optional): Eğitim süresi (epoch). Varsayılan 1.\n",
        "        max_seq_length (int, optional): Girdi dizisi tokensel azami uzunluk.\n",
        "        save_per_epoch (int, optional): Bir epoch kaç parçaya bölünerek\n",
        "            checkpoint alınacağını belirler. Örn. 4 → epoch/4 adımda bir.\n",
        "        log_per_epoch (int, optional): Bir epoch kaç parçaya bölünerek\n",
        "            loss değeri loglanacağını belirler.\n",
        "    Returns:\n",
        "        SFTConfig: TRL/Trainer ile uyumlu, ön ayarlı konfigürasyon nesnesi.\n",
        "    \"\"\"\n",
        "\n",
        "    # ----------------- Türetilmiş değerleri hesapla ----------------------- #\n",
        "    ngpu: int = max(torch.cuda.device_count(), 1)         # GPU adedi\n",
        "    effective_batch: int = batch_size * grad_accum * ngpu # Toplam güncel batch\n",
        "    steps_per_epoch: int = math.ceil(veri_kumesi_uzunlugu / effective_batch)\n",
        "    max_steps: int        = epochs * steps_per_epoch\n",
        "    save_steps: int       = max(1, steps_per_epoch // save_per_epoch)\n",
        "    logging_steps: int    = max(1, steps_per_epoch // log_per_epoch)\n",
        "\n",
        "    print(\n",
        "        f\"[INFO] veri: {veri_kumesi_uzunlugu} örnek | gpu: {ngpu} | \"\n",
        "        f\"effective_batch: {effective_batch} | steps/epoch: {steps_per_epoch} | \"\n",
        "        f\"total steps: {max_steps}\"\n",
        "    )\n",
        "\n",
        "    # ----------------- SFTConfig için argüman sözlüğü --------------------- #\n",
        "    cfg_kwargs: Dict[str, Any] = dict(\n",
        "        output_dir = kaydetme_dizin,\n",
        "\n",
        "        # Temel hiper-parametreler\n",
        "        learning_rate               = learning_rate,\n",
        "        per_device_train_batch_size = batch_size,\n",
        "        gradient_accumulation_steps = grad_accum,\n",
        "\n",
        "        # Sekuans/eos ayarları\n",
        "        max_seq_length = max_seq_length,\n",
        "\n",
        "        # Optimizasyon\n",
        "        optim             = \"paged_adamw_8bit\",\n",
        "        weight_decay      = 0.1,\n",
        "        warmup_ratio      = 0.1,\n",
        "        lr_scheduler_type = \"cosine\",\n",
        "        max_grad_norm     = 0.1,\n",
        "\n",
        "        # Loglama\n",
        "        logging_dir      = os.path.join(kaydetme_dizin, \"logs\"),\n",
        "        logging_strategy = \"steps\",\n",
        "        logging_steps    = logging_steps,\n",
        "        report_to        = \"tensorboard\",\n",
        "        run_name         = os.path.basename(kaydetme_dizin),\n",
        "\n",
        "        # Checkpoint\n",
        "        save_strategy = \"steps\",\n",
        "        save_steps    = save_steps,\n",
        "\n",
        "        # Epoch tabanlı çalışma\n",
        "        num_train_epochs = epochs,\n",
        "        max_steps        = -1,  # -1 → Trainer epoch modunda çalış\n",
        "    )\n",
        "\n",
        "    # Nesneyi oluşturup geri döndür\n",
        "    return SFTConfig(**cfg_kwargs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "21jgGqeniGM9"
      },
      "source": [
        "### Model Yükleme fonksiyonu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yA7f5Bq3Rjcl"
      },
      "outputs": [],
      "source": [
        "def model_ve_tokenizer_yukle(\n",
        "    model_adi      : str,\n",
        "    max_seq_length : int                           = max_seq_length,\n",
        "    lora_rank      : int                           = 16,\n",
        "    random_state   : int                           = veri_kumesi_karistirma,\n",
        "    target_modules : List[str]                     = (\"c_attn\", \"c_proj\", \"c_fc\"),\n",
        "    ozel_tokenler  : List[str] | None              = None,\n",
        "    quant_mode     : Literal[\"4bit\", \"8bit\", \"16bit\"] = \"16bit\",\n",
        ") -> Tuple[torch.nn.Module, PreTrainedTokenizer]:\n",
        "    \"\"\"\n",
        "    İstenen GPT-2 tabanlı modeli; LoRA adaptörleri ve (opsiyonel) 4/8-bit\n",
        "    quantization ile belleğe yükler. Ayrıca tokenizer’ı hazırlar.\n",
        "\n",
        "    Args:\n",
        "        model_adi (str): Hugging Face model adı veya yol.\n",
        "        max_seq_length (int, optional): Modelin `max_position_embeddings` değeri.\n",
        "        lora_rank (int, optional): LoRA rank değeri (R). Varsayılan 16.\n",
        "        random_state (int, optional): CUDA random seed.\n",
        "        target_modules (List[str], optional): LoRA uygulanacak katman adları.\n",
        "        ozel_tokenler (List[str] | None): Ek special token listesi.\n",
        "        quant_mode (Literal), optional: \"4bit\", \"8bit\" veya \"16bit\".\n",
        "            • \"4bit\": QLoRA nf4  \n",
        "            • \"8bit\": bitsandbytes 8-bit  \n",
        "            • \"16bit\": Tam presizyon (fp16/bf16)\n",
        "    Returns:\n",
        "        Tuple[torch.nn.Module, PreTrainedTokenizer]: (model, tokenizer)\n",
        "    \"\"\"\n",
        "\n",
        "    # ---------------- Tokenizer hazırlığı --------------------------------- #\n",
        "    tokenizer: PreTrainedTokenizer = AutoTokenizer.from_pretrained(\n",
        "        model_adi, use_fast=True\n",
        "    )\n",
        "    # Sağ tarafta pad / truncate\n",
        "    tokenizer.padding_side, tokenizer.truncation_side = \"right\", \"right\"\n",
        "\n",
        "    # Model pad_token tanımlı değilse EOS’u pad olarak ekle\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.add_special_tokens({\"pad_token\": tokenizer.eos_token})\n",
        "\n",
        "    # ---------------- Quantization / dtype seçimi ------------------------- #\n",
        "    quant_cfg: BitsAndBytesConfig | None = None\n",
        "    model_kwargs: Dict[str, Any] = dict(device_map=\"auto\", trust_remote_code=True)\n",
        "\n",
        "    if quant_mode == \"4bit\":\n",
        "        # QLoRA konfigürasyonu\n",
        "        quant_cfg = BitsAndBytesConfig(\n",
        "            load_in_4bit=True,\n",
        "            bnb_4bit_use_double_quant=True,\n",
        "            bnb_4bit_quant_type=\"nf4\",\n",
        "            bnb_4bit_compute_dtype = (\n",
        "                torch.bfloat16 if torch.cuda.get_device_capability(0)[0] >= 8\n",
        "                else torch.float16\n",
        "            ),\n",
        "        )\n",
        "        model_kwargs[\"quantization_config\"] = quant_cfg\n",
        "\n",
        "    elif quant_mode == \"8bit\":\n",
        "        quant_cfg = BitsAndBytesConfig(load_in_8bit=True)\n",
        "        model_kwargs[\"quantization_config\"] = quant_cfg\n",
        "\n",
        "    else:  # \"16bit\"\n",
        "        model_kwargs[\"torch_dtype\"] = (\n",
        "            torch.bfloat16 if torch.cuda.get_device_capability(0)[0] >= 8\n",
        "            else torch.float16\n",
        "        )\n",
        "\n",
        "    # ---------------- Modeli yükle ---------------------------------------- #\n",
        "    model = AutoModelForCausalLM.from_pretrained(model_adi, **model_kwargs)\n",
        "\n",
        "    # Konfig ayarları\n",
        "    model.config.max_position_embeddings = max_seq_length\n",
        "    model.config.use_cache = False\n",
        "    model.gradient_checkpointing_enable()  # bellek tasarrufu\n",
        "\n",
        "    # ---------------- LoRA adaptörü ekle ---------------------------------- #\n",
        "    lora_cfg: LoraConfig = LoraConfig(\n",
        "        r = lora_rank,\n",
        "        lora_alpha = lora_rank,\n",
        "        target_modules = list(target_modules),\n",
        "        bias = \"none\",\n",
        "        task_type = \"CAUSAL_LM\",\n",
        "        fan_in_fan_out = True,\n",
        "    )\n",
        "    model = get_peft_model(model, lora_cfg)\n",
        "    model.print_trainable_parameters()\n",
        "\n",
        "    # ---------------- Özel token’lar ekle --------------------------------- #\n",
        "    if ozel_tokenler:\n",
        "        tokenizer.add_special_tokens({\"additional_special_tokens\": ozel_tokenler})\n",
        "        model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "    # Rastgeleliği sabitle\n",
        "    torch.cuda.manual_seed(random_state)\n",
        "    print(f\"{model_adi} modeli '{quant_mode}' kipinde yüklendi (+LoRA).\")\n",
        "\n",
        "    return model, tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hVl_03-EiI49"
      },
      "source": [
        "### Eğitici Getirme fonksiyonu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iPD3wCILaJez"
      },
      "outputs": [],
      "source": [
        "def trainer_getir(\n",
        "    model          : torch.nn.Module,\n",
        "    training_args  : SFTConfig,\n",
        "    veri_kumesi    : List[str],\n",
        "    tokenizer      : PreTrainedTokenizer\n",
        ") -> SFTTrainer:\n",
        "    \"\"\"\n",
        "    Metin listesini tokenize edip `SFTTrainer` için PyTorch\n",
        "    tensörlerine dönüştürür ve eğitici nesnesini oluşturur.\n",
        "\n",
        "    Args:\n",
        "        model (torch.nn.Module): PEFT (LoRA) ile zenginleştirilmiş GPT-2 modeli.\n",
        "        training_args (SFTConfig): `training_arguments_getir` çıktısı.\n",
        "        veri_kumesi (List[str]): Önceden formatlanmış eğitim metinleri listesi.\n",
        "        tokenizer (PreTrainedTokenizer): Aynı modele ait tokenizer.\n",
        "\n",
        "    Returns:\n",
        "        SFTTrainer: TRL eğitici nesnesi.\n",
        "    \"\"\"\n",
        "\n",
        "    # 1) Tokenizasyon → padding + truncation (max_length parametresi zaten var)\n",
        "    toks = tokenizer(\n",
        "        veri_kumesi,\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=training_args.max_seq_length,\n",
        "    )\n",
        "\n",
        "    # 2) Tensör formatına dönüştür\n",
        "    data_dict: Dict[str, List[List[int]]] = {\n",
        "        \"input_ids\":      toks[\"input_ids\"],\n",
        "        \"attention_mask\": toks[\"attention_mask\"],\n",
        "    }\n",
        "    train_dataset: Dataset = Dataset.from_dict(data_dict)\n",
        "    train_dataset.set_format(type=\"torch\")\n",
        "\n",
        "    print(f\"Eğitim için {len(train_dataset)} örnek hazırlandı.\")\n",
        "\n",
        "    # 3) SFTTrainer oluştur\n",
        "    trainer: SFTTrainer = SFTTrainer(\n",
        "        model         = model,\n",
        "        args          = training_args,\n",
        "        train_dataset = train_dataset,\n",
        "    )\n",
        "    return trainer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LzxRcRlPiLPV"
      },
      "source": [
        "### Veri kümesi özelinde model eğitme fonksiyonu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-qiyNRlVgna4"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "def model_egit_ve_kaydet(\n",
        "    model                : torch.nn.Module,\n",
        "    tokenizer            : PreTrainedTokenizer,\n",
        "    veri_kumesi          : List[str],\n",
        "    model_kaydetme_dizini: str,\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Verilen LoRA’lı modeli,\n",
        "    • `veri_kumesi` ile eğitir,  \n",
        "    • checkpoint/log’ları `model_kaydetme_dizini` altına kaydeder  \n",
        "    • eğitim bitince belleği temizler.\n",
        "\n",
        "    Args:\n",
        "        model (torch.nn.Module): Eğitilecek (LoRA adaptörlü) model.\n",
        "        tokenizer (PreTrainedTokenizer): İlgili tokenizer.\n",
        "        veri_kumesi (List[str]): Formatlanmış eğitim örnekleri.\n",
        "        model_kaydetme_dizini (str): Model & log kayıt dizini.\n",
        "\n",
        "    Returns:\n",
        "        None\n",
        "    \"\"\"\n",
        "\n",
        "    # 0) Çıktı dizini oluştur\n",
        "    os.makedirs(model_kaydetme_dizini, exist_ok=True)\n",
        "\n",
        "    # 1) Eğitim argümanlarını hazırla\n",
        "    training_args: SFTConfig = training_arguments_getir(\n",
        "        kaydetme_dizin      = model_kaydetme_dizini,\n",
        "        veri_kumesi_uzunlugu= len(veri_kumesi),\n",
        "    )\n",
        "\n",
        "    # 2) Trainer al\n",
        "    trainer: SFTTrainer = trainer_getir(\n",
        "        model         = model,\n",
        "        training_args = training_args,\n",
        "        veri_kumesi   = veri_kumesi,\n",
        "        tokenizer     = tokenizer,\n",
        "    )\n",
        "\n",
        "    # 3) Eğitim döngüsü\n",
        "    print(f\"\\n[Eğitim] '{model_kaydetme_dizini}' dizininde eğitim başlıyor...\")\n",
        "    trainer.train()\n",
        "\n",
        "    # 4) Model / tokenizer / logları kaydet\n",
        "    trainer.save_model(model_kaydetme_dizini)\n",
        "    tokenizer.save_pretrained(model_kaydetme_dizini)\n",
        "\n",
        "    log_json_path: str = os.path.join(model_kaydetme_dizini, \"training_log.json\")\n",
        "    with open(log_json_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(trainer.state.log_history, f, ensure_ascii=False, indent=4)\n",
        "\n",
        "    print(f\"Model ve tokenizer '{model_kaydetme_dizini}' dizinine kaydedildi.\")\n",
        "\n",
        "    # 5) Bellek temizliği\n",
        "    del model\n",
        "    torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Su5-t-l5XI6I"
      },
      "source": [
        "## GPT-2 Medium modelini GPT-4o veri kümesi ile eğit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 744,
          "referenced_widgets": [
            "c8e08de74c1043249868ef8e426d5c80",
            "942bb0ac2f1949c39754078a7563083b",
            "12fd0ce04fcd4707b0b556263d3c5fd7",
            "cbf2d870cc5f423196f91f20cc8e0c97",
            "bb69833cddd4467f908fdcaf52d6f2e2",
            "6f06747518ff490ab1fade3978c8350c",
            "23fc929f9ca443f58bf7b9f28e847224",
            "fb738a3c808d4ae08b4ac7f67db77591",
            "ac2596e4ed1f4e1fbc4b3bcd2679e286",
            "dfb6d4ff4a3b43f6b3182b780cea51b9",
            "ca451813d5ce49e4ac36d0aaf062121b"
          ]
        },
        "id": "aoe3ddtxRb7Y",
        "outputId": "cde48b2c-9ee9-4b29-956d-3aceb2abdbf4"
      },
      "outputs": [],
      "source": [
        "print(\"\\nGPT-2 Medium modeli GPT-4o veri kümesi ile eğitiliyor...\")\n",
        "model_medium, tokenizer_medium = model_ve_tokenizer_yukle(gpt2_medium_model_adi)\n",
        "# GPT-4o veri kümesiyle Medium modeli eğit\n",
        "model_egit_ve_kaydet(\n",
        "    model_medium,\n",
        "    tokenizer_medium,\n",
        "    veri_kumesi_gpt4o,\n",
        "    gpt2_medium_kaydetme_dizini_gpt4o,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ddx-uWtTiWvc"
      },
      "source": [
        "## GPT-2 Medium modelini DeepSeek veri kümesi ile eğit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VuSP5gzVhKvX"
      },
      "outputs": [],
      "source": [
        "# DeepSeek veri kümesiyle Medium modeli eğit\n",
        "print(\"\\nGPT-2 Medium modeli DeepSeek veri kümesi ile eğitiliyor...\")\n",
        "model_medium, tokenizer_medium = model_ve_tokenizer_yukle(gpt2_medium_model_adi)\n",
        "model_egit_ve_kaydet(\n",
        "    model_medium,\n",
        "    tokenizer_medium,\n",
        "    veri_kumesi_deepseek,\n",
        "    gpt2_medium_kaydetme_dizini_deepseek,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E5xacutTicNp"
      },
      "source": [
        "## GPT-2 Large modelini GPT-4o veri kümesi ile eğit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "75Ql1WILhKpf"
      },
      "outputs": [],
      "source": [
        "# GPT-4o veri kümesiyle Large modeli eğit\n",
        "print(\"\\nGPT-2 Large modeli GPT-4o veri kümesi ile eğitiliyor...\")\n",
        "model_large, tokenizer_large = model_ve_tokenizer_yukle(gpt2_large_model_adi)\n",
        "model_egit_ve_kaydet(\n",
        "    model_large,\n",
        "    tokenizer_large,\n",
        "    veri_kumesi_gpt4o,\n",
        "    gpt2_large_kaydetme_dizini_gpt4o,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wwf2Sey5ie6E"
      },
      "source": [
        "## GPT-2 Large modelini DeepSeek veri kümesi ile eğit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z7u9qL0BhKeP"
      },
      "outputs": [],
      "source": [
        "# DeepSeek veri kümesiyle Large modeli eğit\n",
        "print(\"\\nGPT-2 Large modeli DeepSeek veri kümesi ile eğitiliyor...\")\n",
        "model_large, tokenizer_large = model_ve_tokenizer_yukle(gpt2_large_model_adi)\n",
        "model_egit_ve_kaydet(\n",
        "    model_large,\n",
        "    tokenizer_large,\n",
        "    veri_kumesi_deepseek,\n",
        "    gpt2_large_kaydetme_dizini_deepseek,\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "12fd0ce04fcd4707b0b556263d3c5fd7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fb738a3c808d4ae08b4ac7f67db77591",
            "max": 13891,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ac2596e4ed1f4e1fbc4b3bcd2679e286",
            "value": 13891
          }
        },
        "23fc929f9ca443f58bf7b9f28e847224": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6f06747518ff490ab1fade3978c8350c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "942bb0ac2f1949c39754078a7563083b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6f06747518ff490ab1fade3978c8350c",
            "placeholder": "​",
            "style": "IPY_MODEL_23fc929f9ca443f58bf7b9f28e847224",
            "value": "Truncating train dataset: 100%"
          }
        },
        "ac2596e4ed1f4e1fbc4b3bcd2679e286": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "bb69833cddd4467f908fdcaf52d6f2e2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c8e08de74c1043249868ef8e426d5c80": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_942bb0ac2f1949c39754078a7563083b",
              "IPY_MODEL_12fd0ce04fcd4707b0b556263d3c5fd7",
              "IPY_MODEL_cbf2d870cc5f423196f91f20cc8e0c97"
            ],
            "layout": "IPY_MODEL_bb69833cddd4467f908fdcaf52d6f2e2"
          }
        },
        "ca451813d5ce49e4ac36d0aaf062121b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cbf2d870cc5f423196f91f20cc8e0c97": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dfb6d4ff4a3b43f6b3182b780cea51b9",
            "placeholder": "​",
            "style": "IPY_MODEL_ca451813d5ce49e4ac36d0aaf062121b",
            "value": " 13891/13891 [00:00&lt;00:00, 84964.67 examples/s]"
          }
        },
        "dfb6d4ff4a3b43f6b3182b780cea51b9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fb738a3c808d4ae08b4ac7f67db77591": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

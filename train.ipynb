{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNy4DzKf9Rh1qOVDOIlUOQJ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Giriş işlemleri"],"metadata":{"id":"eFIbL_V3PIwT"}},{"cell_type":"code","source":["%%capture\n","!pip install -q unsloth transformers accelerate bitsandbytes"],"metadata":{"id":"Igj6qGERQRee"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Kütüphane içe aktarma"],"metadata":{"id":"lLjbKqLWPMCs"}},{"cell_type":"code","source":["import os\n","import pandas as pd\n","from unsloth import FastLanguageModel\n","import torch\n","from trl import GRPOTrainer, GRPOConfig\n","from datasets import Dataset\n","from typing import List, Optional, Callable, Any, Dict, Union"],"metadata":{"id":"whu-0JW1OV36"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[],"metadata":{"id":"1VOz7owfPO2Q"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"AKfyD5oZN8k5"},"outputs":[],"source":["def is_colab() -> bool:\n","    \"\"\"\n","    Google Colab ortamında çalışıp çalışmadığını kontrol eden fonksiyon.\n","\n","    Args:\n","        None\n","\n","    Returns:\n","        bool: Eğer kod Google Colab'da çalışıyorsa True, aksi halde False döndürür\n","    \"\"\"\n","    try:\n","        import google.colab\n","        return True\n","    except ImportError:\n","        return False"]},{"cell_type":"code","source":["# Kök dizin belirleme\n","if is_colab():\n","    \"\"\"\n","    Eğer kod Google Colab ortamında çalışıyorsa, Google Drive'ı bağlar ve\n","    kök dizini Google Drive içindeki \"turkish_gpt2_finetuning\" klasörü olarak ayarlar.\n","    \"\"\"\n","    from google.colab import drive\n","    drive.mount('/content/drive')  # Google Drive'ı Colab ortamına bağlar\n","    kok_dizin = \"/content/drive/MyDrive/turkish_gpt2_finetuning\"  # Drive içindeki çalışma klasörünü belirler\n","else:\n","    \"\"\"\n","    Eğer kod yerel bir ortamda çalışıyorsa, kök dizini mevcut çalışma dizini olarak ayarlar.\n","    \"\"\"\n","    kok_dizin = os.getcwd()  # Mevcut çalışma dizinini alır\n","\n","# Belirlenen kök dizini kullanıcıya bilgi olarak gösterir\n","print(f\"Kök dizin: {kok_dizin}\\n Not: eğer colab kullanıyorsanız, dizini değiştirmeniz gerekir.\")"],"metadata":{"id":"XeFEFuO2N_Qz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Veri kümesi biçimlendirme, yol ve model yolu tanımlamaları"],"metadata":{"id":"STt93S85Yyxx"}},{"cell_type":"code","source":["# Veri kümesi yolu ve karıştırma seed değeri tanımlamaları\n","veri_kumesi_yolu = os.path.join(kok_dizin, \"soru_cevap.csv\")\n","veri_kumesi_karistirma = 571  # Karıştırmada kullanılacak sabit seed değeri\n","\n","# Model kaydetme dizinleri\n","# gpt4o verisiyle eğitilmiş modeller\n","gpt2_medium_kaydetme_dizini_gpt4o = os.path.join(kok_dizin, \"gpt2_medium_gpt4o\")\n","gpt2_large_kaydetme_dizini_gpt4o = os.path.join(kok_dizin, \"gpt2_large_gpt4o\")\n","\n","# deepseek verisiyle eğitilmiş modeller\n","gpt2_medium_kaydetme_dizini_deepseek = os.path.join(kok_dizin, \"gpt2_medium_deepseek\")\n","gpt2_large_kaydetme_dizini_deepseek = os.path.join(kok_dizin, \"gpt2_large_deepseek\")\n","\n","# Model adı tanımlamaları\n","gpt2_medium_model_adi = \"ytu-ce-cosmos/turkish-gpt2-medium\"\n","gpt2_large_model_adi = \"ytu-ce-cosmos/turkish-gpt2-large\"\n","\n","# Veri kümesi sütun tanımlamaları\n","soru_sutunu = \"Soru\"\n","gpt4o_cevap_sutunu = \"gpt4o cevabı\"\n","deepseek_cevap_sutunu = \"deepseek cevabı\"\n","\n","# Özel token tanımlamaları (eğitim formatı için)\n","soru_baslangic = \"<SORU>\"\n","soru_bitis = \"</SORU>\"\n","cevap_baslangic = \"<CEVAP>\"\n","cevap_bitis = \"</CEVAP>\"\n","ornek_bitis = \"<|endoftext|>\"  # GPT-2'nin EOS tokeni\n","\n","# Tokenizer'a eklenecek özel tokenler listesi\n","ozel_tokenler = [soru_baslangic, soru_bitis, cevap_baslangic, cevap_bitis]"],"metadata":{"id":"EeKc_832OR8P"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Veri kümesi okuma"],"metadata":{"id":"gwpuWbgSPQtI"}},{"cell_type":"code","source":["# CSV dosyasını okuma\n","try:\n","    # CSV dosyasını okuyoruz\n","    df = pd.read_csv(veri_kumesi_yolu)\n","\n","    # İstenen sütunların varlığını kontrol ediyoruz\n","    gereken_sutunlar = [\"Soru\", gpt4o_cevap_sutunu, deepseek_cevap_sutunu]\n","\n","    for sutun in gereken_sutunlar:\n","        if sutun not in df.columns:\n","            print(f\"'{sutun}' sütunu veri kümesinde bulunamadı!\")\n","\n","    # İstenen sütunları ayıklama\n","    soru_cevap_df = df[gereken_sutunlar]\n","\n","    # Doğrulama için ilk birkaç satırı görüntüleme\n","    print(\"Veri kümesinin ilk birkaç satırı:\")\n","    print(soru_cevap_df.head(5))\n","\n","    print(f\"\\nToplam {len(soru_cevap_df)} soru-cevap çifti bulundu.\")\n","\n","except FileNotFoundError:\n","    print(f\"'{veri_kumesi_yolu}' dosyası bulunamadı. Lütfen dosya yolunu kontrol ediniz.\")\n","except Exception as e:\n","    print(f\"Veri okuma hatası: {e}\")"],"metadata":{"id":"rZnjjD0oPBjo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Veri kümesini belirtilen seed değeriyle karıştırma\n","shuffled_df = soru_cevap_df.sample(frac=1, random_state=veri_kumesi_karistirma)\n","\n","# İndeksleri sıfırlama\n","shuffled_df = shuffled_df.reset_index(drop=True)\n","\n","# Karıştırılmış veri kümesinin ilk birkaç satırını gösterme\n","print(\"Karıştırılmış veri kümesinin ilk birkaç satırı:\")\n","print(shuffled_df.head(5))\n","\n","print(f\"\\nKarıştırılmış veri kümesi boyutu: {len(shuffled_df)} satır\")"],"metadata":{"id":"HILJV27GPiIc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Veri kümesi işleme"],"metadata":{"id":"JXG9nHCqZSFT"}},{"cell_type":"markdown","source":["## Veri kümesi biçimlendirme fonksiyonu"],"metadata":{"id":"eWMjPMiuW44_"}},{"cell_type":"code","source":["def veri_kumesini_egitim_formatina_donustur(df: pd.DataFrame, cevap_sutunu: str,\n","                                            soru_baslangic: str = soru_baslangic,\n","                                            soru_bitis: str = soru_bitis,\n","                                            cevap_baslangic: str = cevap_baslangic,\n","                                            cevap_bitis: str = cevap_bitis,\n","                                            ornek_bitis: str = ornek_bitis) -> list:\n","    \"\"\"\n","    Veri kümesindeki soru ve cevapları GPT-2 eğitimi için uygun formata dönüştürür.\n","    Metinsel formatta örnekler döndürür, tokenize işlemi uygulamaz.\n","\n","    Args:\n","        df: Soru ve cevapları içeren DataFrame\n","        cevap_sutunu: Cevap metinlerini içeren sütunun adı\n","        soru_baslangic: Soru başlangıç etiketi\n","        soru_bitis: Soru bitiş etiketi\n","        cevap_baslangic: Cevap başlangıç etiketi\n","        cevap_bitis: Cevap bitiş etiketi\n","        ornek_bitis: Her örneğin sonunu belirten etiket\n","\n","    Returns:\n","        list: Eğitim için hazırlanmış metinsel örnekler listesi\n","    \"\"\"\n","\n","    egitim_metinleri = []\n","\n","    # Her bir soru-cevap çifti için formatlı metinler oluştur\n","    for _, satir in df.iterrows():\n","        soru = satir[\"Soru\"].strip()\n","        cevap = satir[cevap_sutunu].strip()\n","\n","        # Soru ve cevabı belirli bir formatta birleştir\n","        bicimlendirilmis_metin = f\"{soru_baslangic} {soru} {soru_bitis} {cevap_baslangic} {cevap} {cevap_bitis}{ornek_bitis}\"\n","\n","        egitim_metinleri.append(bicimlendirilmis_metin)\n","\n","    print(f\"Toplam {len(egitim_metinleri)} adet eğitim örneği oluşturuldu.\")\n","    print(f\"Örnek biçimi: {soru_baslangic} [Soru] {soru_bitis} {cevap_baslangic} [Cevap] {cevap_bitis}{ornek_bitis}\")\n","    return egitim_metinleri"],"metadata":{"id":"JWeG9G8pURiv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Veri kümelerini işle ve oluştur"],"metadata":{"id":"MfiIVCNKh28b"}},{"cell_type":"code","source":["veri_kumesi_gpt4o = veri_kumesini_egitim_formatina_donustur(shuffled_df, gpt4o_cevap_sutunu)\n","veri_kumesi_deepseek = veri_kumesini_egitim_formatina_donustur(shuffled_df, deepseek_cevap_sutunu)"],"metadata":{"id":"5GskjGIMZTnE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Eğitim işlemleri"],"metadata":{"id":"US-pveshRaVV"}},{"cell_type":"markdown","source":["## Model Yükleme ve Eğitme Fonksiyonları"],"metadata":{"id":"HSNOJdatXExf"}},{"cell_type":"markdown","source":["### Eğitim argümanları oluşturma fonksiyonu"],"metadata":{"id":"vN_BKyEaiBQV"}},{"cell_type":"code","source":["# Eğitim parametreleri\n","max_seq_length = 1024  # Uzun metinler için artırılabilir\n","max_prompt_length = 256  # Soru için maksimum uzunluk\n","\n","def training_arguments_getir(\n","    kaydetme_dizin: str,\n","    learning_rate: float = 5e-6,\n","    batch_size: int = 1,\n","    grad_accum_steps: int = 16,\n","    num_gens: int = 6,\n","    max_steps: int = 250,\n","    epochs: int = None\n",") -> GRPOConfig:\n","    \"\"\"\n","    GRPO (Generative Reinforced Policy Optimization) eğitim konfigürasyonu oluşturur.\n","\n","    Args:\n","        kaydetme_dizin: Model ve checkpoint'lerin kaydedileceği dizin\n","        learning_rate: Öğrenme oranı\n","        batch_size: Cihaz başına eğitim batch büyüklüğü\n","        grad_accum_steps: Gradyan biriktirme adımları\n","        num_gens: Bellek kullanımını etkileyen üretim sayısı\n","        max_steps: Maksimum eğitim adım sayısı\n","        epochs: Eğitim döngü (epoch) sayısı, None ise max_steps kullanılır\n","\n","    Returns:\n","        GRPOConfig: Eğitim için yapılandırma nesnesi\n","    \"\"\"\n","    # GRPO eğitim konfigürasyonu\n","    training_args = GRPOConfig(\n","        learning_rate=learning_rate,\n","        adam_beta1=0.9,\n","        adam_beta2=0.99,\n","        weight_decay=0.1,\n","        warmup_ratio=0.1,\n","        lr_scheduler_type=\"cosine\",\n","        optim=\"paged_adamw_8bit\",\n","        logging_steps=1,\n","        per_device_train_batch_size=batch_size,\n","        gradient_accumulation_steps=grad_accum_steps,\n","        num_generations=num_gens,\n","        max_prompt_length=max_prompt_length,\n","        max_completion_length=max_seq_length - max_prompt_length,\n","        num_train_epochs=epochs,  # None ise max_steps kullanılır\n","        max_steps=max_steps if epochs is None else -1,\n","        save_steps=max_steps // 2,  # Daha sık kaydetmek için ayarlanabilir\n","        max_grad_norm=0.1,\n","        report_to=\"none\",  # Metrics raporlama için \"wandb\" kullanılabilir\n","        output_dir=kaydetme_dizin,\n","    )\n","\n","    return training_args  # Oluşturulan konfigürasyon nesnesini döndür"],"metadata":{"id":"R4fThwcfZ8mR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Model Yükleme fonksiyonu"],"metadata":{"id":"21jgGqeniGM9"}},{"cell_type":"code","source":["def model_ve_tokenizer_yukle(\n","    model_adi: str,\n","    max_seq_length: int = max_seq_length,\n","    lora_rank: int = 32,\n","    random_state: int = 571,  # veri_kumesi_karistirma değişkeni burada tanımlı olmayabilir\n","    target_modules: list = [\"c_attn\", \"c_proj\", \"c_fc\"],\n","    ozel_tokenler: list = ozel_tokenler,\n","):\n","    \"\"\"\n","    Unsloth kullanarak bir dil modelini yükler ve LoRA için hazırlar.\n","\n","    Args:\n","        model_adi: Kullanılacak modelin adı (örn. \"ytu-ce-cosmos/turkish-gpt2-large\")\n","        max_seq_length: Maximum sekans uzunluğu\n","        lora_rank: LoRA rank değeri\n","        random_state: Rastgele seed değeri\n","        target_modules: LoRA için hedeflenecek modül katmanları\n","\n","    Returns:\n","        tuple: (model, tokenizer) çifti\n","    \"\"\"\n","    model, tokenizer = FastLanguageModel.from_pretrained(\n","        model_name = model_adi,\n","        max_seq_length = max_seq_length,\n","        load_in_4bit = False,  # 16-bit için False\n","        fast_inference = True,\n","        max_lora_rank = lora_rank,\n","        gpu_memory_utilization = 0.6,\n","    )\n","\n","    model = FastLanguageModel.get_peft_model(\n","        model,\n","        r = lora_rank,\n","        target_modules = target_modules,\n","        lora_alpha = lora_rank,\n","        use_gradient_checkpointing = \"unsloth\",\n","        random_state = random_state,\n","    )\n","    tokenizer.add_tokens(ozel_tokenler)\n","    model.resize_token_embeddings(len(tokenizer))\n","    print(f\"{model_adi} modeli Unsloth ile başarıyla yüklendi ve LoRA için hazırlandı.\")\n","    return model, tokenizer"],"metadata":{"id":"yA7f5Bq3Rjcl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Eğitici Getirme fonksiyonu"],"metadata":{"id":"hVl_03-EiI49"}},{"cell_type":"code","source":["def trainer_getir(\n","    model,\n","    tokenizer,\n","    training_args: Optional[GRPOConfig],\n","    veri_kumesi: list,\n","    egitim_batch_boyutu: int = 1\n",") -> GRPOTrainer:\n","    \"\"\"\n","    Model, tokenizer ve eğitim argümanlarını kullanarak GRPOTrainer oluşturur.\n","\n","    Args:\n","        model: Eğitilecek dil modeli\n","        tokenizer: Modelle kullanılacak tokenizer\n","        training_args: GRPO eğitim konfigürasyonu, None ise varsayılan oluşturulur\n","        veri_kumesi: Eğitim için hazırlanmış metinsel örnekler listesi\n","        egitim_batch_boyutu: Eğitim için batch boyutu\n","\n","    Returns:\n","        GRPOTrainer: Eğitimi yürütecek trainer nesnesi\n","    \"\"\"\n","    # Eğer training_args belirtilmemişse, varsayılan değerlerle oluştur\n","    if training_args is None:\n","        training_args = training_arguments_getir(\n","            kaydetme_dizin=\"./model_cikti\",\n","            batch_size=egitim_batch_boyutu\n","        )\n","\n","    # Metin verilerini tokenize et ve eğitim için hazırla\n","    def tokenize_function(examples):\n","        # Tek bir metin listesi olduğu için özel bir işleme\n","        tokenized = tokenizer(\n","            examples,\n","            padding=\"max_length\",\n","            truncation=True,\n","            max_length=training_args.max_prompt_length + training_args.max_completion_length,\n","            return_tensors=\"pt\"\n","        )\n","        return tokenized\n","\n","    # Metinsel verileri tokenize et\n","    tokenized_dataset = tokenize_function(veri_kumesi)\n","\n","    # Dataset oluşturma\n","    from torch.utils.data import Dataset\n","\n","    class TextDataset(Dataset):\n","        def __init__(self, tokenized_texts):\n","            self.input_ids = tokenized_texts[\"input_ids\"]\n","            self.attention_mask = tokenized_texts[\"attention_mask\"]\n","\n","        def __len__(self):\n","            return len(self.input_ids)\n","\n","        def __getitem__(self, idx):\n","            # GRPO için input_ids ve attention_mask\n","            sample = {\n","                \"input_ids\": self.input_ids[idx],\n","                \"attention_mask\": self.attention_mask[idx],\n","            }\n","            return sample\n","\n","    # Dataset oluştur\n","    train_dataset = TextDataset(tokenized_dataset)\n","\n","    print(f\"Eğitim için {len(train_dataset)} örnek hazırlandı.\")\n","\n","    # GRPOTrainer oluştur\n","    trainer = GRPOTrainer(\n","        model=model,\n","        tokenizer=tokenizer,\n","        args=training_args,\n","        train_dataset=train_dataset,\n","    )\n","\n","    return trainer"],"metadata":{"id":"iPD3wCILaJez"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Veri kümesi özelinde model eğitme fonksiyonu"],"metadata":{"id":"LzxRcRlPiLPV"}},{"cell_type":"code","source":["def model_egit_ve_kaydet(\n","    model,\n","    tokenizer,\n","    veri_kumesi,\n","    model_kaydetme_dizini,\n","):\n","    \"\"\"\n","    Modeli belirtilen veri kümesi üzerinde eğitir ve kaydeder.\n","\n","    Args:\n","        model: Eğitilecek model\n","        tokenizer: Modelin tokenizer'ı\n","        veri_kumesi: Eğitim veri kümesi listesi\n","        model_kaydetme_dizini: Eğitilmiş modelin kaydedileceği dizin\n","\n","    Returns:\n","        Eğitilmiş model\n","    \"\"\"\n","    # Eğitim klasörünü oluştur\n","    os.makedirs(model_kaydetme_dizini, exist_ok=True)\n","\n","    # Eğitim argümanlarını oluştur\n","    training_args = training_arguments_getir(\n","        kaydetme_dizin=model_kaydetme_dizini,\n","    )\n","\n","    # Trainer oluştur\n","    trainer = trainer_getir(\n","        model=model,\n","        tokenizer=tokenizer,\n","        training_args=training_args,\n","        veri_kumesi=veri_kumesi,\n","    )\n","\n","    # Eğitimi başlat\n","    print(f\"Model eğitimine başlanıyor: {model_kaydetme_dizini}\")\n","    trainer.train()\n","\n","    # Modelin tam versiyonunu kaydet\n","    model.save_pretrained(model_kaydetme_dizini)\n","    tokenizer.save_pretrained(model_kaydetme_dizini)\n","    print(f\"Model başarıyla kaydedildi: {model_kaydetme_dizini}\")\n","\n","    del model\n","    torch.cuda.empty_cache()"],"metadata":{"id":"-qiyNRlVgna4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## GPT-2 Medium modelini GPT-4o veri kümesi ile eğit"],"metadata":{"id":"Su5-t-l5XI6I"}},{"cell_type":"code","source":["# GPT-4o veri kümesiyle Medium modeli eğit\n","print(\"\\nGPT-2 Medium modeli GPT-4o veri kümesi ile eğitiliyor...\")\n","model_medium, tokenizer_medium = model_ve_tokenizer_yukle(gpt2_medium_model_adi)\n","model_egit_ve_kaydet(\n","    model_medium,\n","    tokenizer_medium,\n","    veri_kumesi_gpt4o,\n","    gpt2_medium_kaydetme_dizini_gpt4o,\n",")"],"metadata":{"id":"aoe3ddtxRb7Y"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## GPT-2 Medium modelini DeepSeek veri kümesi ile eğit"],"metadata":{"id":"ddx-uWtTiWvc"}},{"cell_type":"code","source":["# DeepSeek veri kümesiyle Medium modeli eğit\n","print(\"\\nGPT-2 Medium modeli DeepSeek veri kümesi ile eğitiliyor...\")\n","model_medium, tokenizer_medium = model_ve_tokenizer_yukle(gpt2_medium_model_adi)\n","model_egit_ve_kaydet(\n","    model_medium,\n","    tokenizer_medium,\n","    veri_kumesi_deepseek,\n","    gpt2_medium_kaydetme_dizini_deepseek,\n",")"],"metadata":{"id":"VuSP5gzVhKvX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## GPT-2 Large modelini GPT-4o veri kümesi ile eğit"],"metadata":{"id":"E5xacutTicNp"}},{"cell_type":"code","source":["# GPT-4o veri kümesiyle Large modeli eğit\n","print(\"\\nGPT-2 Large modeli GPT-4o veri kümesi ile eğitiliyor...\")\n","model_large, tokenizer_large = model_ve_tokenizer_yukle(gpt2_large_model_adi)\n","model_egit_ve_kaydet(\n","    model_large,\n","    tokenizer_large,\n","    veri_kumesi_gpt4o,\n","    gpt2_large_kaydetme_dizini_gpt4o,\n",")"],"metadata":{"id":"75Ql1WILhKpf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## GPT-2 Large modelini DeepSeek veri kümesi ile eğit"],"metadata":{"id":"Wwf2Sey5ie6E"}},{"cell_type":"code","source":["# DeepSeek veri kümesiyle Large modeli eğit\n","print(\"\\nGPT-2 Large modeli DeepSeek veri kümesi ile eğitiliyor...\")\n","model_large, tokenizer_large = model_ve_tokenizer_yukle(gpt2_large_model_adi)\n","model_egit_ve_kaydet(\n","    model_large,\n","    tokenizer_large,\n","    veri_kumesi_deepseek,\n","    gpt2_large_kaydetme_dizini_deepseek,\n",")"],"metadata":{"id":"z7u9qL0BhKeP"},"execution_count":null,"outputs":[]}]}
{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eFIbL_V3PIwT"
      },
      "source": [
        "# Giriş işlemleri"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vZonvfzWJ4MV"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install trl bitsandbytes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lLjbKqLWPMCs"
      },
      "source": [
        "## Kütüphane içe aktarma"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "whu-0JW1OV36"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import torch\n",
        "from trl import SFTTrainer, SFTConfig\n",
        "from datasets import Dataset\n",
        "from typing import List, Literal, Tuple, Any, Dict, Set\n",
        "from transformers import (AutoModelForCausalLM,\n",
        "                          AutoTokenizer,\n",
        "                          BitsAndBytesConfig, PreTrainedTokenizer)\n",
        "from peft import LoraConfig, get_peft_model\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AKfyD5oZN8k5"
      },
      "outputs": [],
      "source": [
        "def is_colab() -> bool:\n",
        "    \"\"\"\n",
        "    Google Colab ortamında çalışıp çalışmadığını kontrol eden fonksiyon.\n",
        "\n",
        "    Args:\n",
        "        None\n",
        "\n",
        "    Returns:\n",
        "        bool: Eğer kod Google Colab'da çalışıyorsa True, aksi halde False döndürür\n",
        "    \"\"\"\n",
        "    try:\n",
        "        import google.colab\n",
        "        return True\n",
        "    except ImportError:\n",
        "        return False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XeFEFuO2N_Qz",
        "outputId": "7e5621ca-f6b9-406e-f62c-96951e211170"
      },
      "outputs": [],
      "source": [
        "# Kök dizin belirleme\n",
        "if is_colab():\n",
        "    \"\"\"\n",
        "    Eğer kod Google Colab ortamında çalışıyorsa, Google Drive'ı bağlar ve\n",
        "    kök dizini Google Drive içindeki \"turkish_gpt2_finetuning\" klasörü olarak ayarlar.\n",
        "    \"\"\"\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')  # Google Drive'ı Colab ortamına bağlar\n",
        "    kok_dizin = \"/content/drive/MyDrive/turkish_gpt2_finetuning\"  # Drive içindeki çalışma klasörünü belirler\n",
        "else:\n",
        "    \"\"\"\n",
        "    Eğer kod yerel bir ortamda çalışıyorsa, kök dizini mevcut çalışma dizini olarak ayarlar.\n",
        "    \"\"\"\n",
        "    kok_dizin = os.getcwd()  # Mevcut çalışma dizinini alır\n",
        "\n",
        "# Belirlenen kök dizini kullanıcıya bilgi olarak gösterir\n",
        "print(f\"Kök dizin: {kok_dizin}\\n Not: eğer colab kullanıyorsanız, dizini değiştirmeniz gerekir.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "STt93S85Yyxx"
      },
      "source": [
        "## Veri kümesi biçimlendirme, yol ve model yolu tanımlamaları"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EeKc_832OR8P"
      },
      "outputs": [],
      "source": [
        "# Veri kümesi yolu ve karıştırma seed değeri tanımlamaları\n",
        "veri_kumesi_yolu = os.path.join(kok_dizin, \"soru_cevap.csv\")\n",
        "veri_kumesi_karistirma = 571  # Karıştırmada kullanılacak sabit seed değeri\n",
        "sonuc_dizini = os.path.join(kok_dizin, \"sonuclar\")  # Sonuçların kaydedileceği dizin\n",
        "\n",
        "# Model kaydetme dizinleri\n",
        "# gpt4o verisiyle eğitilmiş modeller\n",
        "gpt2_medium_kaydetme_dizini_gpt4o = os.path.join(kok_dizin, \"gpt2_medium_gpt4o\")\n",
        "gpt2_large_kaydetme_dizini_gpt4o = os.path.join(kok_dizin, \"gpt2_large_gpt4o\")\n",
        "\n",
        "# deepseek verisiyle eğitilmiş modeller\n",
        "gpt2_medium_kaydetme_dizini_deepseek = os.path.join(kok_dizin, \"gpt2_medium_deepseek\")\n",
        "gpt2_large_kaydetme_dizini_deepseek = os.path.join(kok_dizin, \"gpt2_large_deepseek\")\n",
        "\n",
        "# Model adı tanımlamaları\n",
        "gpt2_medium_model_adi = \"ytu-ce-cosmos/turkish-gpt2-medium\"\n",
        "gpt2_large_model_adi = \"ytu-ce-cosmos/turkish-gpt2-large\"\n",
        "\n",
        "# Veri kümesi sütun tanımlamaları\n",
        "soru_sutunu = \"Soru\"\n",
        "gpt4o_cevap_sutunu = \"gpt4o cevabı\"\n",
        "deepseek_cevap_sutunu = \"deepseek cevabı\"\n",
        "\n",
        "# Özel token tanımlamaları (eğitim formatı için)\n",
        "soru_baslangic = \"<SORU>\"\n",
        "soru_bitis = \"</SORU>\"\n",
        "cevap_baslangic = \"<CEVAP>\"\n",
        "cevap_bitis = \"</CEVAP>\"\n",
        "ornek_bitis = \"<|endoftext|>\"  # GPT-2'nin EOS tokeni\n",
        "\n",
        "# Tokenizer'a eklenecek özel tokenler listesi\n",
        "ozel_tokenler = [soru_baslangic, soru_bitis, cevap_baslangic, cevap_bitis]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gwpuWbgSPQtI"
      },
      "source": [
        "## Veri kümesi okuma"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rZnjjD0oPBjo",
        "outputId": "649a496f-9fb9-4141-f9be-ed76bdc1bb3d"
      },
      "outputs": [],
      "source": [
        "# -----------------------------------------------\n",
        "# CSV dosyasını okuma ve gerekli sütunları ayıklama\n",
        "# -----------------------------------------------\n",
        "\n",
        "try:\n",
        "    # -------------------------------------------------\n",
        "    # 1) CSV dosyasını diskteki yolundan (veri_kumesi_yolu)\n",
        "    #    pandas DataFrame olarak içeri aktar.\n",
        "    # -------------------------------------------------\n",
        "    df = pd.read_csv(veri_kumesi_yolu)\n",
        "\n",
        "    # -------------------------------------------------\n",
        "    # 2) Kullanıcıdan/konfigürasyondan gelen beklenen\n",
        "    #    sütun isimlerini bir liste hâlinde tanımla.\n",
        "    #\n",
        "    #    Burada:\n",
        "    #      - \"Soru\" : soruları içeren sütun\n",
        "    #      - gpt4o_cevap_sutunu : GPT-4o modelinin cevapları\n",
        "    #      - deepseek_cevap_sutunu : DeepSeek modelinin cevapları\n",
        "    # -------------------------------------------------\n",
        "    gereken_sutunlar = [\"Soru\", gpt4o_cevap_sutunu, deepseek_cevap_sutunu]\n",
        "\n",
        "    # -------------------------------------------------\n",
        "    # 3) Her bir gerekli sütunun DataFrame’de mevcut\n",
        "    #    olup olmadığını kontrol et. Eksikse uyarı ver.\n",
        "    # -------------------------------------------------\n",
        "    for sutun in gereken_sutunlar:\n",
        "        if sutun not in df.columns:\n",
        "            print(f\"'{sutun}' sütunu veri kümesinde bulunamadı!\")\n",
        "\n",
        "    # -------------------------------------------------\n",
        "    # 4) Sadece gerekli sütunları içeren yeni bir\n",
        "    #    DataFrame (soru_cevap_df) oluştur.\n",
        "    # -------------------------------------------------\n",
        "    soru_cevap_df = df[gereken_sutunlar]\n",
        "\n",
        "    # -------------------------------------------------\n",
        "    # 5) Doğrulama amacıyla ilk 5 satırı ekrana yazdır.\n",
        "    # -------------------------------------------------\n",
        "    print(\"Veri kümesinin ilk birkaç satırı:\")\n",
        "    print(soru_cevap_df.head(5))\n",
        "\n",
        "    # -------------------------------------------------\n",
        "    # 6) Toplam kaç soru-cevap çifti olduğunu bildir.\n",
        "    # -------------------------------------------------\n",
        "    print(f\"\\nToplam {len(soru_cevap_df)} soru-cevap çifti bulundu.\")\n",
        "\n",
        "# -------------------------------------------------\n",
        "# 7) Dosya bulunamazsa kullanıcıyı bilgilendir.\n",
        "# -------------------------------------------------\n",
        "except FileNotFoundError:\n",
        "    print(f\"'{veri_kumesi_yolu}' dosyası bulunamadı. Lütfen dosya yolunu kontrol ediniz.\")\n",
        "\n",
        "# -------------------------------------------------\n",
        "# 8) Diğer tüm hataları genel başlıkta yakala\n",
        "#    ve mesajını göster.\n",
        "# -------------------------------------------------\n",
        "except Exception as e:\n",
        "    print(f\"Veri okuma hatası: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HILJV27GPiIc",
        "outputId": "ece2173c-c02f-4a8f-e243-bb0886bd6eb0"
      },
      "outputs": [],
      "source": [
        "# ----------------------------------------------\n",
        "# Veri kümesini karıştırma (shuffle) işlemi\n",
        "# ----------------------------------------------\n",
        "\n",
        "# 1) sample(frac=1) metodu:\n",
        "#    • frac=1 -> DataFrame’in tamamını örnekle, yani tüm satırları rastgele sıraya diz.\n",
        "#    • random_state=veri_kumesi_karistirma ->\n",
        "#      Aynı seed değeriyle her çalıştırmada aynı karışık sıralamayı elde\n",
        "#      etmeyi sağlar (deneysellik için tekrar üretilebilirlik) [[2]].\n",
        "shuffled_df = soru_cevap_df.sample(\n",
        "    frac=1,                     # Tüm satırları al\n",
        "    random_state=veri_kumesi_karistirma  # Sabit tohum (seed)\n",
        ")\n",
        "\n",
        "# ----------------------------------------------\n",
        "# İndeksleri sıfırlama\n",
        "# ----------------------------------------------\n",
        "# 2) Karıştırma sonrası eski indeksler anlamını yitirir.\n",
        "#    reset_index(drop=True) ile:\n",
        "#      • drop=True   -> Eski indeks sütunu DataFrame’e eklenmez,\n",
        "#                       doğrudan atılır.\n",
        "#      • Yeni indeksler 0..N-1 şeklinde oluşturulur.\n",
        "shuffled_df = shuffled_df.reset_index(drop=True)\n",
        "\n",
        "# ----------------------------------------------\n",
        "# Karıştırılmış veri kümesini gözlemleme\n",
        "# ----------------------------------------------\n",
        "# 3) İlk 5 satırı yazdırarak karıştırmanın\n",
        "#    başarıyla gerçekleştiğini doğrula.\n",
        "print(\"Karıştırılmış veri kümesinin ilk birkaç satırı:\")\n",
        "print(shuffled_df.head(5))\n",
        "\n",
        "# 4) Toplam satır sayısını ekrana yazdır.\n",
        "print(f\"\\nKarıştırılmış veri kümesi boyutu: {len(shuffled_df)} satır\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JXG9nHCqZSFT"
      },
      "source": [
        "# Veri kümesi işleme"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eWMjPMiuW44_"
      },
      "source": [
        "## Veri kümesi biçimlendirme fonksiyonu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JWeG9G8pURiv"
      },
      "outputs": [],
      "source": [
        "def veri_kumesini_egitim_formatina_donustur(\n",
        "    df              : pd.DataFrame,   # Soru–cevap verilerini tutan pandas DataFrame\n",
        "    cevap_sutunu    : str,            # Hangi sütunun “cevap” olarak kullanılacağını belirtir\n",
        "    *,\n",
        "    soru_baslangic  : str = \"<SORU>\",      # Soru başlangıcını işaretleyen özel etiket\n",
        "    soru_bitis      : str = \"</SORU>\",     # Soru bitişini   işaretleyen özel etiket\n",
        "    cevap_baslangic : str = \"<CEVAP>\",     # Cevap başlangıcını işaretleyen özel etiket\n",
        "    cevap_bitis     : str = \"</CEVAP>\",    # Cevap bitişini   işaretleyen özel etiket\n",
        "    ornek_bitis     : str = \"<|endoftext|>\"# Her örneğin sonunda yer alacak EOS biçimindeki etiket\n",
        ") -> List[str]:\n",
        "    \"\"\"\n",
        "    Soru–cevap veri kümesini GPT-2 eğitimine uygun tekil metin dizilerine dönüştürür.\n",
        "\n",
        "    Fonksiyon, her satır (soru-cevap çifti) için aşağıdaki genel formata sahip\n",
        "    birleştirilmiş bir string üretir ve çıkan tüm örnekleri liste hâlinde döndürür:\n",
        "\n",
        "        <SORU>  {soru_metni}  </SORU>  <CEVAP>  {cevap_metni}  </CEVAP><|endoftext|>\n",
        "\n",
        "    NOT: Yalnızca metin birleştirmesi yapar, herhangi bir tokenizasyon işlemi uygulamaz.\n",
        "\n",
        "    Parametreler\n",
        "    ------------\n",
        "    df : pandas.DataFrame\n",
        "        “Soru” ve istenen cevap sütununu içeren veri çerçevesi.\n",
        "    cevap_sutunu : str\n",
        "        Cevapların bulunduğu sütun adı (örn. \"gpt4o cevabı\").\n",
        "    soru_baslangic : str, varsayılan \"<SORU>\"\n",
        "        Soru başlangıcı için kullanılacak özel belirteç.\n",
        "    soru_bitis : str, varsayılan \"</SORU>\"\n",
        "        Soru bitişi   için kullanılacak özel belirteç.\n",
        "    cevap_baslangic : str, varsayılan \"<CEVAP>\"\n",
        "        Cevap başlangıcı için kullanılacak özel belirteç.\n",
        "    cevap_bitis : str, varsayılan \"</CEVAP>\"\n",
        "        Cevap bitişi   için kullanılacak özel belirteç.\n",
        "    ornek_bitis : str, varsayılan \"<|endoftext|>\"\n",
        "        Her örneğin sonunda eklenecek bitiş/EOS belirteci.\n",
        "\n",
        "    Dönüş Değeri\n",
        "    ------------\n",
        "    List[str]\n",
        "        Eğitimde doğrudan kullanılmaya hazır, “birleştirilmiş” metin dizilerinin listesi.\n",
        "    \"\"\"\n",
        "\n",
        "    # ---------------------------------------------------------------------------- #\n",
        "    # 1) Çıktıları tutmak için boş bir liste oluşturuyoruz.                        #\n",
        "    #    Her iterasyonda, yeni biçimlendirilmiş metin bu listeye eklenecek.        #\n",
        "    # ---------------------------------------------------------------------------- #\n",
        "    egitim_metinleri: List[str] = []\n",
        "\n",
        "    # ---------------------------------------------------------------------------- #\n",
        "    # 2) DataFrame'deki her satır (index, satir) ikilisi üzerinde döngü kuruyoruz. #\n",
        "    #    • _      : index değeri (kullanılmadığı için alt tire)                    #\n",
        "    #    • satir  : pandas.Series, o anki satıra ait veriler                       #\n",
        "    # ---------------------------------------------------------------------------- #\n",
        "    for _, satir in df.iterrows():\n",
        "\n",
        "        # Satırdan “Soru” sütununu alıp baştaki/sondaki boşlukları temizliyoruz.\n",
        "        soru: str = satir[\"Soru\"].strip()\n",
        "\n",
        "        # Aynı işlemi cevap sütunu için de yapıyoruz (gönderilen isim dinamik).\n",
        "        cevap: str = satir[cevap_sutunu].strip()\n",
        "\n",
        "        # -------------------------------------------------------------------- #\n",
        "        # 3) Soru ve cevabı istenen etiketlerle sarmalayıp tek bir string      #\n",
        "        #    oluşturuyoruz. Yapı:                                              #\n",
        "        #      <SORU> Soru </SORU> <CEVAP> Cevap </CEVAP><|endoftext|>         #\n",
        "        # -------------------------------------------------------------------- #\n",
        "        bicimlendirilmis_metin: str = (\n",
        "            f\"{soru_baslangic} {soru} {soru_bitis} \"\n",
        "            f\"{cevap_baslangic} {cevap} {cevap_bitis}\"\n",
        "            f\"{ornek_bitis}\"\n",
        "        )\n",
        "\n",
        "        # 4) Elde edilen string’i çıktı listesine ekliyoruz.\n",
        "        egitim_metinleri.append(bicimlendirilmis_metin)\n",
        "\n",
        "    # ---------------------------------------------------------------------------- #\n",
        "    # 5) Özet bilgi: toplam kaç örnek üretildiğini ve şablonun nasıl göründüğünü   #\n",
        "    #    kullanıcıya yazdırıyoruz.                                                #\n",
        "    # ---------------------------------------------------------------------------- #\n",
        "    print(f\"Toplam {len(egitim_metinleri)} adet eğitim örneği oluşturuldu.\")\n",
        "    print(\n",
        "        f\"Örnek biçimi: \"\n",
        "        f\"{soru_baslangic} [Soru] {soru_bitis} \"\n",
        "        f\"{cevap_baslangic} [Cevap] {cevap_bitis}{ornek_bitis}\"\n",
        "    )\n",
        "\n",
        "    # 6) Hazırlanan tüm metinleri geri döndürüyoruz.\n",
        "    return egitim_metinleri"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dNb7dIqM1TpU"
      },
      "source": [
        "## PAD gibi özel tokenlar loss hesabına katılmasın sınıfı"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L3O0BIswxR2P"
      },
      "outputs": [],
      "source": [
        "from torch import Tensor\n",
        "from transformers import PreTrainedTokenizerBase\n",
        "class QACollator:\n",
        "    \"\"\"\n",
        "    Soru-cevap (QA) görevleri için “collate_fn” (toplayıcı) sınıfı.\n",
        "\n",
        "    Temel Görev\n",
        "    -----------\n",
        "    1. `input_ids` üzerinden bir `labels` tensörü üretir.\n",
        "    2. Aşağıdaki konumları **-100** ile maskeleyerek (ignore_index)\n",
        "       kayıp hesaplamasına (loss) dahil edilmemesini sağlar:\n",
        "         • Padding konumları (`attention_mask == 0`)\n",
        "         • Kullanıcı tarafından verilen özel token’lar\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        tokenizer: PreTrainedTokenizerBase,\n",
        "        ozel_tokenler: List[str],\n",
        "    ) -> None:\n",
        "        \"\"\"\n",
        "        QACollator kurucu metodu.\n",
        "\n",
        "        Args:\n",
        "            tokenizer (PreTrainedTokenizerBase):\n",
        "                HuggingFace biçimindeki tokenizer.\n",
        "                `convert_tokens_to_ids` metodu ile string token'ları tamsayı\n",
        "                ID'lere dönüştürmekte kullanılır.\n",
        "            ozel_tokenler (List[str]):\n",
        "                Kayıp fonksiyonundan hariç tutulacak, kullanıcı tanımlı\n",
        "                özel string token listesi (örn. <SORU>, </SORU> vb.).\n",
        "\n",
        "        Returns:\n",
        "            None\n",
        "        \"\"\"\n",
        "\n",
        "        # Tokenizer nesnesini sakla\n",
        "        self.tokenizer: PreTrainedTokenizerBase = tokenizer\n",
        "\n",
        "        # Özel token’ları ID’ye çevirip sete aktar.\n",
        "        # Set kullanımı O(1) üyelik sorgusu sağlar.\n",
        "        self.ozel_token_id_set: Set[int] = {\n",
        "            tokenizer.convert_tokens_to_ids(tok) for tok in ozel_tokenler\n",
        "        }\n",
        "\n",
        "    # Collate sınıfları __call__ ile DataLoader içinde fonksiyon gibi kullanılır\n",
        "    def __call__(\n",
        "        self,\n",
        "        features: List[Dict[str, Tensor]]\n",
        "    ) -> Dict[str, Tensor]:\n",
        "        \"\"\"\n",
        "        DataLoader'ın her minibatch adımında çağrılır.\n",
        "\n",
        "        İş Akışı\n",
        "        --------\n",
        "        1. Verilen `features` listesindeki her sözlüğü `torch.stack` ile\n",
        "           birleştirerek \"batched\" tensörler üretir.\n",
        "        2. Aynı boyutlu bir `labels` tensörü oluşturmak için\n",
        "           `input_ids` tensörünü **klonlar**.\n",
        "        3. İki aşamalı maskeleme yapar:\n",
        "           (a) `attention_mask == 0` olan (padding) konumları\n",
        "           (b) `ozel_token_id_set` içinde yer alan token ID'lerini\n",
        "           `-100` değeriyle işaretler.\n",
        "        4. Son olarak `labels` anahtarını batch sözlüğüne ekler.\n",
        "\n",
        "        Args:\n",
        "            features (List[Dict[str, Tensor]]):\n",
        "                Dataset'ten gelen örnekleri temsil eden sözlük listesi.\n",
        "                Tipik anahtarlar: \"input_ids\", \"attention_mask\",\n",
        "                isteğe bağlı olarak \"token_type_ids\" vb.\n",
        "\n",
        "        Returns:\n",
        "            Dict[str, Tensor]:\n",
        "                Modelin `forward` metoduna doğrudan verilebilecek,\n",
        "                ek olarak `labels` içeren batched tensör sözlüğü.\n",
        "        \"\"\"\n",
        "\n",
        "        # --------------- 1) Torch stack ile batch oluşturma ------------------\n",
        "        # Her anahtar için sıra ile tüm feature'lar toplanır.\n",
        "        # Örn. \"input_ids\": [T1, T2, ...]  ->  stack([T1, T2, ...])\n",
        "        batch: Dict[str, Tensor] = {\n",
        "            key: torch.stack(\n",
        "                [torch.as_tensor(f[key], dtype=torch.long) for f in features], dim=0\n",
        "            )\n",
        "            for key in features[0].keys()\n",
        "        }\n",
        "\n",
        "\n",
        "        # --------------- 2) Labels tensörünün oluşturulması ------------------\n",
        "        # `clone()` kullanarak gradient paylaşımından kaçınılır.\n",
        "        labels: Tensor = batch[\"input_ids\"].clone()\n",
        "\n",
        "        # --------------- 3a) Padding konumlarının maskelenmesi ---------------\n",
        "        # attention_mask == 0  →  padding\n",
        "        labels[batch[\"attention_mask\"] == 0] = -100\n",
        "\n",
        "        # --------------- 3b) Özel token konumlarının maskelenmesi ------------\n",
        "        for tok_id in self.ozel_token_id_set:\n",
        "            labels[batch[\"input_ids\"] == tok_id] = -100\n",
        "\n",
        "        # --------------- 4) Sonuç sözlüğü ------------------------------------\n",
        "        batch[\"labels\"] = labels\n",
        "        return batch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RXyg7-8C1bgi"
      },
      "source": [
        "## Eklenen tokenların ağırlıkları güncellensin diye sınıf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CW4GwOb81aSR"
      },
      "outputs": [],
      "source": [
        "from torch.nn import Parameter\n",
        "class ExtraTokenAdapter(nn.Module):\n",
        "    \"\"\"\n",
        "    Mevcut (eski) bir gömme tablosunu (nn.Embedding) tamamen\n",
        "    **dondurur** (yani geriye dönük ve güncellemesiz hâle getirir) ve\n",
        "    sadece modele yeni eklenen token’lar (sözcük, alt-birim vb.)\n",
        "    için ayrı bir gömme tablosu (nn.Embedding) tanımlar.\n",
        "\n",
        "    Böylece:\n",
        "        • Var olan ağırlıklar değişmez – ince ayar sırasında\n",
        "          gradyan almazlar.\n",
        "        • Eklenen yeni satırlar (token’lar) eğitilebilir kalır.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    base_emb : torch.nn.Embedding\n",
        "        Dondurulacak, ön-eğitilmiş veya mevcut gömme tablosu.\n",
        "    num_new : int\n",
        "        Modele sonradan eklenen token sayısı.\n",
        "    dim : int\n",
        "        Gömme vektörlerinin boyutu (embedding_dim).\n",
        "\n",
        "    Notes\n",
        "    -----\n",
        "    Bu katman; `base_emb` ve `extra` adlı iki farklı gömme tablosunu\n",
        "    tek bir “birleşik” ağırlık matrisi gibi davranacak şekilde\n",
        "    birleştirir.  `self.weight` özelliği salt okunur (requires_grad=False)\n",
        "    bir `Parameter` olarak sunulur; böylece ağırlık paylaşımı (tying)\n",
        "    yapılırken ayrı bir gradyan hesaplanmaz.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 base_emb: nn.Embedding,\n",
        "                 num_new: int,\n",
        "                 dim: int) -> None:\n",
        "        \"\"\"\n",
        "        Parameters\n",
        "        ----------\n",
        "        base_emb : nn.Embedding\n",
        "            Donuk kalacak orijinal embedding tablosu\n",
        "            (ör. GPT-2'de model.transformer.wte).\n",
        "        num_new : int\n",
        "            Eklenecek yeni token sayısı.\n",
        "        dim : int\n",
        "            Embedding vektör boyutu (base_emb.embedding_dim ile aynı).\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        if num_new == 0:\n",
        "            # Ek token yoksa kendimizi doğrudan base_emb’e proxy yapabiliriz\n",
        "            self.forward = base_emb.forward      # type: ignore\n",
        "            self.weight  = base_emb.weight       # type: ignore\n",
        "            self.num_frozen = base_emb.num_embeddings\n",
        "            self.num_new = 0\n",
        "            return\n",
        "\n",
        "        # 1)  Eski tabloyu tamamen dondur\n",
        "        self.base_emb = base_emb\n",
        "        for p in self.base_emb.parameters():\n",
        "            p.requires_grad = False\n",
        "\n",
        "        self.num_frozen = self.base_emb.num_embeddings\n",
        "        self.num_new    = num_new\n",
        "        self.dim        = dim\n",
        "\n",
        "        # 2)  Yeni, eğitilebilir tablo\n",
        "        device = base_emb.weight.device\n",
        "        dtype = base_emb.weight.dtype\n",
        "        self.extra = nn.Embedding(num_new, dim, device=device, dtype=dtype)\n",
        "        nn.init.normal_(self.extra.weight, mean=0.0,\n",
        "                        std=self.base_emb.weight.data.std().item())\n",
        "\n",
        "        # 3)  Tek bir Parameter üret → HF tie_weights() ile tam uyum\n",
        "        concat = torch.cat(\n",
        "            [self.base_emb.weight.data,\n",
        "             self.extra.weight.data], dim=0\n",
        "        )\n",
        "        self.register_parameter(\"weight\",\n",
        "            nn.Parameter(concat, requires_grad=True)\n",
        "        )\n",
        "\n",
        "        # 4)  Gradyan kancası → eski satırlar için grad = 0\n",
        "        def _mask_frozen_rows(grad: torch.Tensor) -> torch.Tensor:\n",
        "            if grad is None:          # güvenlik\n",
        "                return grad\n",
        "            grad[:self.num_frozen].zero_()\n",
        "            return grad\n",
        "\n",
        "        self.weight.register_hook(_mask_frozen_rows)\n",
        "\n",
        "    # ---------------------------------------------------------------------\n",
        "    # İleri besleme / Forward\n",
        "    # ---------------------------------------------------------------------\n",
        "    def forward(self, input_ids: Tensor) -> Tensor:\n",
        "        \"\"\"\n",
        "        Girdi olarak gelen `input_ids` tensöründeki her bir indeksi,\n",
        "        doğru gömme tablosuna bakarak (lookup) karşılık gelen vektör\n",
        "        ile değiştirir.\n",
        "\n",
        "        İşlem adımları\n",
        "        ---------------\n",
        "        1. `input_ids` < `split` ise eski tablo kullanılır.\n",
        "        2. `input_ids` >= `split` ise yeni tablo (`extra`) kullanılır\n",
        "           ve indeks değeri `split` çıkarılarak kaydırma yapılır.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        input_ids : torch.Tensor\n",
        "            Şekli ( * , ) `[B, …]` olan indis tensörü.  Veri tipi\n",
        "            integral (int64) olmalıdır.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        torch.Tensor\n",
        "            Şekli `(*input_ids.shape, embedding_dim)` olan,\n",
        "            her indekse karşılık gelen gömme vektörlerini içeren\n",
        "            tensör.  Dtype ve cihaz (device) `base_emb.weight` ile\n",
        "            aynıdır.\n",
        "        \"\"\"\n",
        "        # --------------------------------------------------------------\n",
        "        # 1) Eski (donuk) ve yeni (eğitilebilir) tablo sınırını bul\n",
        "        # --------------------------------------------------------------\n",
        "        split: int = self.base_emb.num_embeddings  # eski tablo satır sayısı\n",
        "\n",
        "        # `base_mask`  True → eski tablo kullan,\n",
        "        #              False → yeni tablo kullan.\n",
        "        base_mask: Tensor = input_ids < split\n",
        "        # Tersi: base_mask False olduğunda burası True\n",
        "        extra_mask: Tensor = ~base_mask\n",
        "\n",
        "        # --------------------------------------------------------------\n",
        "        # 2) Çıktı tensörünü ilkin sıfırlarla oluştur\n",
        "        # --------------------------------------------------------------\n",
        "        out: Tensor = torch.zeros(\n",
        "            (*input_ids.shape, self.base_emb.embedding_dim),\n",
        "            dtype=self.base_emb.weight.dtype,\n",
        "            device=input_ids.device,\n",
        "        )\n",
        "\n",
        "        # --------------------------------------------------------------\n",
        "        # 3) Maske bazlı indeksleme ile değerleri yerleştir\n",
        "        # --------------------------------------------------------------\n",
        "        if base_mask.any():\n",
        "            # Eski tablodan indeksle\n",
        "            out[base_mask] = self.base_emb(\n",
        "                input_ids[base_mask]\n",
        "            )\n",
        "\n",
        "        if extra_mask.any():\n",
        "            # Yeni tablodan indeksle.\n",
        "            # İndeksleri kaydır:  (id - split) ∈ [0, num_new)\n",
        "            out[extra_mask] = self.extra(\n",
        "                input_ids[extra_mask] - split\n",
        "            )\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MfiIVCNKh28b"
      },
      "source": [
        "## Veri kümelerini işle ve oluştur"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5GskjGIMZTnE",
        "outputId": "78f63975-d643-4874-e4c0-545ee1e6662b"
      },
      "outputs": [],
      "source": [
        "veri_kumesi_gpt4o = veri_kumesini_egitim_formatina_donustur(shuffled_df, gpt4o_cevap_sutunu)\n",
        "veri_kumesi_deepseek = veri_kumesini_egitim_formatina_donustur(shuffled_df, deepseek_cevap_sutunu)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "US-pveshRaVV"
      },
      "source": [
        "# Eğitim işlemleri"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HSNOJdatXExf"
      },
      "source": [
        "## Model Yükleme ve Eğitme Fonksiyonları"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vN_BKyEaiBQV"
      },
      "source": [
        "### Eğitim argümanları oluşturma fonksiyonu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R4fThwcfZ8mR"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "max_seq_length = 1024   # üstte sabit tanım\n",
        "\n",
        "def training_arguments_getir(\n",
        "    kaydetme_dizin      : str,\n",
        "    veri_kumesi_uzunlugu: int,\n",
        "    *,\n",
        "    learning_rate : float = 1e-4,\n",
        "    batch_size    : int   = 2,\n",
        "    grad_accum    : int   = 8,\n",
        "    epochs        : int   = 2,\n",
        "    max_seq_length: int   = max_seq_length,\n",
        "    save_per_epoch: int   = 4,\n",
        "    log_per_epoch : int   = 12,\n",
        ") -> SFTConfig:\n",
        "    \"\"\"\n",
        "    TRL kütüphanesinin `SFTConfig` sınıfını kolayca oluşturmak için\n",
        "    öntanımlı hiper-parametrelerle donatılmış yardımcı fonksiyon.\n",
        "\n",
        "    Args:\n",
        "        kaydetme_dizin (str): Checkpoint ve log dosyalarının kaydedileceği dizin.\n",
        "        veri_kumesi_uzunlugu (int): Toplam örnek (satır) sayısı.\n",
        "        learning_rate (float, optional): Öğrenme oranı. Varsayılan 1e-4.\n",
        "        batch_size (int, optional): GPU başına batch büyüklüğü. Varsayılan 2.\n",
        "        grad_accum (int, optional): Gradient accumulation adım sayısı.\n",
        "        epochs (int, optional): Eğitim süresi (epoch). Varsayılan 1.\n",
        "        max_seq_length (int, optional): Girdi dizisi tokensel azami uzunluk.\n",
        "        save_per_epoch (int, optional): Bir epoch kaç parçaya bölünerek\n",
        "            checkpoint alınacağını belirler. Örn. 4 → epoch/4 adımda bir.\n",
        "        log_per_epoch (int, optional): Bir epoch kaç parçaya bölünerek\n",
        "            loss değeri loglanacağını belirler.\n",
        "    Returns:\n",
        "        SFTConfig: TRL/Trainer ile uyumlu, ön ayarlı konfigürasyon nesnesi.\n",
        "    \"\"\"\n",
        "\n",
        "    # ----------------- Türetilmiş değerleri hesapla ----------------------- #\n",
        "    ngpu: int = max(torch.cuda.device_count(), 1)         # GPU adedi\n",
        "    effective_batch: int = batch_size * grad_accum * ngpu # Toplam güncel batch\n",
        "    steps_per_epoch: int = math.ceil(veri_kumesi_uzunlugu / effective_batch)\n",
        "    max_steps: int        = epochs * steps_per_epoch\n",
        "    save_steps: int       = max(1, steps_per_epoch // save_per_epoch)\n",
        "    logging_steps: int    = max(1, steps_per_epoch // log_per_epoch)\n",
        "\n",
        "    print(\n",
        "        f\"[INFO] veri: {veri_kumesi_uzunlugu} örnek | gpu: {ngpu} | \"\n",
        "        f\"effective_batch: {effective_batch} | steps/epoch: {steps_per_epoch} | \"\n",
        "        f\"total steps: {max_steps}\"\n",
        "    )\n",
        "\n",
        "    # ----------------- SFTConfig için argüman sözlüğü --------------------- #\n",
        "    cfg_kwargs: Dict[str, Any] = dict(\n",
        "        output_dir = kaydetme_dizin,\n",
        "\n",
        "        # Temel hiper-parametreler\n",
        "        learning_rate               = learning_rate,\n",
        "        per_device_train_batch_size = batch_size,\n",
        "        gradient_accumulation_steps = grad_accum,\n",
        "\n",
        "        # Sekuans/eos ayarları\n",
        "        max_seq_length = max_seq_length,\n",
        "\n",
        "        # Optimizasyon\n",
        "        optim             = \"paged_adamw_8bit\",\n",
        "        weight_decay      = 0.01,\n",
        "        warmup_ratio      = 0.03,\n",
        "        lr_scheduler_type = \"linear\",\n",
        "        max_grad_norm     = 0.1,\n",
        "\n",
        "        # Loglama\n",
        "        logging_dir      = os.path.join(kaydetme_dizin, \"logs\"),\n",
        "        logging_strategy = \"steps\",\n",
        "        logging_steps    = logging_steps,\n",
        "        report_to        = \"tensorboard\",\n",
        "        run_name         = os.path.basename(kaydetme_dizin),\n",
        "\n",
        "        # Checkpoint\n",
        "        save_strategy = \"steps\",\n",
        "        save_steps    = save_steps,\n",
        "\n",
        "        # Epoch tabanlı çalışma\n",
        "        num_train_epochs = epochs,\n",
        "        max_steps        = -1,  # -1 → Trainer epoch modunda çalış\n",
        "    )\n",
        "\n",
        "    # Nesneyi oluşturup geri döndür\n",
        "    return SFTConfig(**cfg_kwargs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "21jgGqeniGM9"
      },
      "source": [
        "### Model Yükleme fonksiyonu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yA7f5Bq3Rjcl"
      },
      "outputs": [],
      "source": [
        "def model_ve_tokenizer_yukle(\n",
        "    model_adi      : str,\n",
        "    max_seq_length : int                           = max_seq_length,\n",
        "    lora_rank      : int                           = 8,\n",
        "    lora_alpha     : int                           = 16,\n",
        "    random_state   : int                           = veri_kumesi_karistirma,\n",
        "    target_modules : List[str]                     = (\"c_attn\", \"c_proj\", \"c_fc\"),\n",
        "    ozel_tokenler  : List[str]                     = ozel_tokenler,\n",
        "    quant_mode     : Literal[\"4bit\", \"8bit\", \"16bit\"] = \"16bit\",\n",
        ") -> Tuple[torch.nn.Module, PreTrainedTokenizer]:\n",
        "    \"\"\"\n",
        "    GPT-2 (ve türevleri) tabanlı bir modeli aşağıdaki özelliklerle RAM/GPU’ya\n",
        "    yükler:\n",
        "\n",
        "        • 4-bit QLoRA (nf4) ‑ 8-bit ‑ 16-bit (tam presizyon) çalıştırma kipi\n",
        "        • LoRA adaptörleri (PEFT) ekleme\n",
        "        • İsteğe bağlı yeni special token’lar ekleme\n",
        "        • Ağırlık paylaşımı (weight-tying) ve ExtraTokenAdapter ile genişletilmiş\n",
        "          gömme tablosu (embedding table) desteği\n",
        "        • Rastgelelik (seed) sabitleme\n",
        "\n",
        "    Args:\n",
        "        model_adi (str):\n",
        "            Hugging Face Hub’da barındırılan model ismi veya diskteki yol.\n",
        "        max_seq_length (int, optional):\n",
        "            `config.max_position_embeddings` alanını ezerek yeni bağlam\n",
        "            uzunluğu tanımlar.\n",
        "        lora_rank (int, optional):\n",
        "            LoRA faktörizasyon r (rank) değeri. Küçük r ⇒ daha az parametre.\n",
        "        lora_alpha (int, optional):\n",
        "            LoRA ölçekleme katsayısı (α). Genellikle 2×r ya da 4×r seçilir.\n",
        "        random_state (int, optional):\n",
        "            CUDA rastgele tohum değeri. Tekrar üretilebilirlik için önemlidir.\n",
        "        target_modules (List[str], optional):\n",
        "            LoRA eklenecek katman isimleri (ör. “c_attn”, “c_proj”…).\n",
        "        ozel_tokenler (List[str] | None, optional):\n",
        "            Tokenizer’a eklenecek yeni special token listesi.\n",
        "        quant_mode (Literal[\"4bit\", \"8bit\", \"16bit\"], optional):\n",
        "            Sayısal hassasiyet / sıkıştırma kipi.\n",
        "                - \"4bit\"  → QLoRA (nf4 + double-quant)\n",
        "                - \"8bit\"  → bitsandbytes 8-bit yükleme\n",
        "                - \"16bit\" → Tam presizyon (bf16 eğer GPU >= Ada / Ampere)\n",
        "    Returns:\n",
        "        Tuple[torch.nn.Module, PreTrainedTokenizer]:\n",
        "            (Eğitilebilir LoRA model nesnesi, buna uygun tokenizer)\n",
        "    \"\"\"\n",
        "    # ------------------------------------------------------------------ #\n",
        "    # 1) Tokenizer hazırlığı\n",
        "    # ------------------------------------------------------------------ #\n",
        "    # • Hugging Face tokenizer'ı hızlı (Rust tabanlı) versiyon ile yükle.\n",
        "    # • \"right\" padding + truncate: Causal LM’lerde geleneksel tercih.\n",
        "    tokenizer: PreTrainedTokenizer = AutoTokenizer.from_pretrained(\n",
        "        model_adi,\n",
        "        use_fast=True,\n",
        "    )\n",
        "    tokenizer.padding_side   = \"right\"\n",
        "    tokenizer.truncation_side = \"right\"\n",
        "\n",
        "    # Modelde pad_token yoksa => EOS token'ını aynı zamanda PAD olarak ata.\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.add_special_tokens({\"pad_token\": tokenizer.eos_token})\n",
        "\n",
        "    # ------------------------------------------------------------------ #\n",
        "    # 2) Quantization yapılandırması\n",
        "    # ------------------------------------------------------------------ #\n",
        "    quant_cfg: BitsAndBytesConfig | None = None          # bitsandbytes ayarları\n",
        "    model_kwargs: Dict[str, Any] = {                     # AutoModel argümanları\n",
        "        \"device_map\": \"auto\",            # katmanları GPU'ya otomatik saç\n",
        "        \"trust_remote_code\": True,       # repo 'modeling_xxx.py' kullanabilir\n",
        "    }\n",
        "\n",
        "    if quant_mode == \"4bit\":\n",
        "        # QLoRA için nf4 + double-quant ayarları\n",
        "        quant_cfg = BitsAndBytesConfig(\n",
        "            load_in_4bit=True,\n",
        "            bnb_4bit_use_double_quant=True,\n",
        "            bnb_4bit_quant_type=\"nf4\",\n",
        "            # CUDA 8.0 (Ampere) ve üstünde bf16 ile hesaplama daha hızlı/stabil\n",
        "            bnb_4bit_compute_dtype=(\n",
        "                torch.bfloat16 if torch.cuda.get_device_capability(0)[0] >= 8\n",
        "                else torch.float16\n",
        "            ),\n",
        "        )\n",
        "        model_kwargs[\"quantization_config\"] = quant_cfg\n",
        "\n",
        "    elif quant_mode == \"8bit\":\n",
        "        quant_cfg = BitsAndBytesConfig(load_in_8bit=True)\n",
        "        model_kwargs[\"quantization_config\"] = quant_cfg\n",
        "\n",
        "    else:  # \"16bit\"\n",
        "        # Yükleme dtype'ını belirle (bf16 destekliyse onu kullan)\n",
        "        model_kwargs[\"torch_dtype\"] = (\n",
        "            torch.bfloat16 if torch.cuda.get_device_capability(0)[0] >= 8\n",
        "            else torch.float16\n",
        "        )\n",
        "\n",
        "    # ------------------------------------------------------------------ #\n",
        "    # 3) Modeli indirme / belleğe yerleştirme\n",
        "    # ------------------------------------------------------------------ #\n",
        "    model: torch.nn.Module = AutoModelForCausalLM.from_pretrained(\n",
        "        model_adi,\n",
        "        **model_kwargs,\n",
        "    )\n",
        "\n",
        "    # Konfig alanlarını güncelle\n",
        "    model.config.max_position_embeddings = max_seq_length\n",
        "    model.config.use_cache = False                     # train sırasında gereksiz\n",
        "    model.gradient_checkpointing_enable()              # VRAM tasarrufu\n",
        "\n",
        "    # ------------------------------------------------------------------ #\n",
        "    # 4) LoRA adaptörlerini ekleme (PEFT)\n",
        "    # ------------------------------------------------------------------ #\n",
        "    lora_cfg: LoraConfig = LoraConfig(\n",
        "        r=lora_rank,\n",
        "        lora_alpha=lora_alpha,\n",
        "        target_modules=list(target_modules),\n",
        "        bias=\"none\",                   # LoRA sadece ağırlıklara uygula\n",
        "        task_type=\"CAUSAL_LM\",\n",
        "        fan_in_fan_out=True,           # GPT-J/GPT-Neo gibi modellerde önemli\n",
        "    )\n",
        "    model = get_peft_model(model, lora_cfg)\n",
        "    model.print_trainable_parameters()  # kaç parametre güncellenecek → stdout\n",
        "\n",
        "    # ------------------------------------------------------------------ #\n",
        "    # 5) İsteğe bağlı özel token ekleme\n",
        "    # ------------------------------------------------------------------ #\n",
        "    if ozel_tokenler:\n",
        "        # Tokenizer'a ekle\n",
        "        num_new = tokenizer.add_special_tokens({\"additional_special_tokens\": ozel_tokenler})\n",
        "\n",
        "        # GPT-2 mimarilerinde gömme tablosu transformer.wte'dir\n",
        "        model.transformer.wte = ExtraTokenAdapter(\n",
        "            base_emb = model.transformer.wte,           # yalnızca eski satırlar\n",
        "            num_new  = num_new,\n",
        "            dim      = model.config.n_embd,\n",
        "        )\n",
        "        model.config.vocab_size = len(tokenizer)\n",
        "        model.tie_weights()  # HF yardımcı çağrı\n",
        "\n",
        "    # ------------------------------------------------------------------ #\n",
        "    # 6) Rastgelelik sabitleme\n",
        "    # ------------------------------------------------------------------ #\n",
        "    torch.cuda.manual_seed(random_state)\n",
        "\n",
        "    # ------------------------------------------------------------------ #\n",
        "    # 7) Durum mesajı ve dönüş\n",
        "    # ------------------------------------------------------------------ #\n",
        "    print(f\"'{model_adi}' modeli '{quant_mode}' kipinde yüklendi (LoRA dâhil).\")\n",
        "\n",
        "    return model, tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hVl_03-EiI49"
      },
      "source": [
        "### Eğitici Getirme fonksiyonu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iPD3wCILaJez"
      },
      "outputs": [],
      "source": [
        "def trainer_getir(\n",
        "    model          : torch.nn.Module,\n",
        "    training_args  : SFTConfig,\n",
        "    veri_kumesi    : List[str],\n",
        "    tokenizer      : PreTrainedTokenizer\n",
        ") -> SFTTrainer:\n",
        "    \"\"\"\n",
        "    Metin listesini tokenize edip `SFTTrainer` için PyTorch\n",
        "    tensörlerine dönüştürür ve eğitici nesnesini oluşturur.\n",
        "\n",
        "    Args:\n",
        "        model (torch.nn.Module): PEFT (LoRA) ile zenginleştirilmiş GPT-2 modeli.\n",
        "        training_args (SFTConfig): `training_arguments_getir` çıktısı.\n",
        "        veri_kumesi (List[str]): Önceden formatlanmış eğitim metinleri listesi.\n",
        "        tokenizer (PreTrainedTokenizer): Aynı modele ait tokenizer.\n",
        "\n",
        "    Returns:\n",
        "        SFTTrainer: TRL eğitici nesnesi.\n",
        "    \"\"\"\n",
        "\n",
        "    # 1) Tokenizasyon → padding + truncation (max_length parametresi zaten var)\n",
        "    toks = tokenizer(\n",
        "        veri_kumesi,\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=training_args.max_seq_length,\n",
        "    )\n",
        "\n",
        "    # 2) Tensör formatına dönüştür\n",
        "    data_dict: Dict[str, List[List[int]]] = {\n",
        "        \"input_ids\":      toks[\"input_ids\"],\n",
        "        \"attention_mask\": toks[\"attention_mask\"],\n",
        "    }\n",
        "    train_dataset: Dataset = Dataset.from_dict(data_dict)\n",
        "    train_dataset.set_format(type=\"torch\")\n",
        "\n",
        "    print(f\"Eğitim için {len(train_dataset)} örnek hazırlandı.\")\n",
        "\n",
        "    # 3) Collator nesnesini oluştur (PAD + özel token maskesi)\n",
        "    collator = QACollator(\n",
        "        tokenizer      = tokenizer,\n",
        "        ozel_tokenler  = ozel_tokenler,   # global listeden geliyor\n",
        "    )\n",
        "    # 4) SFTTrainer oluştur\n",
        "    trainer: SFTTrainer = SFTTrainer(\n",
        "        model         = model,\n",
        "        args          = training_args,\n",
        "        train_dataset = train_dataset,\n",
        "        data_collator=  collator,\n",
        "    )\n",
        "    return trainer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LzxRcRlPiLPV"
      },
      "source": [
        "### Veri kümesi özelinde model eğitme fonksiyonu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-qiyNRlVgna4"
      },
      "outputs": [],
      "source": [
        "from transformers.trainer_utils import get_last_checkpoint\n",
        "import json\n",
        "\n",
        "def model_egit_ve_kaydet(\n",
        "    model                : torch.nn.Module,\n",
        "    tokenizer            : PreTrainedTokenizer,\n",
        "    veri_kumesi          : List[str],\n",
        "    model_kaydetme_dizini: str,\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    LoRA-adaptörlü modeli verilen veri kümesiyle eğitir ve çıktı/ checkpoint\n",
        "    dizinini yönetir. Eğer dizinde geçerli bir checkpoint varsa devam eder,\n",
        "    yoksa eğitimi sıfırdan başlatır.\n",
        "\n",
        "    Args:\n",
        "        model (torch.nn.Module): Eğitilecek model.\n",
        "        tokenizer (PreTrainedTokenizer): İlgili tokenizer.\n",
        "        veri_kumesi (List[str]): Eğitim örnekleri listesi.\n",
        "        model_kaydetme_dizini (str): Checkpoint / log dizini yolu.\n",
        "\n",
        "    Returns:\n",
        "        None\n",
        "    \"\"\"\n",
        "\n",
        "    # ------------------------------------------------------------------ #\n",
        "    # 0) Çıktı dizinini oluştur (varsa dokunma)                          #\n",
        "    # ------------------------------------------------------------------ #\n",
        "    os.makedirs(model_kaydetme_dizini, exist_ok=True)\n",
        "\n",
        "    # ------------------------------------------------------------------ #\n",
        "    # 1) Eğitim argümanlarını hazırla                                    #\n",
        "    # ------------------------------------------------------------------ #\n",
        "    training_args: SFTConfig = training_arguments_getir(\n",
        "        kaydetme_dizin       = model_kaydetme_dizini,\n",
        "        veri_kumesi_uzunlugu = len(veri_kumesi),\n",
        "    )\n",
        "\n",
        "    # ------------------------------------------------------------------ #\n",
        "    # 2) Trainer oluştur                                                #\n",
        "    # ------------------------------------------------------------------ #\n",
        "    trainer: SFTTrainer = trainer_getir(\n",
        "        model         = model,\n",
        "        training_args = training_args,\n",
        "        veri_kumesi   = veri_kumesi,\n",
        "        tokenizer     = tokenizer,\n",
        "    )\n",
        "\n",
        "    # ------------------------------------------------------------------ #\n",
        "    # 3) Checkpoint araması                                              #\n",
        "    #    transformers.trainer_utils.get_last_checkpoint()               #\n",
        "    #    - Geçerli bir checkpoint klasörü bulursa yolunu döndürür        #\n",
        "    #    - Hiç bulamazsa `None` döndürür                                 #\n",
        "    # ------------------------------------------------------------------ #\n",
        "    checkpoint_yolu: str | None = None\n",
        "    if os.path.isdir(model_kaydetme_dizini):\n",
        "        checkpoint_yolu = get_last_checkpoint(model_kaydetme_dizini)\n",
        "\n",
        "    if checkpoint_yolu:\n",
        "        print(f\"[Eğitim] Geçerli checkpoint bulundu: {checkpoint_yolu}\\n\"\n",
        "              f\"         Eğitime kaldığı yerden devam ediliyor...\")\n",
        "    else:\n",
        "        print(\"[Eğitim] Geçerli checkpoint bulunamadı. Eğitim sıfırdan başlıyor...\")\n",
        "\n",
        "    # ------------------------------------------------------------------ #\n",
        "    # 4) Eğitim döngüsü                                                  #\n",
        "    #    • Checkpoint varsa resume_from_checkpoint ile devam et          #\n",
        "    #    • Hata durumunda yakalayıp temiz bir başlangıca geç             #\n",
        "    # ------------------------------------------------------------------ #\n",
        "    try:\n",
        "        trainer.train(resume_from_checkpoint=checkpoint_yolu)\n",
        "    except (ValueError, FileNotFoundError) as e:\n",
        "        # Checkpoint bozuk / eksik dosya → temiz başlangıç\n",
        "        print(f\"[Uyarı] Checkpoint okunamadı ({e}). \"\n",
        "              f\"Eğitim sıfırdan tekrar başlatılıyor...\")\n",
        "        # Dilerseniz sorunlu checkpoint klasörünü silebilirsiniz:\n",
        "        trainer.train()  # sıfırdan\n",
        "\n",
        "    # ------------------------------------------------------------------ #\n",
        "    # 5) Model, tokenizer ve logları kaydet                              #\n",
        "    # ------------------------------------------------------------------ #\n",
        "    trainer.save_model(model_kaydetme_dizini)\n",
        "    tokenizer.save_pretrained(model_kaydetme_dizini)\n",
        "\n",
        "    log_json_path: str = os.path.join(model_kaydetme_dizini, \"training_log.json\")\n",
        "    with open(log_json_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(trainer.state.log_history, f, ensure_ascii=False, indent=4)\n",
        "\n",
        "    print(f\"[Eğitim] Model ve tokenizer '{model_kaydetme_dizini}' dizinine kaydedildi.\")\n",
        "\n",
        "    # ------------------------------------------------------------------ #\n",
        "    # 6) Bellek temizliği                                               #\n",
        "    # ------------------------------------------------------------------ #\n",
        "    del model\n",
        "    torch.cuda.empty_cache()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Su5-t-l5XI6I"
      },
      "source": [
        "## GPT-2 Medium modelini GPT-4o veri kümesi ile eğit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 284,
          "referenced_widgets": [
            "901cb4757c174232bc41d3d9f1af9e93",
            "7becf5f2e372431bada285816e087645",
            "960798f166384a5e91d5a33b551a53cc",
            "32a0f17621824e44ba5994423c6b60ba",
            "f60728b99fab46c5a5e275f0e812da04",
            "7b0c2278b2ff4fb4b277f813e30286cd",
            "55f9e29ec98943dd90f561eb1b8699cc",
            "c288a6ad1fab4367805d3f9f7ff4280b",
            "3240efab4c264d12a48a6a451e12ed25",
            "2ab1489f4c3e45e991fdf03e1245899c",
            "7866c84db105430c9de6d4d0244778d2"
          ]
        },
        "id": "aoe3ddtxRb7Y",
        "outputId": "0b16f9de-494b-41c2-f9c6-95ec8a55a170"
      },
      "outputs": [],
      "source": [
        "print(\"\\nGPT-2 Medium modeli GPT-4o veri kümesi ile eğitiliyor...\")\n",
        "model_medium, tokenizer_medium = model_ve_tokenizer_yukle(gpt2_medium_model_adi)\n",
        "# GPT-4o veri kümesiyle Medium modeli eğit\n",
        "model_egit_ve_kaydet(\n",
        "    model_medium,\n",
        "    tokenizer_medium,\n",
        "    veri_kumesi_gpt4o,\n",
        "    gpt2_medium_kaydetme_dizini_gpt4o,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ddx-uWtTiWvc"
      },
      "source": [
        "## GPT-2 Medium modelini DeepSeek veri kümesi ile eğit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VuSP5gzVhKvX"
      },
      "outputs": [],
      "source": [
        "# DeepSeek veri kümesiyle Medium modeli eğit\n",
        "print(\"\\nGPT-2 Medium modeli DeepSeek veri kümesi ile eğitiliyor...\")\n",
        "model_medium, tokenizer_medium = model_ve_tokenizer_yukle(gpt2_medium_model_adi)\n",
        "model_egit_ve_kaydet(\n",
        "    model_medium,\n",
        "    tokenizer_medium,\n",
        "    veri_kumesi_deepseek,\n",
        "    gpt2_medium_kaydetme_dizini_deepseek,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E5xacutTicNp"
      },
      "source": [
        "## GPT-2 Large modelini GPT-4o veri kümesi ile eğit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "75Ql1WILhKpf"
      },
      "outputs": [],
      "source": [
        "# GPT-4o veri kümesiyle Large modeli eğit\n",
        "print(\"\\nGPT-2 Large modeli GPT-4o veri kümesi ile eğitiliyor...\")\n",
        "model_large, tokenizer_large = model_ve_tokenizer_yukle(gpt2_large_model_adi)\n",
        "model_egit_ve_kaydet(\n",
        "    model_large,\n",
        "    tokenizer_large,\n",
        "    veri_kumesi_gpt4o,\n",
        "    gpt2_large_kaydetme_dizini_gpt4o,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wwf2Sey5ie6E"
      },
      "source": [
        "## GPT-2 Large modelini DeepSeek veri kümesi ile eğit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z7u9qL0BhKeP"
      },
      "outputs": [],
      "source": [
        "# DeepSeek veri kümesiyle Large modeli eğit\n",
        "print(\"\\nGPT-2 Large modeli DeepSeek veri kümesi ile eğitiliyor...\")\n",
        "model_large, tokenizer_large = model_ve_tokenizer_yukle(gpt2_large_model_adi)\n",
        "model_egit_ve_kaydet(\n",
        "    model_large,\n",
        "    tokenizer_large,\n",
        "    veri_kumesi_deepseek,\n",
        "    gpt2_large_kaydetme_dizini_deepseek,\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "2ab1489f4c3e45e991fdf03e1245899c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3240efab4c264d12a48a6a451e12ed25": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "32a0f17621824e44ba5994423c6b60ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2ab1489f4c3e45e991fdf03e1245899c",
            "placeholder": "​",
            "style": "IPY_MODEL_7866c84db105430c9de6d4d0244778d2",
            "value": " 13891/13891 [00:00&lt;00:00, 86203.03 examples/s]"
          }
        },
        "55f9e29ec98943dd90f561eb1b8699cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7866c84db105430c9de6d4d0244778d2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7b0c2278b2ff4fb4b277f813e30286cd": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7becf5f2e372431bada285816e087645": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7b0c2278b2ff4fb4b277f813e30286cd",
            "placeholder": "​",
            "style": "IPY_MODEL_55f9e29ec98943dd90f561eb1b8699cc",
            "value": "Truncating train dataset: 100%"
          }
        },
        "901cb4757c174232bc41d3d9f1af9e93": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7becf5f2e372431bada285816e087645",
              "IPY_MODEL_960798f166384a5e91d5a33b551a53cc",
              "IPY_MODEL_32a0f17621824e44ba5994423c6b60ba"
            ],
            "layout": "IPY_MODEL_f60728b99fab46c5a5e275f0e812da04"
          }
        },
        "960798f166384a5e91d5a33b551a53cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c288a6ad1fab4367805d3f9f7ff4280b",
            "max": 13891,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3240efab4c264d12a48a6a451e12ed25",
            "value": 13891
          }
        },
        "c288a6ad1fab4367805d3f9f7ff4280b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f60728b99fab46c5a5e275f0e812da04": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

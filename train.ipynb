{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eFIbL_V3PIwT"
      },
      "source": [
        "# Giriş işlemleri"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vZonvfzWJ4MV"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install trl bitsandbytes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lLjbKqLWPMCs"
      },
      "source": [
        "## Kütüphane içe aktarma"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "whu-0JW1OV36"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import torch\n",
        "from trl import SFTTrainer, SFTConfig\n",
        "from datasets import Dataset\n",
        "from typing import List, Optional, Callable, Any, Dict, Union, Literal, Tuple\n",
        "from transformers import (AutoModelForCausalLM,\n",
        "                          AutoTokenizer,\n",
        "                          BitsAndBytesConfig, PreTrainedTokenizer)\n",
        "from peft import LoraConfig, get_peft_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1VOz7owfPO2Q"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AKfyD5oZN8k5"
      },
      "outputs": [],
      "source": [
        "def is_colab() -> bool:\n",
        "    \"\"\"\n",
        "    Google Colab ortamında çalışıp çalışmadığını kontrol eden fonksiyon.\n",
        "\n",
        "    Args:\n",
        "        None\n",
        "\n",
        "    Returns:\n",
        "        bool: Eğer kod Google Colab'da çalışıyorsa True, aksi halde False döndürür\n",
        "    \"\"\"\n",
        "    try:\n",
        "        import google.colab\n",
        "        return True\n",
        "    except ImportError:\n",
        "        return False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XeFEFuO2N_Qz",
        "outputId": "41b50acb-d1b3-4447-98bb-944b4daf8424"
      },
      "outputs": [],
      "source": [
        "# Kök dizin belirleme\n",
        "if is_colab():\n",
        "    \"\"\"\n",
        "    Eğer kod Google Colab ortamında çalışıyorsa, Google Drive'ı bağlar ve\n",
        "    kök dizini Google Drive içindeki \"turkish_gpt2_finetuning\" klasörü olarak ayarlar.\n",
        "    \"\"\"\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')  # Google Drive'ı Colab ortamına bağlar\n",
        "    kok_dizin = \"/content/drive/MyDrive/turkish_gpt2_finetuning\"  # Drive içindeki çalışma klasörünü belirler\n",
        "else:\n",
        "    \"\"\"\n",
        "    Eğer kod yerel bir ortamda çalışıyorsa, kök dizini mevcut çalışma dizini olarak ayarlar.\n",
        "    \"\"\"\n",
        "    kok_dizin = os.getcwd()  # Mevcut çalışma dizinini alır\n",
        "\n",
        "# Belirlenen kök dizini kullanıcıya bilgi olarak gösterir\n",
        "print(f\"Kök dizin: {kok_dizin}\\n Not: eğer colab kullanıyorsanız, dizini değiştirmeniz gerekir.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "STt93S85Yyxx"
      },
      "source": [
        "## Veri kümesi biçimlendirme, yol ve model yolu tanımlamaları"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EeKc_832OR8P"
      },
      "outputs": [],
      "source": [
        "# Veri kümesi yolu ve karıştırma seed değeri tanımlamaları\n",
        "veri_kumesi_yolu = os.path.join(kok_dizin, \"soru_cevap.csv\")\n",
        "veri_kumesi_karistirma = 571  # Karıştırmada kullanılacak sabit seed değeri\n",
        "sonuc_dizini = os.path.join(kok_dizin, \"sonuclar\")  # Sonuçların kaydedileceği dizin\n",
        "\n",
        "# Model kaydetme dizinleri\n",
        "# gpt4o verisiyle eğitilmiş modeller\n",
        "gpt2_medium_kaydetme_dizini_gpt4o = os.path.join(kok_dizin, \"gpt2_medium_gpt4o\")\n",
        "gpt2_large_kaydetme_dizini_gpt4o = os.path.join(kok_dizin, \"gpt2_large_gpt4o\")\n",
        "\n",
        "# deepseek verisiyle eğitilmiş modeller\n",
        "gpt2_medium_kaydetme_dizini_deepseek = os.path.join(kok_dizin, \"gpt2_medium_deepseek\")\n",
        "gpt2_large_kaydetme_dizini_deepseek = os.path.join(kok_dizin, \"gpt2_large_deepseek\")\n",
        "\n",
        "# Model adı tanımlamaları\n",
        "gpt2_medium_model_adi = \"ytu-ce-cosmos/turkish-gpt2-medium\"\n",
        "gpt2_large_model_adi = \"ytu-ce-cosmos/turkish-gpt2-large\"\n",
        "\n",
        "# Veri kümesi sütun tanımlamaları\n",
        "soru_sutunu = \"Soru\"\n",
        "gpt4o_cevap_sutunu = \"gpt4o cevabı\"\n",
        "deepseek_cevap_sutunu = \"deepseek cevabı\"\n",
        "\n",
        "# Özel token tanımlamaları (eğitim formatı için)\n",
        "soru_baslangic = \"<SORU>\"\n",
        "soru_bitis = \"</SORU>\"\n",
        "cevap_baslangic = \"<CEVAP>\"\n",
        "cevap_bitis = \"</CEVAP>\"\n",
        "ornek_bitis = \"<|endoftext|>\"  # GPT-2'nin EOS tokeni\n",
        "\n",
        "# Tokenizer'a eklenecek özel tokenler listesi\n",
        "ozel_tokenler = [soru_baslangic, soru_bitis, cevap_baslangic, cevap_bitis]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gwpuWbgSPQtI"
      },
      "source": [
        "## Veri kümesi okuma"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rZnjjD0oPBjo",
        "outputId": "6ac8a558-8d38-44d2-c071-6adceb56825b"
      },
      "outputs": [],
      "source": [
        "# CSV dosyasını okuma\n",
        "try:\n",
        "    # CSV dosyasını okuyoruz\n",
        "    df = pd.read_csv(veri_kumesi_yolu)\n",
        "\n",
        "    # İstenen sütunların varlığını kontrol ediyoruz\n",
        "    gereken_sutunlar = [\"Soru\", gpt4o_cevap_sutunu, deepseek_cevap_sutunu]\n",
        "\n",
        "    for sutun in gereken_sutunlar:\n",
        "        if sutun not in df.columns:\n",
        "            print(f\"'{sutun}' sütunu veri kümesinde bulunamadı!\")\n",
        "\n",
        "    # İstenen sütunları ayıklama\n",
        "    soru_cevap_df = df[gereken_sutunlar]\n",
        "\n",
        "    # Doğrulama için ilk birkaç satırı görüntüleme\n",
        "    print(\"Veri kümesinin ilk birkaç satırı:\")\n",
        "    print(soru_cevap_df.head(5))\n",
        "\n",
        "    print(f\"\\nToplam {len(soru_cevap_df)} soru-cevap çifti bulundu.\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"'{veri_kumesi_yolu}' dosyası bulunamadı. Lütfen dosya yolunu kontrol ediniz.\")\n",
        "except Exception as e:\n",
        "    print(f\"Veri okuma hatası: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HILJV27GPiIc",
        "outputId": "b95b5604-9493-41e2-e259-5c991b762bc6"
      },
      "outputs": [],
      "source": [
        "# Veri kümesini belirtilen seed değeriyle karıştırma\n",
        "shuffled_df = soru_cevap_df.sample(frac=1, random_state=veri_kumesi_karistirma)\n",
        "\n",
        "# İndeksleri sıfırlama\n",
        "shuffled_df = shuffled_df.reset_index(drop=True)\n",
        "\n",
        "# Karıştırılmış veri kümesinin ilk birkaç satırını gösterme\n",
        "print(\"Karıştırılmış veri kümesinin ilk birkaç satırı:\")\n",
        "print(shuffled_df.head(5))\n",
        "\n",
        "print(f\"\\nKarıştırılmış veri kümesi boyutu: {len(shuffled_df)} satır\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JXG9nHCqZSFT"
      },
      "source": [
        "# Veri kümesi işleme"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eWMjPMiuW44_"
      },
      "source": [
        "## Veri kümesi biçimlendirme fonksiyonu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JWeG9G8pURiv"
      },
      "outputs": [],
      "source": [
        "def veri_kumesini_egitim_formatina_donustur(df: pd.DataFrame, cevap_sutunu: str,\n",
        "                                            soru_baslangic: str = soru_baslangic,\n",
        "                                            soru_bitis: str = soru_bitis,\n",
        "                                            cevap_baslangic: str = cevap_baslangic,\n",
        "                                            cevap_bitis: str = cevap_bitis,\n",
        "                                            ornek_bitis: str = ornek_bitis) -> list:\n",
        "    \"\"\"\n",
        "    Veri kümesindeki soru ve cevapları GPT-2 eğitimi için uygun formata dönüştürür.\n",
        "    Metinsel formatta örnekler döndürür, tokenize işlemi uygulamaz.\n",
        "\n",
        "    Args:\n",
        "        df: Soru ve cevapları içeren DataFrame\n",
        "        cevap_sutunu: Cevap metinlerini içeren sütunun adı\n",
        "        soru_baslangic: Soru başlangıç etiketi\n",
        "        soru_bitis: Soru bitiş etiketi\n",
        "        cevap_baslangic: Cevap başlangıç etiketi\n",
        "        cevap_bitis: Cevap bitiş etiketi\n",
        "        ornek_bitis: Her örneğin sonunu belirten etiket\n",
        "\n",
        "    Returns:\n",
        "        list: Eğitim için hazırlanmış metinsel örnekler listesi\n",
        "    \"\"\"\n",
        "\n",
        "    egitim_metinleri = []\n",
        "\n",
        "    # Her bir soru-cevap çifti için formatlı metinler oluştur\n",
        "    for _, satir in df.iterrows():\n",
        "        soru = satir[\"Soru\"].strip()\n",
        "        cevap = satir[cevap_sutunu].strip()\n",
        "\n",
        "        # Soru ve cevabı belirli bir formatta birleştir\n",
        "        bicimlendirilmis_metin = f\"{soru_baslangic} {soru} {soru_bitis} {cevap_baslangic} {cevap} {cevap_bitis}{ornek_bitis}\"\n",
        "\n",
        "        egitim_metinleri.append(bicimlendirilmis_metin)\n",
        "\n",
        "    print(f\"Toplam {len(egitim_metinleri)} adet eğitim örneği oluşturuldu.\")\n",
        "    print(f\"Örnek biçimi: {soru_baslangic} [Soru] {soru_bitis} {cevap_baslangic} [Cevap] {cevap_bitis}{ornek_bitis}\")\n",
        "    return egitim_metinleri"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MfiIVCNKh28b"
      },
      "source": [
        "## Veri kümelerini işle ve oluştur"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5GskjGIMZTnE",
        "outputId": "dc130944-715a-4682-f7a2-f5b2827d587f"
      },
      "outputs": [],
      "source": [
        "veri_kumesi_gpt4o = veri_kumesini_egitim_formatina_donustur(shuffled_df, gpt4o_cevap_sutunu)\n",
        "veri_kumesi_deepseek = veri_kumesini_egitim_formatina_donustur(shuffled_df, deepseek_cevap_sutunu)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "US-pveshRaVV"
      },
      "source": [
        "# Eğitim işlemleri"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HSNOJdatXExf"
      },
      "source": [
        "## Model Yükleme ve Eğitme Fonksiyonları"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vN_BKyEaiBQV"
      },
      "source": [
        "### Eğitim argümanları oluşturma fonksiyonu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R4fThwcfZ8mR"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "max_seq_length = 1024   # üstte sabit tanım\n",
        "\n",
        "def training_arguments_getir(\n",
        "    kaydetme_dizin      : str,\n",
        "    veri_kumesi_uzunlugu: int,          # len(dataset)\n",
        "    *,\n",
        "    # hiper-parametreler\n",
        "    learning_rate : float = 1e-4,\n",
        "    batch_size    : int   = 2,\n",
        "    grad_accum_steps    : int   = 32,\n",
        "    # eğitim süresi\n",
        "    epochs        : int   = 1,\n",
        "    # sekuans uzunluğu\n",
        "    max_seq_length: int   = max_seq_length,\n",
        "    # “oran”lar\n",
        "    save_per_epoch: int   = 4,          # her epoch’ta 4 checkpoint\n",
        "    log_per_epoch : int   = 12,          # her epoch’ta 8 loss logu\n",
        ") -> SFTConfig:\n",
        "\n",
        "    # --------------------- türetilen değerler -----------------------------\n",
        "    ngpu = max(torch.cuda.device_count(), 1)\n",
        "    effective_batch = batch_size * grad_accum * ngpu\n",
        "\n",
        "    steps_per_epoch = math.ceil(veri_kumesi_uzunlugu / effective_batch)\n",
        "    max_steps       = epochs * steps_per_epoch\n",
        "    save_steps      = max(1, steps_per_epoch // save_per_epoch)\n",
        "    logging_steps   = max(1, steps_per_epoch // log_per_epoch)\n",
        "\n",
        "    print(f\"[INFO] veri: {veri_kumesi_uzunlugu} örnek  | \"\n",
        "          f\"gpu: {ngpu}  |  effective_batch: {effective_batch}  |  \"\n",
        "          f\"steps/epoch: {steps_per_epoch}  |  total steps: {max_steps}\")\n",
        "\n",
        "    # --------------------- ortak argümanlar --------------------------------\n",
        "    cfg_kwargs = dict(\n",
        "        output_dir = kaydetme_dizin,\n",
        "\n",
        "        # hiper-parametreler\n",
        "        learning_rate               = learning_rate,\n",
        "        per_device_train_batch_size = batch_size,\n",
        "        gradient_accumulation_steps = grad_accum_steps,\n",
        "\n",
        "        # sekuans uzunluğu\n",
        "        max_seq_length = max_seq_length,\n",
        "\n",
        "        # optimizasyon / planlayıcı\n",
        "        optim             = \"paged_adamw_8bit\",\n",
        "        weight_decay      = 0.1,\n",
        "        warmup_ratio      = 0.1,\n",
        "        lr_scheduler_type = \"cosine\",\n",
        "        max_grad_norm     = 0.1,\n",
        "\n",
        "        # loglama\n",
        "        logging_dir      = os.path.join(kaydetme_dizin, \"logs\"),\n",
        "        logging_strategy = \"steps\",\n",
        "        logging_steps    = logging_steps,\n",
        "        report_to        = \"tensorboard\",\n",
        "        run_name         = os.path.basename(kaydetme_dizin),\n",
        "\n",
        "        # checkpoint\n",
        "        save_strategy = \"steps\",\n",
        "        save_steps    = save_steps,\n",
        "\n",
        "        # epoch tabanlı\n",
        "        num_train_epochs = epochs,\n",
        "        max_steps        = -1,          # Trainer epoch modunda -1 ister\n",
        "    )\n",
        "\n",
        "    return SFTConfig(**cfg_kwargs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "21jgGqeniGM9"
      },
      "source": [
        "### Model Yükleme fonksiyonu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yA7f5Bq3Rjcl"
      },
      "outputs": [],
      "source": [
        "def model_ve_tokenizer_yukle(\n",
        "    model_adi: str,\n",
        "    max_seq_length: int = max_seq_length,\n",
        "    lora_rank: int = 16,\n",
        "    random_state: int = veri_kumesi_karistirma,\n",
        "    target_modules: List[str] = (\"c_attn\", \"c_proj\", \"c_fc\"),\n",
        "    ozel_tokenler: List[str] = None,\n",
        "    quant_mode: Literal[\"4bit\", \"8bit\", \"16bit\"] = \"16bit\",\n",
        ") -> Tuple[torch.nn.Module, PreTrainedTokenizer]:\n",
        "    \"\"\"\n",
        "    • quant_mode = \"4bit\"  -> 4-bit QLoRA\n",
        "    • quant_mode = \"8bit\"  -> 8-bit (bnb)\n",
        "    • quant_mode = \"16bit\" -> fp16 / bf16, quantization yok\n",
        "    \"\"\"\n",
        "    # ----------------------- tokenizer -------------------------------------\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_adi, use_fast=True)\n",
        "    tokenizer.padding_side, tokenizer.truncation_side = \"right\", \"right\"\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.add_special_tokens({\"pad_token\": tokenizer.eos_token})\n",
        "\n",
        "    # ----------------------- model (quant / dtype) -------------------------\n",
        "    quant_cfg = None\n",
        "    model_kwargs = dict(device_map=\"auto\", trust_remote_code=True)\n",
        "\n",
        "    if quant_mode == \"4bit\":\n",
        "        quant_cfg = BitsAndBytesConfig(\n",
        "            load_in_4bit=True,\n",
        "            bnb_4bit_use_double_quant=True,\n",
        "            bnb_4bit_quant_type=\"nf4\",\n",
        "            bnb_4bit_compute_dtype=\n",
        "                torch.bfloat16 if torch.cuda.get_device_capability(0)[0] >= 8\n",
        "                else torch.float16,\n",
        "        )\n",
        "        model_kwargs[\"quantization_config\"] = quant_cfg\n",
        "\n",
        "    elif quant_mode == \"8bit\":\n",
        "        quant_cfg = BitsAndBytesConfig(load_in_8bit=True)\n",
        "        model_kwargs[\"quantization_config\"] = quant_cfg\n",
        "\n",
        "    else:  # \"16bit\"\n",
        "        # fp16 ile yükle; Ampere (8.x) ve üstü kartlarda bf16 da seçilebilir\n",
        "        model_kwargs[\"torch_dtype\"] = (\n",
        "            torch.bfloat16 if torch.cuda.get_device_capability(0)[0] >= 8\n",
        "            else torch.float16\n",
        "        )\n",
        "\n",
        "    model = AutoModelForCausalLM.from_pretrained(model_adi, **model_kwargs)\n",
        "    model.config.max_position_embeddings = max_seq_length\n",
        "    model.config.use_cache = False\n",
        "    model.gradient_checkpointing_enable()\n",
        "\n",
        "    # ----------------------- LoRA -----------------------------------------\n",
        "    lora_cfg = LoraConfig(\n",
        "        r=lora_rank,\n",
        "        lora_alpha=lora_rank,\n",
        "        target_modules=list(target_modules),\n",
        "        bias=\"none\",\n",
        "        task_type=\"CAUSAL_LM\",\n",
        "        fan_in_fan_out=True,\n",
        "    )\n",
        "    model = get_peft_model(model, lora_cfg)\n",
        "    model.print_trainable_parameters()\n",
        "\n",
        "    # ----------------------- özel tokenlar ---------------------------------\n",
        "    if ozel_tokenler:\n",
        "        tokenizer.add_special_tokens({\"additional_special_tokens\": ozel_tokenler})\n",
        "        model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "    torch.cuda.manual_seed(random_state)\n",
        "    print(f\"{model_adi} '{quant_mode}' modunda yüklendi (+LoRA).\")\n",
        "    return model, tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hVl_03-EiI49"
      },
      "source": [
        "### Eğitici Getirme fonksiyonu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iPD3wCILaJez"
      },
      "outputs": [],
      "source": [
        "def trainer_getir(model, training_args, veri_kumesi, tokenizer):\n",
        "    toks = tokenizer(\n",
        "        veri_kumesi,\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=training_args.max_seq_length,\n",
        "    )\n",
        "\n",
        "    # torch tensörleri listeye çeviriyoruz\n",
        "    data_dict = {\n",
        "        \"input_ids\":      [ids for ids in toks[\"input_ids\"]],\n",
        "        \"attention_mask\": [mask for mask in toks[\"attention_mask\"]],\n",
        "    }\n",
        "    train_dataset = Dataset.from_dict(data_dict)\n",
        "    train_dataset.set_format(type=\"torch\")\n",
        "\n",
        "    print(f\"Eğitim için {len(train_dataset)} örnek hazırlandı.\")\n",
        "\n",
        "    trainer = SFTTrainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "    )\n",
        "    return trainer\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LzxRcRlPiLPV"
      },
      "source": [
        "### Veri kümesi özelinde model eğitme fonksiyonu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-qiyNRlVgna4"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "def model_egit_ve_kaydet(\n",
        "    model,\n",
        "    tokenizer,\n",
        "    veri_kumesi,\n",
        "    model_kaydetme_dizini,\n",
        "):\n",
        "    \"\"\"\n",
        "    Eğitim döngüsünü başlatır, tamamlandığında modeli ve tokenizer’ı kaydeder.\n",
        "    \"\"\"\n",
        "    os.makedirs(model_kaydetme_dizini, exist_ok=True)\n",
        "\n",
        "    training_args = training_arguments_getir(\n",
        "        kaydetme_dizin=model_kaydetme_dizini,\n",
        "        max_steps=250,\n",
        "        save_steps=50,\n",
        "        logging_steps=25\n",
        "    )\n",
        "\n",
        "    trainer = trainer_getir(\n",
        "        model         = model,\n",
        "        training_args = training_args,\n",
        "        veri_kumesi   = veri_kumesi,\n",
        "        tokenizer     = tokenizer,\n",
        "    )\n",
        "\n",
        "\n",
        "    print(f\"\\n[Eğitim] {model_kaydetme_dizini} dizininde eğitim başlatılıyor...\")\n",
        "    trainer.train()\n",
        "    trainer.save_model(model_kaydetme_dizini)\n",
        "    tokenizer.save_pretrained(model_kaydetme_dizini)\n",
        "    log_json_path = os.path.join(model_kaydetme_dizini, \"training_log.json\")\n",
        "    with open(log_json_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(trainer.state.log_history, f, ensure_ascii=False, indent=4)\n",
        "    print(f\"Model ve tokenizer '{model_kaydetme_dizini}' dizinine kaydedildi.\")\n",
        "\n",
        "    # Belleği temizle\n",
        "    del model\n",
        "    torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Su5-t-l5XI6I"
      },
      "source": [
        "## GPT-2 Medium modelini GPT-4o veri kümesi ile eğit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 744,
          "referenced_widgets": [
            "c8e08de74c1043249868ef8e426d5c80",
            "942bb0ac2f1949c39754078a7563083b",
            "12fd0ce04fcd4707b0b556263d3c5fd7",
            "cbf2d870cc5f423196f91f20cc8e0c97",
            "bb69833cddd4467f908fdcaf52d6f2e2",
            "6f06747518ff490ab1fade3978c8350c",
            "23fc929f9ca443f58bf7b9f28e847224",
            "fb738a3c808d4ae08b4ac7f67db77591",
            "ac2596e4ed1f4e1fbc4b3bcd2679e286",
            "dfb6d4ff4a3b43f6b3182b780cea51b9",
            "ca451813d5ce49e4ac36d0aaf062121b"
          ]
        },
        "id": "aoe3ddtxRb7Y",
        "outputId": "cde48b2c-9ee9-4b29-956d-3aceb2abdbf4"
      },
      "outputs": [],
      "source": [
        "print(\"\\nGPT-2 Medium modeli GPT-4o veri kümesi ile eğitiliyor...\")\n",
        "model_medium, tokenizer_medium = model_ve_tokenizer_yukle(gpt2_medium_model_adi)\n",
        "# GPT-4o veri kümesiyle Medium modeli eğit\n",
        "model_egit_ve_kaydet(\n",
        "    model_medium,\n",
        "    tokenizer_medium,\n",
        "    veri_kumesi_gpt4o,\n",
        "    gpt2_medium_kaydetme_dizini_gpt4o,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ddx-uWtTiWvc"
      },
      "source": [
        "## GPT-2 Medium modelini DeepSeek veri kümesi ile eğit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VuSP5gzVhKvX"
      },
      "outputs": [],
      "source": [
        "# DeepSeek veri kümesiyle Medium modeli eğit\n",
        "print(\"\\nGPT-2 Medium modeli DeepSeek veri kümesi ile eğitiliyor...\")\n",
        "model_medium, tokenizer_medium = model_ve_tokenizer_yukle(gpt2_medium_model_adi)\n",
        "model_egit_ve_kaydet(\n",
        "    model_medium,\n",
        "    tokenizer_medium,\n",
        "    veri_kumesi_deepseek,\n",
        "    gpt2_medium_kaydetme_dizini_deepseek,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E5xacutTicNp"
      },
      "source": [
        "## GPT-2 Large modelini GPT-4o veri kümesi ile eğit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "75Ql1WILhKpf"
      },
      "outputs": [],
      "source": [
        "# GPT-4o veri kümesiyle Large modeli eğit\n",
        "print(\"\\nGPT-2 Large modeli GPT-4o veri kümesi ile eğitiliyor...\")\n",
        "model_large, tokenizer_large = model_ve_tokenizer_yukle(gpt2_large_model_adi)\n",
        "model_egit_ve_kaydet(\n",
        "    model_large,\n",
        "    tokenizer_large,\n",
        "    veri_kumesi_gpt4o,\n",
        "    gpt2_large_kaydetme_dizini_gpt4o,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wwf2Sey5ie6E"
      },
      "source": [
        "## GPT-2 Large modelini DeepSeek veri kümesi ile eğit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z7u9qL0BhKeP"
      },
      "outputs": [],
      "source": [
        "# DeepSeek veri kümesiyle Large modeli eğit\n",
        "print(\"\\nGPT-2 Large modeli DeepSeek veri kümesi ile eğitiliyor...\")\n",
        "model_large, tokenizer_large = model_ve_tokenizer_yukle(gpt2_large_model_adi)\n",
        "model_egit_ve_kaydet(\n",
        "    model_large,\n",
        "    tokenizer_large,\n",
        "    veri_kumesi_deepseek,\n",
        "    gpt2_large_kaydetme_dizini_deepseek,\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "12fd0ce04fcd4707b0b556263d3c5fd7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fb738a3c808d4ae08b4ac7f67db77591",
            "max": 13891,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ac2596e4ed1f4e1fbc4b3bcd2679e286",
            "value": 13891
          }
        },
        "23fc929f9ca443f58bf7b9f28e847224": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6f06747518ff490ab1fade3978c8350c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "942bb0ac2f1949c39754078a7563083b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6f06747518ff490ab1fade3978c8350c",
            "placeholder": "​",
            "style": "IPY_MODEL_23fc929f9ca443f58bf7b9f28e847224",
            "value": "Truncating train dataset: 100%"
          }
        },
        "ac2596e4ed1f4e1fbc4b3bcd2679e286": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "bb69833cddd4467f908fdcaf52d6f2e2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c8e08de74c1043249868ef8e426d5c80": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_942bb0ac2f1949c39754078a7563083b",
              "IPY_MODEL_12fd0ce04fcd4707b0b556263d3c5fd7",
              "IPY_MODEL_cbf2d870cc5f423196f91f20cc8e0c97"
            ],
            "layout": "IPY_MODEL_bb69833cddd4467f908fdcaf52d6f2e2"
          }
        },
        "ca451813d5ce49e4ac36d0aaf062121b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cbf2d870cc5f423196f91f20cc8e0c97": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dfb6d4ff4a3b43f6b3182b780cea51b9",
            "placeholder": "​",
            "style": "IPY_MODEL_ca451813d5ce49e4ac36d0aaf062121b",
            "value": " 13891/13891 [00:00&lt;00:00, 84964.67 examples/s]"
          }
        },
        "dfb6d4ff4a3b43f6b3182b780cea51b9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fb738a3c808d4ae08b4ac7f67db77591": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75228538",
   "metadata": {},
   "source": [
    "# Giriş işlemleri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "054a2645",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca03771a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_colab() -> bool:\n",
    "    \"\"\"\n",
    "    Google Colab ortamında çalışıp çalışmadığını kontrol eden fonksiyon.\n",
    "\n",
    "    Args:\n",
    "        None\n",
    "\n",
    "    Returns:\n",
    "        bool: Eğer kod Google Colab'da çalışıyorsa True, aksi halde False döndürür\n",
    "    \"\"\"\n",
    "    try:\n",
    "        import google.colab\n",
    "        return True\n",
    "    except ImportError:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b57e32c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kök dizin belirleme\n",
    "if is_colab():\n",
    "    \"\"\"\n",
    "    Eğer kod Google Colab ortamında çalışıyorsa, Google Drive'ı bağlar ve\n",
    "    kök dizini Google Drive içindeki \"turkish_gpt2_finetuning\" klasörü olarak ayarlar.\n",
    "    \"\"\"\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')  # Google Drive'ı Colab ortamına bağlar\n",
    "    kok_dizin = \"/content/drive/MyDrive/turkish_gpt2_finetuning\"  # Drive içindeki çalışma klasörünü belirler\n",
    "else:\n",
    "    \"\"\"\n",
    "    Eğer kod yerel bir ortamda çalışıyorsa, kök dizini mevcut çalışma dizini olarak ayarlar.\n",
    "    \"\"\"\n",
    "    kok_dizin = os.getcwd()  # Mevcut çalışma dizinini alır\n",
    "\n",
    "# Belirlenen kök dizini kullanıcıya bilgi olarak gösterir\n",
    "print(f\"Kök dizin: {kok_dizin}\\n Not: eğer colab kullanıyorsanız, dizini değiştirmeniz gerekir.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed9cca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "veri_kumesi_yolu = os.path.join(kok_dizin, \"sinama_sorulari.csv\")\n",
    "sonuc_dizini     = os.path.join(kok_dizin, \"sonuclar\")\n",
    "# Model kaydetme dizinleri\n",
    "# gpt4o verisiyle eğitilmiş modeller\n",
    "gpt2_medium_kaydetme_dizini_gpt4o = os.path.join(kok_dizin, \"gpt2_medium_gpt4o\")\n",
    "gpt2_large_kaydetme_dizini_gpt4o = os.path.join(kok_dizin, \"gpt2_large_gpt4o\")\n",
    "\n",
    "# deepseek verisiyle eğitilmiş modeller\n",
    "gpt2_medium_kaydetme_dizini_deepseek = os.path.join(kok_dizin, \"gpt2_medium_deepseek\")\n",
    "gpt2_large_kaydetme_dizini_deepseek = os.path.join(kok_dizin, \"gpt2_large_deepseek\")\n",
    "\n",
    "model_yollari = {\n",
    "    \"gpt2_medium_gpt4o\"   : gpt2_medium_kaydetme_dizini_gpt4o,\n",
    "    \"gpt2_large_gpt4o\"    : gpt2_large_kaydetme_dizini_gpt4o,\n",
    "    \"gpt2_medium_deepseek\": gpt2_medium_kaydetme_dizini_deepseek,\n",
    "    \"gpt2_large_deepseek\" : gpt2_large_kaydetme_dizini_deepseek,\n",
    "}\n",
    "\n",
    "soru_sutunu = \"soru\"\n",
    "\n",
    "# Özel token tanımlamaları (sınama biçimi için)\n",
    "soru_baslangic = \"<SORU>\"\n",
    "soru_bitis = \"</SORU>\"\n",
    "cevap_baslangic = \"<CEVAP>\"\n",
    "cevap_bitis = \"</CEVAP>\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd54335c",
   "metadata": {},
   "source": [
    "# Fonksiyonlar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aadb348",
   "metadata": {},
   "source": [
    "## Soruyu girdi biçimine çevirme fonksiyonu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b402a343",
   "metadata": {},
   "outputs": [],
   "source": [
    "def soruyu_girdi_bicimine_cevir(soru: str) -> str:\n",
    "    \"\"\"\n",
    "    <SORU> ... </SORU> <CEVAP> biçiminde prompt döndürür.\n",
    "    Model cevabı üretirken </CEVAP> veya EOS verdiği anda durduracağız.\n",
    "    \"\"\"\n",
    "    soru = soru.strip()\n",
    "    return f\"{soru_baslangic} {soru} {soru_bitis} {cevap_baslangic} \""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66af4847",
   "metadata": {},
   "source": [
    "## Modeli yükleme fonksiyonu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a381238c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_yukle(model_yolu: str, dtype: str = \"auto\"):\n",
    "    \"\"\"\n",
    "    bitsandbytes kullanmadan doğrudan AutoModelForCausalLM yükler.\n",
    "    dtype:\n",
    "        • \"auto\"  → GPU varsa float16 / bf16, yoksa float32\n",
    "        • \"fp16\"  → torch.float16\n",
    "        • \"bf16\"  → torch.bfloat16\n",
    "        • \"fp32\"  → torch.float32\n",
    "    \"\"\"\n",
    "    if dtype == \"auto\":\n",
    "        if torch.cuda.is_available():\n",
    "            major = torch.cuda.get_device_capability(0)[0]\n",
    "            kullanicidtype = torch.bfloat16 if major >= 8 else torch.float16\n",
    "        else:\n",
    "            kullanicidtype = torch.float32\n",
    "    elif dtype == \"fp16\": kullanicidtype = torch.float16\n",
    "    elif dtype == \"bf16\": kullanicidtype = torch.bfloat16\n",
    "    else:                 kullanicidtype = torch.float32\n",
    "\n",
    "    print(f\"\\n[MODEL] {model_yolu} yükleniyor ({str(kullanicidtype)})...\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_yolu,\n",
    "        torch_dtype=kullanicidtype,\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_yolu, use_fast=True)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf8e606b",
   "metadata": {},
   "source": [
    "## Modelden cevabı alma fonksiyonu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2389d865",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cevap_uret(\n",
    "        model,\n",
    "        tokenizer,\n",
    "        prompt: str,\n",
    "        *,\n",
    "        max_new_tokens: int      = 1024,\n",
    "        no_repeat_ngram_size: int = 3,\n",
    "        repetition_penalty: float = 1.05,\n",
    "        eos_token_id: int        = None):\n",
    "    \"\"\"\n",
    "    • do_sample=False  → greedy\n",
    "    • temperature/top_p dikkate alınmaz\n",
    "    • no_repeat_ngram_size 3 → aynı 3-gram tekrar edemez\n",
    "    • repetition_penalty >1  → tekrar eden tokenu cezalandır\n",
    "    \"\"\"\n",
    "    if eos_token_id is None:\n",
    "        eos_token_id = tokenizer.convert_tokens_to_ids(\"</CEVAP>\")\n",
    "\n",
    "    giris_ids = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    cikti_ids = model.generate(\n",
    "        **giris_ids,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        do_sample=False,                 # <-- deterministik\n",
    "        eos_token_id=eos_token_id,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        no_repeat_ngram_size=no_repeat_ngram_size,\n",
    "        repetition_penalty=repetition_penalty,\n",
    "    )\n",
    "\n",
    "    yanit = tokenizer.decode(cikti_ids[0][giris_ids[\"input_ids\"].shape[-1]:],\n",
    "                             skip_special_tokens=True)\n",
    "    return yanit.split(\"</CEVAP>\")[0].strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cdd68fe",
   "metadata": {},
   "source": [
    "# Sonuç üretme kısmı"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "694a0882",
   "metadata": {},
   "source": [
    "## Veri kümesi oku"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42bbee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sorular = pd.read_csv(veri_kumesi_yolu)\n",
    "print(f\"{len(df_sorular)} soru yüklendi.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d44e12b2",
   "metadata": {},
   "source": [
    "## Her model için sonuç üret ve kaydet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782b4d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sonuçlar dizinini oluştur\n",
    "os.makedirs(sonuc_dizini, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17152c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_etiket, model_folder in model_yollari.items():\n",
    "    cikti_yolu = os.path.join(sonuc_dizini, f\"{model_etiket}.csv\")\n",
    "    if os.path.exists(cikti_yolu):\n",
    "        print(f\"[SKIP] '{cikti_yolu}' zaten var, geçiliyor.\")\n",
    "        continue\n",
    "\n",
    "    # model + tokenizer\n",
    "    model, tokenizer = model_yukle(model_folder)\n",
    "    model.eval()\n",
    "\n",
    "    cevaplar = []\n",
    "    for soru in tqdm(df_sorular[\"soru\"], desc=model_etiket):\n",
    "        prompt = soruyu_girdi_bicimine_cevir(soru)\n",
    "        cevap  = cevap_uret(model, tokenizer, prompt)\n",
    "        cevaplar.append(cevap)\n",
    "\n",
    "    # sonuç DataFrame’i\n",
    "    df_sonuclar = df_sorular.copy()\n",
    "    df_sonuclar[\"cevap\"] = cevaplar\n",
    "    df_sonuclar.to_csv(cikti_yolu, index=False, encoding=\"utf-8\")\n",
    "    print(f\"[OK] {model_etiket} sonuçları '{cikti_yolu}' dosyasına kaydedildi.\")\n",
    "\n",
    "    # belleği temizle\n",
    "    del model\n",
    "    torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "75228538",
      "metadata": {
        "id": "75228538"
      },
      "source": [
        "# Giriş işlemleri"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "054a2645",
      "metadata": {
        "id": "054a2645"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import os\n",
        "import torch\n",
        "import pandas as pd\n",
        "from tqdm.auto import tqdm\n",
        "from typing import Optional, Tuple\n",
        "from peft import PeftModel\n",
        "import random\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ca03771a",
      "metadata": {
        "id": "ca03771a"
      },
      "outputs": [],
      "source": [
        "def is_colab() -> bool:\n",
        "    \"\"\"\n",
        "    Google Colab ortamında çalışıp çalışmadığını kontrol eden fonksiyon.\n",
        "\n",
        "    Args:\n",
        "        None\n",
        "\n",
        "    Returns:\n",
        "        bool: Eğer kod Google Colab'da çalışıyorsa True, aksi halde False döndürür\n",
        "    \"\"\"\n",
        "    try:\n",
        "        import google.colab\n",
        "        return True\n",
        "    except ImportError:\n",
        "        return False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3b57e32c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3b57e32c",
        "outputId": "2be896eb-ac94-4351-9363-0e136dbf13ee"
      },
      "outputs": [],
      "source": [
        "# Kök dizin belirleme\n",
        "if is_colab():\n",
        "    \"\"\"\n",
        "    Eğer kod Google Colab ortamında çalışıyorsa, Google Drive'ı bağlar ve\n",
        "    kök dizini Google Drive içindeki \"turkish_gpt2_finetuning\" klasörü olarak ayarlar.\n",
        "    \"\"\"\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')  # Google Drive'ı Colab ortamına bağlar\n",
        "    kok_dizin = \"/content/drive/MyDrive/turkish_gpt2_finetuning\"  # Drive içindeki çalışma klasörünü belirler\n",
        "else:\n",
        "    \"\"\"\n",
        "    Eğer kod yerel bir ortamda çalışıyorsa, kök dizini mevcut çalışma dizini olarak ayarlar.\n",
        "    \"\"\"\n",
        "    kok_dizin = os.getcwd()  # Mevcut çalışma dizinini alır\n",
        "\n",
        "# Belirlenen kök dizini kullanıcıya bilgi olarak gösterir\n",
        "print(f\"Kök dizin: {kok_dizin}\\n Not: eğer colab kullanıyorsanız, dizini değiştirmeniz gerekir.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5ed9cca4",
      "metadata": {
        "id": "5ed9cca4"
      },
      "outputs": [],
      "source": [
        "veri_kumesi_yolu = os.path.join(kok_dizin, \"sinama_sorulari.csv\")\n",
        "sonuc_dizini     = os.path.join(kok_dizin, \"sonuclar\")\n",
        "# Model kaydetme dizinleri\n",
        "# gpt4o verisiyle eğitilmiş modeller\n",
        "gpt2_medium_kaydetme_dizini_gpt4o = os.path.join(kok_dizin, \"gpt2_medium_gpt4o\")\n",
        "gpt2_large_kaydetme_dizini_gpt4o = os.path.join(kok_dizin, \"gpt2_large_gpt4o\")\n",
        "\n",
        "# deepseek verisiyle eğitilmiş modeller\n",
        "gpt2_medium_kaydetme_dizini_deepseek = os.path.join(kok_dizin, \"gpt2_medium_deepseek\")\n",
        "gpt2_large_kaydetme_dizini_deepseek = os.path.join(kok_dizin, \"gpt2_large_deepseek\")\n",
        "\n",
        "# Model adı tanımlamaları\n",
        "gpt2_medium_model_adi = \"ytu-ce-cosmos/turkish-gpt2-medium\"\n",
        "gpt2_large_model_adi = \"ytu-ce-cosmos/turkish-gpt2-large\"\n",
        "\n",
        "model_bilgileri = [\n",
        "    (gpt2_medium_model_adi, \"gpt2_medium_gpt4o\" , gpt2_medium_kaydetme_dizini_gpt4o),\n",
        "    (gpt2_medium_model_adi, \"gpt2_medium_deepseek\", gpt2_medium_kaydetme_dizini_deepseek),\n",
        "    (gpt2_large_model_adi , \"gpt2_large_gpt4o\" , gpt2_large_kaydetme_dizini_gpt4o),\n",
        "    (gpt2_large_model_adi , \"gpt2_large_deepseek\", gpt2_large_kaydetme_dizini_deepseek),\n",
        "]\n",
        "\n",
        "soru_sutunu = \"soru\"\n",
        "\n",
        "# Özel token tanımlamaları (sınama biçimi için)\n",
        "soru_baslangic = \"<SORU>\"\n",
        "soru_bitis = \"</SORU>\"\n",
        "cevap_baslangic = \"<CEVAP>\"\n",
        "cevap_bitis = \"</CEVAP>\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bd54335c",
      "metadata": {
        "id": "bd54335c"
      },
      "source": [
        "# Fonksiyonlar"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0aadb348",
      "metadata": {
        "id": "0aadb348"
      },
      "source": [
        "## Soruyu girdi biçimine çevirme fonksiyonu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b402a343",
      "metadata": {
        "id": "b402a343"
      },
      "outputs": [],
      "source": [
        "def soruyu_girdi_bicimine_cevir(soru: str) -> str:\n",
        "    \"\"\"\n",
        "    Verilen ham soruyu, dil modeline uygun istem (prompt) biçimine dönüştürür.\n",
        "\n",
        "    Fonksiyon; dışarıda global olarak tanımlı özel etiketleri (<SORU>, </SORU>,\n",
        "    <CEVAP>) kullanarak aşağıdaki kalıpta bir metin hazırlar:\n",
        "\n",
        "        \"<SORU> {soru} </SORU> <CEVAP> \"\n",
        "\n",
        "    Model tarafında generate() çağrısı sırasında, </CEVAP> veya EOS token’ı\n",
        "    üretildiğinde üretim sonlandırılır ve yalnızca cevaba ait kısım alınır.\n",
        "    Böylece sorunun başı ve bitişi ile cevabın başlangıcı tutarlı bir şekilde\n",
        "    modellenmiş olur.\n",
        "\n",
        "    Args:\n",
        "        soru (str): Kullanıcıdan gelen ham soru metni. Baş-son boşlukları\n",
        "                    fonksiyon içinde temizlenir (str.strip()).\n",
        "\n",
        "    Returns:\n",
        "        str: Dil modeline beslenmeye hazır, etiketlenmiş prompt dizgesi.\n",
        "\n",
        "    Örnek:\n",
        "        >>> soruyu_girdi_bicimine_cevir(\"Türkiye'nin başkenti neresidir?\")\n",
        "        '<SORU> Türkiye'nin başkenti neresidir? </SORU> <CEVAP> '\n",
        "    \"\"\"\n",
        "\n",
        "    # ---------------------------------------------------------- #\n",
        "    # 1) Giriş temizliği\n",
        "    # ---------------------------------------------------------- #\n",
        "    # Sorunun başında / sonunda fazladan boşluk varsa yok sayılır.\n",
        "    soru = soru.strip()\n",
        "\n",
        "    # ---------------------------------------------------------- #\n",
        "    # 2) Prompt kalıbını oluştur\n",
        "    # ---------------------------------------------------------- #\n",
        "    # Global sabitler:\n",
        "    #   soru_baslangic = \"<SORU>\"\n",
        "    #   soru_bitis     = \"</SORU>\"\n",
        "    #   cevap_baslangic= \"<CEVAP>\"\n",
        "    #\n",
        "    # Model, </CEVAP> token’ına (veya eos_token_id’ye) ulaştığında duracağı\n",
        "    # için cevap_bitis etiketine şimdi ihtiyaç yok; generate sonrasında\n",
        "    # kesilecek.\n",
        "    prompt = f\"{soru_baslangic} {soru} {soru_bitis} {cevap_baslangic} \"\n",
        "\n",
        "    # ---------------------------------------------------------- #\n",
        "    # 3) Sonucu döndür\n",
        "    # ---------------------------------------------------------- #\n",
        "    return prompt\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "66af4847",
      "metadata": {
        "id": "66af4847"
      },
      "source": [
        "## Modeli yükleme fonksiyonu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a381238c",
      "metadata": {
        "id": "a381238c"
      },
      "outputs": [],
      "source": [
        "def model_yukle(\n",
        "    ince_ayari_yolu: str,\n",
        "    *,\n",
        "    base_model_adi: str,\n",
        "    dtype: str = \"auto\",\n",
        ") -> Tuple[AutoModelForCausalLM, AutoTokenizer]:\n",
        "    \"\"\"\n",
        "    İnce-ayarlı (fine-tuned) bir dil modelini veya yalnızca LoRA/adapter dosyalarını\n",
        "    otomatik olarak yükler.\n",
        "\n",
        "    • Eğer `ince_ayari_yolu` klasöründe TAM model ağırlıkları bulunuyorsa\n",
        "      (pytorch_model.bin veya model.safetensors) → doğrudan modeli yükler.\n",
        "    • Klasörde yalnızca LoRA/adapter dosyaları varsa →\n",
        "      1) Önce `base_model_adi` ile temel (base) modeli indirir/yükler,\n",
        "      2) Daha sonra adapter’ı temel modele uygular ve LoRA katmanlarını\n",
        "         `merge_and_unload()` ile birleşik hâle getirip hafızadan çıkarır.\n",
        "\n",
        "    Args:\n",
        "        ince_ayari_yolu (str):\n",
        "            İnce-ayarlı ağırlıkların (veya LoRA dosyalarının) yer aldığı klasör yolu.\n",
        "        base_model_adi (str):\n",
        "            Sadece adapter varsa indirilmesi gereken temel modelin HuggingFace hub adı\n",
        "            veya yerel yolu. Bu parametre *keyword-only* olarak tanımlıdır;\n",
        "            fonksiyonu çağırırken `base_model_adi=\"...\"` şeklinde belirtilmelidir.\n",
        "        dtype (str, optional):\n",
        "            Kullanılacak PyTorch *dtype*’ı.\n",
        "            \"auto\"  → Cihaz yeteneğine göre bf16 / fp16 / fp32 seçilir (varsayılan).\n",
        "            \"fp16\"  → Zorla float16.\n",
        "            \"bf16\"  → Zorla bfloat16.\n",
        "            Diğer   → float32.\n",
        "            NOT: Burada yalnızca hesaplamalarda kullanılacak veri tipi belirlenir;\n",
        "            model mimarisi değişmez.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[AutoModelForCausalLM, AutoTokenizer]:\n",
        "            0. indis → Yüklenen (varsa birleştirilmiş) dil modeli\n",
        "            1. indis → İnce-ayarlı klasörden gelen, pad token’ı ayarlanmış tokenizer\n",
        "    \"\"\"\n",
        "\n",
        "    # ------------------------------------------------------------------\n",
        "    # 1) dtype SEÇİM MANTIĞI\n",
        "    # ------------------------------------------------------------------\n",
        "    # \"auto\" modunda:\n",
        "    #   • Eğer CUDA GPU varsa, cihazın \"major compute capability\" değeri >= 8 ise\n",
        "    #     (ör: Ampere veya üstü) bfloat16, aksi takdirde float16 kullanılır.\n",
        "    #   • CUDA yoksa zorunlu olarak float32'e düşer.\n",
        "    # \"fp16\"/\"bf16\" → kullanıcı tarafından belirtilen dtype alınır.\n",
        "    # Diğer tüm değerler → float32.\n",
        "    # ------------------------------------------------------------------\n",
        "    if dtype == \"auto\":\n",
        "        if torch.cuda.is_available():\n",
        "            major_cc = torch.cuda.get_device_capability(0)[0]  # (major, minor) tuple → major\n",
        "            kull_dtype = torch.bfloat16 if major_cc >= 8 else torch.float16\n",
        "        else:\n",
        "            kull_dtype = torch.float32\n",
        "    elif dtype == \"fp16\":\n",
        "        kull_dtype = torch.float16\n",
        "    elif dtype == \"bf16\":\n",
        "        kull_dtype = torch.bfloat16\n",
        "    else:\n",
        "        kull_dtype = torch.float32\n",
        "\n",
        "    print(f\"\\n[MODEL] '{ince_ayari_yolu}' yükleniyor (dtype={kull_dtype})...\")\n",
        "\n",
        "    # ------------------------------------------------------------------\n",
        "    # 2) KLASÖRDE TAM MODEL AĞIRLIKLARI VAR MI?\n",
        "    # ------------------------------------------------------------------\n",
        "    #   • pytorch_model.bin  → Klasik PyTorch checkpoint\n",
        "    #   • model.safetensors → Daha güvenli / hızlı safetensors biçimi\n",
        "    # Varlıklardan herhangi biri bulunursa `tam_model_var` True olur.\n",
        "    # ------------------------------------------------------------------\n",
        "    tam_model_var = any(\n",
        "        os.path.isfile(os.path.join(ince_ayari_yolu, f))\n",
        "        for f in (\"pytorch_model.bin\", \"model.safetensors\")\n",
        "    )\n",
        "\n",
        "\n",
        "    # ------------------------------------------------------------------\n",
        "    # 3) TOKENIZER → DAİMA İNCE-AYARLI KLASÖRDEN\n",
        "    # ------------------------------------------------------------------\n",
        "    tokenizer = AutoTokenizer.from_pretrained(ince_ayari_yolu, use_fast=True)\n",
        "\n",
        "\n",
        "    # ------------------------------------------------------------------\n",
        "    # 4) MODELİ YÜKLE / BİRLEŞTİR\n",
        "    # ------------------------------------------------------------------\n",
        "    if tam_model_var:\n",
        "        # ► Klasördeki TAM modeli doğrudan belleğe al.\n",
        "        print(\"[MODEL] Tam model yükleniyor...\")\n",
        "        model = AutoModelForCausalLM.from_pretrained(\n",
        "            ince_ayari_yolu,\n",
        "            torch_dtype=kull_dtype,\n",
        "            device_map=\"auto\",   # HuggingFace → GPU RAM’ine otomatik dağıtım\n",
        "        )\n",
        "    else:\n",
        "        # ► Yalnızca adapter varsa:\n",
        "        #   a) Base modeli getir,\n",
        "        #   b) Adapter’ı üzerine uygula,\n",
        "        #   c) merge_and_unload() ile LoRA ağırlıklarını kalıcı şekilde birleştir.\n",
        "        print(\"[MODEL] LoRA ağırlıkları yükleniyor...\")\n",
        "        base_model = AutoModelForCausalLM.from_pretrained(\n",
        "            base_model_adi,\n",
        "            torch_dtype=kull_dtype,\n",
        "            device_map=\"auto\",\n",
        "        )\n",
        "        base_model.resize_token_embeddings(len(tokenizer))\n",
        "        model = PeftModel.from_pretrained(base_model, ince_ayari_yolu)\n",
        "        model = model.merge_and_unload()  # Inference’ta tek parça model kullanımı için\n",
        "\n",
        "    # Eğer tokenizer’da pad_token tanımlı değilse, EOS token’ını PAD olarak ayarla.\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    # ------------------------------------------------------------------\n",
        "    # 5) FONKSİYON ÇIKIŞI\n",
        "    # ------------------------------------------------------------------\n",
        "    return model, tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bf8e606b",
      "metadata": {
        "id": "bf8e606b"
      },
      "source": [
        "## Modelden cevabı alma fonksiyonu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2389d865",
      "metadata": {
        "id": "2389d865"
      },
      "outputs": [],
      "source": [
        "def cevap_uret(\n",
        "    model: AutoModelForCausalLM,\n",
        "    tokenizer: AutoTokenizer,\n",
        "    prompt: str,\n",
        "    *,\n",
        "    max_new_tokens: int = 200,           # Üretilecek azamî yeni token sayısı\n",
        "    temperature: float = 0.8,            # 1’den küçük => daha tutarlı / deterministik\n",
        "    top_p: float = 0.90,                 # Nucleus Sampling oranı\n",
        "    typical_p: float = 0.95,             # Tekrarı bastırmaya yardımcı olur\n",
        "    no_repeat_ngram_size: int = 3,       # Belirtilen uzunluktaki n-gram’lerin tekrarını engeller\n",
        "    repetition_penalty: float = 1.15,    # Tekrarı cezalandırma katsayısı\n",
        "    eos_token_id: Optional[int] = None,  # Erken durdurma için özel <EOS> belirteci\n",
        "    seed: int = 571                      # Sonuçların tekrarlanabilirliği için rasgelelik tohumu\n",
        ") -> str:\n",
        "    \"\"\"\n",
        "    Örneklemeye (``do_sample=True``) dayalı olarak,\n",
        "    girdiye (“prompt”) uygun, aşırı uzamayan bir yanıt üretir.\n",
        "\n",
        "    Args:\n",
        "        model (AutoModelForCausalLM): Yanıtı üretecek, nedensel dil modeli.\n",
        "        tokenizer (AutoTokenizer): Modele uygun belirteçleştirici (tokenizer).\n",
        "        prompt (str): Modelle beslenecek giriş metni.\n",
        "        max_new_tokens (int, optional): Üretilecek en fazla yeni token sayısı.\n",
        "        temperature (float, optional): Sıcaklık; 1’den düşük değerler çıktıyı tutarlı kılar.\n",
        "        top_p (float, optional): Nucleus Sampling için kümülatif olasılık eşiği.\n",
        "        typical_p (float, optional): “Typical sampling” olasılık eşiği (tekrarı bastırma).\n",
        "        no_repeat_ngram_size (int, optional): Tekrarı engellenecek n-gram uzunluğu.\n",
        "        repetition_penalty (float, optional): Tekrar eden dizileri cezalandırma katsayısı.\n",
        "        eos_token_id (Optional[int], optional): Erken durdurma <EOS> token kimliği.\n",
        "        seed (int, optional): Rasgele sayı üreteci tohum değeri (deterministiklik).\n",
        "\n",
        "    Returns:\n",
        "        str: ``prompt`` sonrası üretilen, temizlenmiş yanıt metni.\n",
        "    \"\"\"\n",
        "\n",
        "    # ---------------------------------------------------------------------\n",
        "    # 1. <EOS> belirtecinin kimliğini ayarla\n",
        "    # ---------------------------------------------------------------------\n",
        "    # Kullanıcı belirli bir `eos_token_id` vermediyse, tokenizer’dan\n",
        "    # özel olarak “</CEVAP>” metninin kimliğini alıyoruz.  \n",
        "    # Bu özel belirteç yanıtın bittiği yeri işaretlemek için kullanılır.\n",
        "    # ---------------------------------------------------------------------\n",
        "    if eos_token_id is None:\n",
        "        eos_token_id = tokenizer.convert_tokens_to_ids(\"</CEVAP>\")\n",
        "\n",
        "    # ---------------------------------------------------------------------\n",
        "    # 2. Tohumları (seed) ayarla\n",
        "    # ---------------------------------------------------------------------\n",
        "    # GPU veya CPU fark etmeksizin tekrarlanabilir sonuçlar almak için\n",
        "    # Python’un, NumPy’nin ve PyTorch’un rasgele sayı üreteçlerine aynı\n",
        "    # tohum değerini aktarırız.\n",
        "    # ---------------------------------------------------------------------\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        # Birden fazla CUDA çekirdeği varsa hepsine uygula\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "    # ---------------------------------------------------------------------\n",
        "    # 3. Girdiyi tensöre çevir\n",
        "    # ---------------------------------------------------------------------\n",
        "    # `tokenizer` ile metin → token kimliği; ardından `.to(model.device)`\n",
        "    # ile modelin bulunduğu cihaza (CPU / GPU) taşırız.\n",
        "    # ---------------------------------------------------------------------\n",
        "    giris_ids = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "    input_len: int = giris_ids[\"input_ids\"].shape[-1]   # Prompt uzunluğu\n",
        "\n",
        "    # ---------------------------------------------------------------------\n",
        "    # 4. Metin üretimi (sampling tabanlı)\n",
        "    # ---------------------------------------------------------------------\n",
        "    # `model.generate` ile istenen parametreler eşliğinde üretim yapılır.\n",
        "    # `do_sample=True` → Greedy yerine olasılığa dayalı seçim.\n",
        "    # `early_stopping=True` → İlk <EOS> token’ında dur.\n",
        "    # ---------------------------------------------------------------------\n",
        "    cikti_ids = model.generate(\n",
        "        **giris_ids,\n",
        "        do_sample=True,\n",
        "        temperature=temperature,\n",
        "        top_p=top_p,\n",
        "        typical_p=typical_p,\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        no_repeat_ngram_size=no_repeat_ngram_size,\n",
        "        repetition_penalty=repetition_penalty,\n",
        "        eos_token_id=eos_token_id,\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "        early_stopping=True\n",
        "    )\n",
        "\n",
        "    # ---------------------------------------------------------------------\n",
        "    # 5. Sadece “prompt” sonrasını al\n",
        "    # ---------------------------------------------------------------------\n",
        "    # Model tüm diziyi (prompt + yanıt) döndürür. Biz yalnızca\n",
        "    # yeni üretilen kısmı (`input_len:`) dekode ederiz.\n",
        "    # ---------------------------------------------------------------------\n",
        "    yeni_tokenlar: torch.Tensor = cikti_ids[0][input_len:]\n",
        "    yanit: str = tokenizer.decode(yeni_tokenlar, skip_special_tokens=True)\n",
        "\n",
        "    # ---------------------------------------------------------------------\n",
        "    # 6. Fazlalıkları temizle ve döndür\n",
        "    # ---------------------------------------------------------------------\n",
        "    # “</CEVAP>” dizesiyle karşılaşıldığında orada kesilir, \n",
        "    # uçlardaki boşluklar temizlenir.\n",
        "    # ---------------------------------------------------------------------\n",
        "    return yanit.split(\"</CEVAP>\")[0].strip()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9cdd68fe",
      "metadata": {
        "id": "9cdd68fe"
      },
      "source": [
        "# Sonuç üretme kısmı"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "694a0882",
      "metadata": {
        "id": "694a0882"
      },
      "source": [
        "## Veri kümesi oku"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a42bbee9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a42bbee9",
        "outputId": "200c0b16-a619-4a52-a319-6489862182bf"
      },
      "outputs": [],
      "source": [
        "df_sorular = pd.read_csv(veri_kumesi_yolu)\n",
        "print(f\"{len(df_sorular)} soru yüklendi.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d44e12b2",
      "metadata": {
        "id": "d44e12b2"
      },
      "source": [
        "## Her model için sonuç üret ve kaydet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "782b4d10",
      "metadata": {
        "id": "782b4d10"
      },
      "outputs": [],
      "source": [
        "# sonuçlar dizinini oluştur\n",
        "os.makedirs(sonuc_dizini, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "17152c64",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243,
          "referenced_widgets": [
            "51017d3f13ee4c059ec655e27d08bddb",
            "4de510348928469a8e1bceb61b56332f",
            "7b67e56f2757472d845996cc74352e5d",
            "1a714da2117a4b2184e1adc5e6f1c731",
            "786121ef5e844fc79f6e13f5040f92c9",
            "693efddd11144e1fba4a28ee9f8d55cd",
            "64a51a7879ab4e49be32b35b44fb6a6b",
            "2f50132ba3bb4bdebde70a30afdafa19",
            "5d32a9c0a70b485db01fc2f541aafa2f",
            "169c5d5009564ee3b8bc1e36af601c7d",
            "8ff97ee8c8b44fb8ac169d58acf3611e"
          ]
        },
        "id": "17152c64",
        "outputId": "aeea5d42-f636-45d5-dc88-55c88dc51e46"
      },
      "outputs": [],
      "source": [
        "# --------------------------------------------------------------------------- #\n",
        "#  Tüm modelleri tek tek dolaşarak, her birine karşılık sınama sorularının\n",
        "#  cevaplarını üretip .csv dosyasına kaydeden ana döngü\n",
        "# --------------------------------------------------------------------------- #\n",
        "\n",
        "for model_bilgisi in model_bilgileri:\n",
        "    model_etiket = model_bilgisi[1]\n",
        "    tuned_folder = model_bilgisi[2]\n",
        "    base_name = model_bilgisi[0]\n",
        "    # ----------------------------------------------------------------------- #\n",
        "    # 1) Çıktı dosyası yolu oluşturma\n",
        "    # ----------------------------------------------------------------------- #\n",
        "    # Örneğin `veri_kumesi = \"gpt2_medium_gpt4o\"` ise:\n",
        "    #   cikti_yolu = \"<kok_dizin>/sonuclar/gpt2_medium_gpt4o.csv\"\n",
        "    cikti_yolu = os.path.join(sonuc_dizini, f\"{model_etiket}.csv\")\n",
        "\n",
        "    # Aynı model için çıktı dosyası daha önce oluşturulmuşsa hesaplama\n",
        "    # tekrarlanmasın; zaman ve kaynak kazancı sağlanır.\n",
        "    if os.path.exists(cikti_yolu):\n",
        "        print(f\"[SKIP] '{cikti_yolu}' zaten var, geçiliyor.\")\n",
        "        continue\n",
        "\n",
        "    # ---- 2) Model + Tokenizer yükle  ----------------------------\n",
        "    model, tokenizer = model_yukle(\n",
        "        tuned_folder,          # ince-ayarlı klasör\n",
        "        base_model_adi=base_name   # hangi temel modelden türedi\n",
        "    )\n",
        "\n",
        "    # `model.eval()`  ⇒  Eğitim modunu kapat, tahmin (inference) moduna geç\n",
        "    #                  (dropout, layer norm gibi davranışlar sabitlenir).\n",
        "    model.eval()\n",
        "\n",
        "    # ----------------------------------------------------------------------- #\n",
        "    # 3) Soru listesi üzerinde döngü – her bir sorudan cevap üret\n",
        "    # ----------------------------------------------------------------------- #\n",
        "    cevaplar = []  # Üretilen cevapları tutacak liste\n",
        "\n",
        "    # tqdm → ilerleme çubuğu; iterasyon durumunu terminalde gösterir\n",
        "    for soru in tqdm(df_sorular[\"soru\"], desc=model_etiket):\n",
        "        # Soruyu, modelin eğitildiği özel etiketli prompt biçimine çevir\n",
        "        prompt = soruyu_girdi_bicimine_cevir(soru)\n",
        "\n",
        "        # cevap_uret() → deterministik (greedy) generate çağrısı yapar\n",
        "        cevap  = cevap_uret(model, tokenizer, prompt)\n",
        "\n",
        "        # Listeye ekle\n",
        "        cevaplar.append(cevap)\n",
        "\n",
        "    # ----------------------------------------------------------------------- #\n",
        "    # 4) Sonuç DataFrame’i oluştur ve diske kaydet\n",
        "    # ----------------------------------------------------------------------- #\n",
        "    #   • Başlangıçta okunan df_sorular DataFrame'inin aynısını kopyalayıp\n",
        "    #     yanına yeni bir \"cevap\" sütunu ekliyoruz.\n",
        "    df_sonuclar = df_sorular.copy()\n",
        "    df_sonuclar[\"cevap\"] = cevaplar\n",
        "\n",
        "    # .csv olarak kaydet → Unicode karakterler için encoding=\"utf-8\"\n",
        "    df_sonuclar.to_csv(cikti_yolu, index=False, encoding=\"utf-8\")\n",
        "    print(f\"[OK] {model_etiket} sonuçları '{cikti_yolu}' dosyasına kaydedildi.\")\n",
        "\n",
        "    # ----------------------------------------------------------------------- #\n",
        "    # 5) Bellek temizliği\n",
        "    # ----------------------------------------------------------------------- #\n",
        "    # Bir sonraki model yüklenmeden önce GPU belleğini boşaltmak kritik.\n",
        "    #   • `del model`           → Python referansını sil\n",
        "    #   • `torch.cuda.empty_cache()` → PyTorch'un ayrılmış fakat kullanılmayan\n",
        "    #                                   GPU belleğini boşalt\n",
        "    del model\n",
        "    torch.cuda.empty_cache()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "169c5d5009564ee3b8bc1e36af601c7d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1a714da2117a4b2184e1adc5e6f1c731": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_169c5d5009564ee3b8bc1e36af601c7d",
            "placeholder": "​",
            "style": "IPY_MODEL_8ff97ee8c8b44fb8ac169d58acf3611e",
            "value": " 1/50 [00:22&lt;18:29, 22.64s/it]"
          }
        },
        "2f50132ba3bb4bdebde70a30afdafa19": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4de510348928469a8e1bceb61b56332f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_693efddd11144e1fba4a28ee9f8d55cd",
            "placeholder": "​",
            "style": "IPY_MODEL_64a51a7879ab4e49be32b35b44fb6a6b",
            "value": "gpt2_medium_gpt4o:   2%"
          }
        },
        "51017d3f13ee4c059ec655e27d08bddb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4de510348928469a8e1bceb61b56332f",
              "IPY_MODEL_7b67e56f2757472d845996cc74352e5d",
              "IPY_MODEL_1a714da2117a4b2184e1adc5e6f1c731"
            ],
            "layout": "IPY_MODEL_786121ef5e844fc79f6e13f5040f92c9"
          }
        },
        "5d32a9c0a70b485db01fc2f541aafa2f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "64a51a7879ab4e49be32b35b44fb6a6b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "693efddd11144e1fba4a28ee9f8d55cd": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "786121ef5e844fc79f6e13f5040f92c9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7b67e56f2757472d845996cc74352e5d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2f50132ba3bb4bdebde70a30afdafa19",
            "max": 50,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5d32a9c0a70b485db01fc2f541aafa2f",
            "value": 1
          }
        },
        "8ff97ee8c8b44fb8ac169d58acf3611e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75228538",
   "metadata": {},
   "source": [
    "# Giriş işlemleri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "054a2645",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "from typing import Optional, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca03771a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_colab() -> bool:\n",
    "    \"\"\"\n",
    "    Google Colab ortamında çalışıp çalışmadığını kontrol eden fonksiyon.\n",
    "\n",
    "    Args:\n",
    "        None\n",
    "\n",
    "    Returns:\n",
    "        bool: Eğer kod Google Colab'da çalışıyorsa True, aksi halde False döndürür\n",
    "    \"\"\"\n",
    "    try:\n",
    "        import google.colab\n",
    "        return True\n",
    "    except ImportError:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b57e32c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kök dizin belirleme\n",
    "if is_colab():\n",
    "    \"\"\"\n",
    "    Eğer kod Google Colab ortamında çalışıyorsa, Google Drive'ı bağlar ve\n",
    "    kök dizini Google Drive içindeki \"turkish_gpt2_finetuning\" klasörü olarak ayarlar.\n",
    "    \"\"\"\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')  # Google Drive'ı Colab ortamına bağlar\n",
    "    kok_dizin = \"/content/drive/MyDrive/turkish_gpt2_finetuning\"  # Drive içindeki çalışma klasörünü belirler\n",
    "else:\n",
    "    \"\"\"\n",
    "    Eğer kod yerel bir ortamda çalışıyorsa, kök dizini mevcut çalışma dizini olarak ayarlar.\n",
    "    \"\"\"\n",
    "    kok_dizin = os.getcwd()  # Mevcut çalışma dizinini alır\n",
    "\n",
    "# Belirlenen kök dizini kullanıcıya bilgi olarak gösterir\n",
    "print(f\"Kök dizin: {kok_dizin}\\n Not: eğer colab kullanıyorsanız, dizini değiştirmeniz gerekir.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed9cca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "veri_kumesi_yolu = os.path.join(kok_dizin, \"sinama_sorulari.csv\")\n",
    "sonuc_dizini     = os.path.join(kok_dizin, \"sonuclar\")\n",
    "# Model kaydetme dizinleri\n",
    "# gpt4o verisiyle eğitilmiş modeller\n",
    "gpt2_medium_kaydetme_dizini_gpt4o = os.path.join(kok_dizin, \"gpt2_medium_gpt4o\")\n",
    "gpt2_large_kaydetme_dizini_gpt4o = os.path.join(kok_dizin, \"gpt2_large_gpt4o\")\n",
    "\n",
    "# deepseek verisiyle eğitilmiş modeller\n",
    "gpt2_medium_kaydetme_dizini_deepseek = os.path.join(kok_dizin, \"gpt2_medium_deepseek\")\n",
    "gpt2_large_kaydetme_dizini_deepseek = os.path.join(kok_dizin, \"gpt2_large_deepseek\")\n",
    "\n",
    "model_yollari = {\n",
    "    \"gpt2_medium_gpt4o\"   : gpt2_medium_kaydetme_dizini_gpt4o,\n",
    "    \"gpt2_large_gpt4o\"    : gpt2_large_kaydetme_dizini_gpt4o,\n",
    "    \"gpt2_medium_deepseek\": gpt2_medium_kaydetme_dizini_deepseek,\n",
    "    \"gpt2_large_deepseek\" : gpt2_large_kaydetme_dizini_deepseek,\n",
    "}\n",
    "\n",
    "soru_sutunu = \"soru\"\n",
    "\n",
    "# Özel token tanımlamaları (sınama biçimi için)\n",
    "soru_baslangic = \"<SORU>\"\n",
    "soru_bitis = \"</SORU>\"\n",
    "cevap_baslangic = \"<CEVAP>\"\n",
    "cevap_bitis = \"</CEVAP>\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd54335c",
   "metadata": {},
   "source": [
    "# Fonksiyonlar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aadb348",
   "metadata": {},
   "source": [
    "## Soruyu girdi biçimine çevirme fonksiyonu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b402a343",
   "metadata": {},
   "outputs": [],
   "source": [
    "def soruyu_girdi_bicimine_cevir(soru: str) -> str:\n",
    "    \"\"\"\n",
    "    Verilen ham soruyu, dil modeline uygun istem (prompt) biçimine dönüştürür.\n",
    "\n",
    "    Fonksiyon; dışarıda global olarak tanımlı özel etiketleri (<SORU>, </SORU>, \n",
    "    <CEVAP>) kullanarak aşağıdaki kalıpta bir metin hazırlar:\n",
    "\n",
    "        \"<SORU> {soru} </SORU> <CEVAP> \"\n",
    "\n",
    "    Model tarafında generate() çağrısı sırasında, </CEVAP> veya EOS token’ı\n",
    "    üretildiğinde üretim sonlandırılır ve yalnızca cevaba ait kısım alınır.\n",
    "    Böylece sorunun başı ve bitişi ile cevabın başlangıcı tutarlı bir şekilde\n",
    "    modellenmiş olur.\n",
    "\n",
    "    Args:\n",
    "        soru (str): Kullanıcıdan gelen ham soru metni. Baş-son boşlukları\n",
    "                    fonksiyon içinde temizlenir (str.strip()).\n",
    "\n",
    "    Returns:\n",
    "        str: Dil modeline beslenmeye hazır, etiketlenmiş prompt dizgesi.\n",
    "\n",
    "    Örnek:\n",
    "        >>> soruyu_girdi_bicimine_cevir(\"Türkiye'nin başkenti neresidir?\")\n",
    "        '<SORU> Türkiye'nin başkenti neresidir? </SORU> <CEVAP> '\n",
    "    \"\"\"\n",
    "\n",
    "    # ---------------------------------------------------------- #\n",
    "    # 1) Giriş temizliği\n",
    "    # ---------------------------------------------------------- #\n",
    "    # Sorunun başında / sonunda fazladan boşluk varsa yok sayılır.\n",
    "    soru = soru.strip()\n",
    "\n",
    "    # ---------------------------------------------------------- #\n",
    "    # 2) Prompt kalıbını oluştur\n",
    "    # ---------------------------------------------------------- #\n",
    "    # Global sabitler:\n",
    "    #   soru_baslangic = \"<SORU>\"\n",
    "    #   soru_bitis     = \"</SORU>\"\n",
    "    #   cevap_baslangic= \"<CEVAP>\"\n",
    "    #\n",
    "    # Model, </CEVAP> token’ına (veya eos_token_id’ye) ulaştığında duracağı\n",
    "    # için cevap_bitis etiketine şimdi ihtiyaç yok; generate sonrasında\n",
    "    # kesilecek.\n",
    "    prompt = f\"{soru_baslangic} {soru} {soru_bitis} {cevap_baslangic} \"\n",
    "\n",
    "    # ---------------------------------------------------------- #\n",
    "    # 3) Sonucu döndür\n",
    "    # ---------------------------------------------------------- #\n",
    "    return prompt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66af4847",
   "metadata": {},
   "source": [
    "## Modeli yükleme fonksiyonu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a381238c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_yukle(model_yolu: str, dtype: str = \"auto\") -> Tuple[AutoModelForCausalLM, AutoTokenizer]:\n",
    "    \"\"\" \n",
    "    İstenen yolu kullanarak, bitsandbytes vb. ek paketler olmaksızın \n",
    "    AutoModelForCausalLM ve ilgili AutoTokenizer nesnelerini yükler.  \n",
    "    Cihaz (CPU / GPU) ve veri tipi (dtype) seçimini akıllıca yapar, \n",
    "    tokenizer’ın pad token’ını garanti altına alır ve (model, tokenizer) ikilisini döndürür.\n",
    "\n",
    "    Args:\n",
    "        model_yolu (str): HuggingFace uyumlu, ön-eğitilmiş veya ince-ayarlı model klasörü / HF hub adı.\n",
    "        dtype (str, optional): Kullanılacak numerik veri tipi.\n",
    "            \"auto\" → GPU var ise (Ampere ve sonrası: bfloat16, öncesi: float16); GPU yoksa float32  \n",
    "            \"fp16\" → torch.float16  \n",
    "            \"bf16\" → torch.bfloat16  \n",
    "            \"fp32\" → torch.float32  \n",
    "            Varsayılan değer \"auto\"dur.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[AutoModelForCausalLM, AutoTokenizer]:\n",
    "            0. indeks → AutoModelForCausalLM: Yüklenen dil modeli  \n",
    "            1. indeks → AutoTokenizer      : Eşleşen tokenizer\n",
    "    \"\"\"\n",
    "\n",
    "    # --------------------------------------------------------------------- #\n",
    "    # 1) dtype seçimi\n",
    "    # --------------------------------------------------------------------- #\n",
    "    # Kullanıcı \"auto\" talep ettiyse, hangi veri tipini kullanacağımıza\n",
    "    # donanıma bakarak karar veriyoruz.\n",
    "    if dtype == \"auto\":\n",
    "        if torch.cuda.is_available():                          # GPU mevcut mu?\n",
    "            major_cc = torch.cuda.get_device_capability(0)[0]  # Compute capability (ör. 8.x ⇒ Ampere)\n",
    "            # Ampere (>= 8.x) kartlar bfloat16 destekler; daha eskilerde float16 güvenlidir.\n",
    "            kullanici_dtype = torch.bfloat16 if major_cc >= 8 else torch.float16\n",
    "        else:\n",
    "            # GPU yok → güvenli yol: float32\n",
    "            kullanici_dtype = torch.float32\n",
    "    # Kullanıcı doğrudan dtype belirtmişse eşle\n",
    "    elif dtype == \"fp16\":\n",
    "        kullanici_dtype = torch.float16\n",
    "    elif dtype == \"bf16\":\n",
    "        kullanici_dtype = torch.bfloat16\n",
    "    else:  # \"fp32\" veya bilinmeyen değerler\n",
    "        kullanici_dtype = torch.float32\n",
    "\n",
    "    # --------------------------------------------------------------------- #\n",
    "    # 2) Model ve tokenizer’ı yükle\n",
    "    # --------------------------------------------------------------------- #\n",
    "    print(f\"\\n[MODEL] '{model_yolu}' yükleniyor (dtype={kullanici_dtype})...\")\n",
    "\n",
    "    # device_map=\"auto\" → HF, model katmanlarını otomatik olarak CPU / GPU’ya dağıtır.\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_yolu,\n",
    "        torch_dtype=kullanici_dtype,\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "\n",
    "    # use_fast=True → mümkünse Rust tabanlı hızlı tokenizer kullan.\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_yolu, use_fast=True)\n",
    "\n",
    "    # --------------------------------------------------------------------- #\n",
    "    # 3) Pad token güvenliği\n",
    "    # --------------------------------------------------------------------- #\n",
    "    # Bazı GPT-2 türevlerinde pad_token tanımlı değildir.\n",
    "    # Yoksa, eos_token’ı pad_token olarak atayarak generate() çağrılarını\n",
    "    # sorunsuz hâle getiriyoruz.\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    # --------------------------------------------------------------------- #\n",
    "    # 4) Model & tokenizer döndür\n",
    "    # --------------------------------------------------------------------- #\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf8e606b",
   "metadata": {},
   "source": [
    "## Modelden cevabı alma fonksiyonu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2389d865",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cevap_uret(\n",
    "        model: AutoModelForCausalLM,\n",
    "        tokenizer: AutoTokenizer,\n",
    "        prompt: str,\n",
    "        *,\n",
    "        max_new_tokens: int = 1024,\n",
    "        no_repeat_ngram_size: int = 3,\n",
    "        repetition_penalty: float = 1.05,\n",
    "        eos_token_id: Optional[int] = None\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Dil modelinden (Causal Language Model) deterministik olarak cevap üretir.\n",
    "\n",
    "    Fonksiyon; verilen prompt’un ( <SORU> … <CEVAP> şeklinde hazırlanmış ) \n",
    "    devamında, model.generate ile greedy (do_sample=False) stratejisi kullanarak  \n",
    "    maksimum `max_new_tokens` uzunluğunda metin oluşturur ve </CEVAP> etiketine\n",
    "    (veya belirtilen eos_token_id’ye) ulaştığında durur.   \n",
    "    Dönüşte yalnızca cevabı (etiketsiz hâliyle) verir.\n",
    "\n",
    "    Args:\n",
    "        model (AutoModelForCausalLM): Cevap üretilecek, causal LM başlıklı model.\n",
    "        tokenizer (PreTrainedTokenizerBase): Prompt’u token’layacak/çıktıyı çözümleyecek tokenizer.\n",
    "        prompt (str): Modelin başlangıç girdi dizisi.\n",
    "        max_new_tokens (int, optional): Üretilecek maksimum yeni token sayısı. Varsayılan 1024.\n",
    "        no_repeat_ngram_size (int, optional): ‘n-gram’ tekrarı engelleme boyutu. Varsayılan 3.\n",
    "        repetition_penalty (float, optional): Tekrarlanan token’lar için ceza katsayısı (>1 arttırır). Varsayılan 1.05.\n",
    "        eos_token_id (Optional[int], optional): </CEVAP> token’ının id’si. \n",
    "            None ise fonksiyon tokenizer ile hesaplar.\n",
    "\n",
    "    Returns:\n",
    "        str: Modelin ürettiği cevabın (etiketsiz) son hâli.\n",
    "\n",
    "    Notlar:\n",
    "        - Greedy (do_sample=False) üretim kullanıldığı için temperature/top_p parametreleri dikkate alınmaz.\n",
    "        - GPU belleğini korumak için giriş tensor’u .to(model.device) ile doğru cihaza aktarılır.\n",
    "        - Dönen yanıt, </CEVAP> token’ı veya EOS görüldüğünde kesilir, \n",
    "          dolayısıyla ek string ayrıştırma gerektirmez.\n",
    "    \"\"\"\n",
    "\n",
    "    # Eğer kullanıcı özel bir sonlandırıcı token id’si vermemişse\n",
    "    # tokenizer’dan </CEVAP> etiketinin tamsayı id’sini bul.\n",
    "    if eos_token_id is None:\n",
    "        eos_token_id = tokenizer.convert_tokens_to_ids(\"</CEVAP>\")\n",
    "\n",
    "    # Girdi cümlesini token’layıp model’in çalıştığı cihaza (CPU/GPU) gönder.\n",
    "    # return_tensors=\"pt\" ⇒ PyTorch tensörü döndür.\n",
    "    giris_ids = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    # Model.generate:\n",
    "    #   - do_sample=False  ⇒ Greedy arama\n",
    "    #   - max_new_tokens   ⇒ Çıkış uzunluk sınırı\n",
    "    #   - no_repeat_ngram_size ⇒ Yinelenen n-gram’ları engelle\n",
    "    #   - repetition_penalty   ⇒ Tekrarlayan token’lara ceza\n",
    "    #   - eos_token_id         ⇒ </CEVAP> görüldüğünde dur\n",
    "    #   - pad_token_id         ⇒ Eksik durumlar için padding (eos_token) kullan\n",
    "    cikti_ids = model.generate(\n",
    "        **giris_ids,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        do_sample=False,  # deterministik üretim\n",
    "        eos_token_id=eos_token_id,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        no_repeat_ngram_size=no_repeat_ngram_size,\n",
    "        repetition_penalty=repetition_penalty,\n",
    "    )\n",
    "\n",
    "    # generate çıktısında, ilk kısım girilen prompt’un id’leridir.\n",
    "    # Biz sadece yeni üretilen token’ları decode etmek istiyoruz.\n",
    "    yeni_tokenlar = cikti_ids[0][giris_ids[\"input_ids\"].shape[-1]:]\n",
    "\n",
    "    # Token’ları metne çevir. skip_special_tokens=True ⇒ <|endoftext|> vb. temizle\n",
    "    yanit = tokenizer.decode(yeni_tokenlar, skip_special_tokens=True)\n",
    "\n",
    "    # </CEVAP> etiketinden sonrası gereksiz; split ile kes ve trim’le.\n",
    "    return yanit.split(\"</CEVAP>\")[0].strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cdd68fe",
   "metadata": {},
   "source": [
    "# Sonuç üretme kısmı"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "694a0882",
   "metadata": {},
   "source": [
    "## Veri kümesi oku"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42bbee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sorular = pd.read_csv(veri_kumesi_yolu)\n",
    "print(f\"{len(df_sorular)} soru yüklendi.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d44e12b2",
   "metadata": {},
   "source": [
    "## Her model için sonuç üret ve kaydet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782b4d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sonuçlar dizinini oluştur\n",
    "os.makedirs(sonuc_dizini, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17152c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------- #\n",
    "#  Tüm modelleri tek tek dolaşarak, her birine karşılık sınama sorularının\n",
    "#  cevaplarını üretip .csv dosyasına kaydeden ana döngü\n",
    "# --------------------------------------------------------------------------- #\n",
    "\n",
    "for model_etiket, model_folder in model_yollari.items():\n",
    "    # ----------------------------------------------------------------------- #\n",
    "    # 1) Çıktı dosyası yolu oluşturma\n",
    "    # ----------------------------------------------------------------------- #\n",
    "    # Örneğin `model_etiket = \"gpt2_medium_gpt4o\"` ise:\n",
    "    #   cikti_yolu = \"<kok_dizin>/sonuclar/gpt2_medium_gpt4o.csv\"\n",
    "    cikti_yolu = os.path.join(sonuc_dizini, f\"{model_etiket}.csv\")\n",
    "\n",
    "    # Aynı model için çıktı dosyası daha önce oluşturulmuşsa hesaplama\n",
    "    # tekrarlanmasın; zaman ve kaynak kazancı sağlanır.\n",
    "    if os.path.exists(cikti_yolu):\n",
    "        print(f\"[SKIP] '{cikti_yolu}' zaten var, geçiliyor.\")\n",
    "        continue  # döngünün bir sonraki modeline atla\n",
    "\n",
    "    # ----------------------------------------------------------------------- #\n",
    "    # 2) Model + Tokenizer yükleme\n",
    "    # ----------------------------------------------------------------------- #\n",
    "    # model_yukle() fonksiyonu:\n",
    "    #   • HuggingFace model klasörünü / hub adını `model_folder` argümanı ile alır\n",
    "    #   • Uygun veri tipinde (dtype) ve uygun cihaza (CPU / GPU) yerleştirilmiş\n",
    "    #     AutoModelForCausalLM nesnesi ve ilişkili AutoTokenizer döndürür\n",
    "    model, tokenizer = model_yukle(model_folder)\n",
    "\n",
    "    # `model.eval()`  ⇒  Eğitim modunu kapat, tahmin (inference) moduna geç\n",
    "    #                  (dropout, layer norm gibi davranışlar sabitlenir).\n",
    "    model.eval()\n",
    "\n",
    "    # ----------------------------------------------------------------------- #\n",
    "    # 3) Soru listesi üzerinde döngü – her bir sorudan cevap üret\n",
    "    # ----------------------------------------------------------------------- #\n",
    "    cevaplar = []  # Üretilen cevapları tutacak liste\n",
    "\n",
    "    # tqdm → ilerleme çubuğu; iterasyon durumunu terminalde gösterir\n",
    "    for soru in tqdm(df_sorular[\"soru\"], desc=model_etiket):\n",
    "        # Soruyu, modelin eğitildiği özel etiketli prompt biçimine çevir\n",
    "        prompt = soruyu_girdi_bicimine_cevir(soru)\n",
    "\n",
    "        # cevap_uret() → deterministik (greedy) generate çağrısı yapar\n",
    "        cevap  = cevap_uret(model, tokenizer, prompt)\n",
    "\n",
    "        # Listeye ekle\n",
    "        cevaplar.append(cevap)\n",
    "\n",
    "    # ----------------------------------------------------------------------- #\n",
    "    # 4) Sonuç DataFrame’i oluştur ve diske kaydet\n",
    "    # ----------------------------------------------------------------------- #\n",
    "    #   • Başlangıçta okunan df_sorular DataFrame'inin aynısını kopyalayıp\n",
    "    #     yanına yeni bir \"cevap\" sütunu ekliyoruz.\n",
    "    df_sonuclar = df_sorular.copy()\n",
    "    df_sonuclar[\"cevap\"] = cevaplar\n",
    "\n",
    "    # .csv olarak kaydet → Unicode karakterler için encoding=\"utf-8\"\n",
    "    df_sonuclar.to_csv(cikti_yolu, index=False, encoding=\"utf-8\")\n",
    "    print(f\"[OK] {model_etiket} sonuçları '{cikti_yolu}' dosyasına kaydedildi.\")\n",
    "\n",
    "    # ----------------------------------------------------------------------- #\n",
    "    # 5) Bellek temizliği\n",
    "    # ----------------------------------------------------------------------- #\n",
    "    # Bir sonraki model yüklenmeden önce GPU belleğini boşaltmak kritik.\n",
    "    #   • `del model`           → Python referansını sil\n",
    "    #   • `torch.cuda.empty_cache()` → PyTorch'un ayrılmış fakat kullanılmayan\n",
    "    #                                   GPU belleğini boşalt\n",
    "    del model\n",
    "    torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
